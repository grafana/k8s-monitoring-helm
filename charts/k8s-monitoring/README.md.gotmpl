<!--
(NOTE: Do not edit README.md directly. It is a generated file!)
(      To make changes, please modify README.md.gotmpl and run `helm-docs`)
-->

{{ template "chart.header" . }}
{{ template "chart.deprecationWarning" . }}

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

## Breaking change announcements

### Version 3.0

#### Alloy Operator

v3.0 introduces the use of the [Alloy Operator](https://github.com/grafana/alloy-operator) to manage the creation and
lifecycle of Alloy instances. When upgrading from v2.0 to v3.0 or later, you may need to install the Alloy CRD.

To do this, run the following command:

```shell
kubectl apply -f https://github.com/grafana/alloy-operator/releases/latest/download/collectors.grafana.com_alloy.yaml
```

#### Pod Logs

v3.0 also moves the `pod` and `k8s.pod.name` fields from labels to structured metadata in the pod logs feature. If your
logs destination does not support structured metadata, you may not see these labels on your logs.

### Version 2.0

v2 introduces some significant changes to the chart configuration values. Refer to the migration [documentation](./docs/Migration.md) for tools and strategies to migrate from v1.

## Usage

### Setup Grafana chart repository

```shell
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

### Build your values

There are some required values that will need to be used with this chart. The basic structure of the values file is:

```yaml
cluster: # Cluster configuration, including the cluster name
  name: my-cluster

destinations: [...] # List of destinations where telemetry data will be sent

# Features to enable, which determines what data to collect
clusterMetrics:
  enabled: true
clusterEvents:
  enabled: true
# etc...
...

# Telemetry collector definitions
alloy-metrics:
  enabled: true
alloy-singleton:
  enabled: true
```

Here is more detail about the different sections:

#### Cluster

This section defines the name of your cluster, which will be set as labels to all telemetry data.

```yaml
cluster:
  name: my-cluster
```

#### Destinations

([Documentation](./docs/destinations/README.md))

This section defines the destinations for your telemetry data. You can configure multiple destinations for logs,
metrics, and traces. Here are the supported destination types:

| Type         | Protocol         | Telemetry Data        | Docs                                      |
|--------------|------------------|-----------------------|-------------------------------------------|
| `prometheus` | Remote Write     | Metrics               | [Docs](./docs/destinations/prometheus.md) |
| `loki`       | Loki             | Logs                  | [Docs](./docs/destinations/loki.md)       |
| `otlp`       | OTLP or OTLPHTTP | Metrics, Logs, Traces | [Docs](./docs/destinations/otlp.md)       |
| `pyroscope`  | Pyroscope        | Profiles              | [Docs](./docs/destinations/pyroscope.md)  |

Here is an example of a destinations section:

```yaml
destinations:
  - name: hostedMetrics
    type: prometheus
    url: https://prometheus.example.com/api/prom/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
  - name: localPrometheus
    type: prometheus
    url: http://prometheus.monitoring.svc.cluster.local:9090
  - name: hostedLogs
    type: loki
    url: https://loki.example.com/loki/api/v1/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
      tenantIdFrom: env("LOKI_TENANT_ID")
```

#### Collectors

([Documentation](./docs/Collectors.md))

Collectors are the actual kubernetes workloads running alloy containers (deployments/statefulsets/daemonset) dedicated to certain tasks. Logs, metrics and app observability having different collection requirements leads to the need for multiple instances/clusters.

The main collectors are:

*   **alloy-logs** is the logs collector. It is deployed as a daemonset and scrapes workload logs on each node
*   **alloy-metrics** is a statefulset that scrapes metrics from prometheus sources like cadvisor and kube-state-metrics
*   **alloy-receiver** is a daemonset to collect metrics sent via HTTP, gRPC, Zipkin, etc

To enable a collector, add a new section to your values file. Ex:

```YAML
alloy-{collector_name}:
  enabled: true
```

**Specific features require specific collector configuration**. For example, the applicationObservability feature requires the alloy-receiver, with specific ports open for select protocols. Check [individual feature documentation](./docs/Features.md) to find out about collector requirements.

#### Features

([Documentation](./docs/Features.md))

This section is where you define which features you want to enable with this chart. Features define what kind of data to collect.

Here is an example of enabling some features:

```yaml
clusterMetrics:
  enabled: true

clusterEvents:
  enabled: true

podLogs:
  enabled: true
```

{{ template "chart.maintainersSection" . }}

<!-- markdownlint-disable no-bare-urls -->
<!-- markdownlint-disable list-marker-space -->
{{ template "chart.sourcesSection" . }}
<!-- markdownlint-enable list-marker-space -->

{{ template "chart.requirementsSection" . }}
<!-- markdownlint-enable no-bare-urls -->

{{ template "chart.valuesSection" . }}
