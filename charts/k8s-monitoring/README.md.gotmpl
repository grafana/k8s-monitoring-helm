<!--
(NOTE: Do not edit README.md directly. It is a generated file!)
(      To make changes, please modify README.md.gotmpl and run `helm-docs`)
-->

{{ template "chart.header" . }}
{{ template "chart.deprecationWarning" . }}
{{ template "chart.badgesSection" . }}
{{ template "chart.description" . }}
{{ template "chart.homepageLine" . }}

## Breaking change announcements

### Version 3.4

<!--alex disable hook-->
<!--alex disable hooks-->
#### Alloy no longer deployed by hooks

Version 3.3 deployed the Alloy instances during a post-install Helm hook. This cause problems where Alloy instances
were not handle properly when running upgrades or when using deployment tools like ArgoCD.

When upgrading to v3.4 or later, the Alloy resources will no longer be deployed by Helm hooks, but the upgrade may fail
with this message:

```text
Error: UPGRADE FAILED: Unable to continue with update: Alloy "k8smon-alloy-metrics" in namespace "default" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key "app.kubernetes.io/managed-by": must be set to "Helm"; annotation validation error: missing key "meta.helm.sh/release-name": must be set to "k8smon"; annotation validation error: missing key "meta.helm.sh/release-namespace": must be set to "default"
```

To resolve this, when you run the upgrade, use the `--take-ownership` flag which will update the Alloy resources to be
properly managed by Helm again.

### Version 3.0

#### Alloy Operator

v3.0 introduces the use of the [Alloy Operator](https://github.com/grafana/alloy-operator) to manage the creation and
lifecycle of Alloy instances. When upgrading from v2.0 to v3.0 or later, you may need to install the Alloy CRD.

To do this, run the following command:

```shell
kubectl apply -f https://github.com/grafana/alloy-operator/releases/latest/download/collectors.grafana.com_alloy.yaml
```

#### Pod Logs

v3.0 also moves the `pod` and `k8s.pod.name` fields from labels to structured metadata in the pod logs feature. If your
logs destination does not support structured metadata, you may not see these labels on your logs.

### Version 2.1

Version 2.1 was re-versioned to be 3.0. If you are on 2.1, please upgrade to 3.0.

### Version 2.0

v2 introduces some significant changes to the chart configuration values. Refer to the migration [documentation](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/helm-chart/migrate-helm-chart/) for tools and strategies to migrate from v1.

## Usage

### Setup Grafana chart repository

```shell
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```

### Build your values

There are some required values that will need to be used with this chart. The basic structure of the values file is:

```yaml
cluster: # Cluster configuration, including the cluster name
  name: my-cluster

destinations: [...] # List of destinations where telemetry data will be sent

# Features to enable, which determines what data to collect
clusterMetrics:
  enabled: true
clusterEvents:
  enabled: true
# etc...
...

# Telemetry collector definitions
alloy-metrics:
  enabled: true
alloy-singleton:
  enabled: true
```

Here is more detail about the different sections:

#### Cluster

This section defines the name of your cluster, which will be set as labels to all telemetry data.

```yaml
cluster:
  name: my-cluster
```

#### Destinations

([Documentation](./docs/destinations/README.md))

This section defines the destinations for your telemetry data. You can configure multiple destinations for logs,
metrics, and traces. Here are the supported destination types:

| Type         | Protocol         | Telemetry Data        | Docs                                      |
|--------------|------------------|-----------------------|-------------------------------------------|
| `prometheus` | Remote Write     | Metrics               | [Docs](./docs/destinations/prometheus.md) |
| `loki`       | Loki             | Logs                  | [Docs](./docs/destinations/loki.md)       |
| `otlp`       | OTLP or OTLPHTTP | Metrics, Logs, Traces | [Docs](./docs/destinations/otlp.md)       |
| `pyroscope`  | Pyroscope        | Profiles              | [Docs](./docs/destinations/pyroscope.md)  |

Here is an example of a destinations section:

```yaml
destinations:
  - name: hostedMetrics
    type: prometheus
    url: https://prometheus.example.com/api/prom/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
  - name: localPrometheus
    type: prometheus
    url: http://prometheus.monitoring.svc.cluster.local:9090
  - name: hostedLogs
    type: loki
    url: https://loki.example.com/loki/api/v1/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
      tenantIdFrom: env("LOKI_TENANT_ID")
```

Alternatively, you may also define a map of destinations. The key for each destination in the map will be used as the name.

```yaml
destinationsMap:
  hostedMetrics:
    type: prometheus
    url: https://prometheus.example.com/api/prom/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
  localPrometheus:
    type: prometheus
    url: http://prometheus.monitoring.svc.cluster.local:9090
  hostedLogs:
    type: loki
    url: https://loki.example.com/loki/api/v1/push
    auth:
      type: basic
      username: "my-username"
      password: "my-password"
      tenantIdFrom: env("LOKI_TENANT_ID")
```

#### Collectors

([Documentation](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/helm-chart/collector-reference/))

Collectors are workloads that are dedicated to gathering metrics, logs, traces, and profiles from the cluster and
from workloads on the cluster. There are multiple collector instances to optimize around the collection requirements.

The list of collectors are:

*   **alloy-metrics** is a StatefulSet that scrapes metrics from sources like cAdvisor, Kubelet, and kube-state-metrics.
*   **alloy-logs** is the logs collector. It is deployed as a DaemonSet and gathers Pod and Node logs.
*   **alloy-receiver** is a DaemonSet to collect telemetry data sent via HTTP, gRPC, Zipkin, etc...
*   **alloy-singleton** is a 1-replica Deployment to collect cluster events.
*   **alloy-profiles** is a DaemonSet used to instrument and collect profiling data.

To enable a collector, add a new section to your values file. Example:

```YAML
alloy-{collector_name}:
  enabled: true
```

**Specific features require specific collector configuration**. For example, the applicationObservability feature requires the alloy-receiver, with specific ports open for select protocols. Check [individual feature documentation](./docs/Features.md) to find out about collector requirements.

#### Features

([Documentation](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/helm-chart/#features))

This section is where you define which features you want to enable with this chart. Features define what kind of data to collect.

Here is an example of enabling some features:

```yaml
clusterMetrics:
  enabled: true

clusterEvents:
  enabled: true

podLogs:
  enabled: true
```

## Compatibility

The Kubernetes Monitoring Helm chart is designed to be compatible with all supported Kubernetes Cluster versions. It
deploys several dependent systems, which may have their own compatibility matrices. See their documentation for more
details:

| System             | Feature in k8s-monitoring | Link to documentation                                                                          |
|--------------------|---------------------------|------------------------------------------------------------------------------------------------|
| Alloy              | all                       | No published compatibility matrix                                                              |
| Alloy Operator     | all                       | No published compatibility matrix                                                              |
| Beyla              | `autoInstrumentation`     | No published compatibility matrix                                                              |
| Kepler             | `clusterMetrics`          | No published compatibility matrix                                                              |
| kube-state-metrics | `clusterMetrics`          | [Compatibility Matrix](https://github.com/kubernetes/kube-state-metrics#compatibility-matrix)] |
| Node Exporter      | `clusterMetrics`          | No published compatibility matrix                                                              |
| OpenCost           | `clusterMetrics`          | No published compatibility matrix                                                              |
| Windows Exporter   | `clusterMetrics`          | No published compatibility matrix                                                              |

<!-- textlint-disable terminology -->
{{ template "chart.maintainersSection" . }}
<!-- textlint-enable terminology -->

<!-- markdownlint-disable no-bare-urls -->
<!-- markdownlint-disable list-marker-space -->
{{ template "chart.sourcesSection" . }}
<!-- markdownlint-enable list-marker-space -->

{{ template "chart.requirementsSection" . }}
<!-- markdownlint-enable no-bare-urls -->

{{ template "chart.valuesSection" . }}
