---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-receiver
  namespace: default
data:
  config.alloy: |
    // Feature: Application Observability
    declare "application_observability" {
      argument "metrics_destinations" {
        comment = "Must be a list of metrics destinations where collected metrics should be forwarded to"
      }
    
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      argument "traces_destinations" {
        comment = "Must be a list of trace destinations where collected trace should be forwarded to"
      }
    
      // OTLP Receiver
      otelcol.receiver.otlp "receiver" {
        http {
          endpoint = "0.0.0.0:4318"
          include_metadata = false
          max_request_body_size = "20MiB"
        }
        debug_metrics {
          disable_high_cardinality_metrics = true
        }
        output {
          metrics = [otelcol.processor.resourcedetection.default.input]
          logs = [otelcol.processor.resourcedetection.default.input]
          traces = [otelcol.processor.resourcedetection.default.input]
        }
      }
    
      // Resource Detection Processor
      otelcol.processor.resourcedetection "default" {
        detectors = ["env","system"]
        override = true
    
        system {
          hostname_sources = ["os"]
        }
    
        output {
          metrics = [otelcol.processor.k8sattributes.default.input]
          logs = [otelcol.processor.k8sattributes.default.input]
          traces = [otelcol.processor.k8sattributes.default.input]
        }
      }
    
      // K8s Attributes Processor
      otelcol.processor.k8sattributes "default" {
        passthrough = false
        extract {
          metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.ip"
          }
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.uid"
          }
        }
        pod_association {
          source {
            from = "connection"
          }
        }
    
        output {
          metrics = [otelcol.processor.transform.default.input]
          logs = [otelcol.processor.transform.default.input]
          traces = [otelcol.processor.transform.default.input, otelcol.connector.host_info.default.input]
        }
      }
    
      // Host Info Connector
      otelcol.connector.host_info "default" {
        host_identifiers = [ "k8s.node.name" ]
    
        output {
          metrics = [otelcol.processor.batch.default.input]
        }
      }
    
      // Transform Processor
      otelcol.processor.transform "default" {
        error_mode = "ignore"
        log_statements {
          context = "resource"
          statements = [
            "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
            "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
            "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          ]
        }
    
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs = [otelcol.processor.batch.default.input]
          traces = [otelcol.processor.batch.default.input]
        }
      }
    
      // Batch Processor
      otelcol.processor.batch "default" {
        send_batch_size = 8192
        send_batch_max_size = 0
        timeout = "2s"
    
        output {
          metrics = argument.metrics_destinations.value
          logs = argument.logs_destinations.value
          traces = argument.traces_destinations.value
        }
      }
    }
    application_observability "feature" {
      metrics_destinations = [
        otelcol.processor.attributes.otlp_gateway.input,
      ]
      logs_destinations = [
        otelcol.processor.attributes.otlp_gateway.input,
      ]
      traces_destinations = [
        otelcol.processor.attributes.otlp_gateway.input,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        otelcol.receiver.prometheus.otlp_gateway.receiver,
      ]
    }
    
    
    
    
    // Destination: otlp-gateway (otlp)
    otelcol.receiver.prometheus "otlp_gateway" {
      output {
        metrics = [otelcol.processor.attributes.otlp_gateway.input]
      }
    }
    otelcol.receiver.loki "otlp_gateway" {
      output {
        logs = [otelcol.processor.attributes.otlp_gateway.input]
      }
    }
    
    otelcol.processor.attributes "otlp_gateway" {
      output {
        metrics = [otelcol.processor.transform.otlp_gateway.input]
        logs = [otelcol.processor.transform.otlp_gateway.input]
        traces = [otelcol.processor.transform.otlp_gateway.input]
      }
    }
    
    otelcol.processor.transform "otlp_gateway" {
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "applications-cluster")`,
          `set(attributes["k8s.cluster.name"], "applications-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      metric_statements {
        context = "datapoint"
        statements = [
          `set(attributes["cluster"], "applications-cluster")`,
          `set(attributes["k8s.cluster.name"], "applications-cluster")`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "applications-cluster")`,
          `set(attributes["k8s.cluster.name"], "applications-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      log_statements {
        context = "log"
        statements = [
          `delete_key(attributes, "loki.attribute.labels")`,
          `delete_key(attributes, "loki.resource.labels")`,
          `set(resource.attributes["k8s.container.name"], attributes["container"] ) where resource.attributes["k8s.container.name"] == nil and attributes["container"] != nil`,
          `delete_key(attributes, "container") where attributes["container"] == resource.attributes["k8s.container.name"]`,
          `set(resource.attributes["k8s.cronjob.name"], attributes["cronjob"] ) where resource.attributes["k8s.cronjob.name"] == nil and attributes["cronjob"] != nil`,
          `delete_key(attributes, "cronjob") where attributes["cronjob"] == resource.attributes["k8s.cronjob.name"]`,
          `set(resource.attributes["k8s.daemonset.name"], attributes["daemonset"] ) where resource.attributes["k8s.daemonset.name"] == nil and attributes["daemonset"] != nil`,
          `delete_key(attributes, "daemonset") where attributes["daemonset"] == resource.attributes["k8s.daemonset.name"]`,
          `set(resource.attributes["k8s.deployment.name"], attributes["deployment"] ) where resource.attributes["k8s.deployment.name"] == nil and attributes["deployment"] != nil`,
          `delete_key(attributes, "deployment") where attributes["deployment"] == resource.attributes["k8s.deployment.name"]`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["k8s.job.name"], attributes["job_name"] ) where resource.attributes["k8s.job.name"] == nil and attributes["job_name"] != nil`,
          `delete_key(attributes, "job_name") where attributes["job_name"] == resource.attributes["k8s.job.name"]`,
          `set(resource.attributes["k8s.namespace.name"], attributes["namespace"] ) where resource.attributes["k8s.namespace.name"] == nil and attributes["namespace"] != nil`,
          `delete_key(attributes, "namespace") where attributes["namespace"] == resource.attributes["k8s.namespace.name"]`,
          `set(resource.attributes["k8s.pod.name"], attributes["pod"] ) where resource.attributes["k8s.pod.name"] == nil and attributes["pod"] != nil`,
          `delete_key(attributes, "pod") where attributes["pod"] == resource.attributes["k8s.pod.name"]`,
          `set(resource.attributes["k8s.replicaset.name"], attributes["replicaset"] ) where resource.attributes["k8s.replicaset.name"] == nil and attributes["replicaset"] != nil`,
          `delete_key(attributes, "replicaset") where attributes["replicaset"] == resource.attributes["k8s.replicaset.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
          `set(resource.attributes["k8s.statefulset.name"], attributes["statefulset"] ) where resource.attributes["k8s.statefulset.name"] == nil and attributes["statefulset"] != nil`,
          `delete_key(attributes, "statefulset") where attributes["statefulset"] == resource.attributes["k8s.statefulset.name"]`,
        ]
      }
    
      trace_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "applications-cluster")`,
          `set(attributes["k8s.cluster.name"], "applications-cluster")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      output {
        metrics = [otelcol.processor.batch.otlp_gateway.input]
        logs = [otelcol.processor.batch.otlp_gateway.input]
        traces = [otelcol.processor.batch.otlp_gateway.input]
      }
    }
    
    otelcol.processor.batch "otlp_gateway" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlp.otlp_gateway.input]
        logs = [otelcol.exporter.otlp.otlp_gateway.input]
        traces = [otelcol.exporter.otlp.otlp_gateway.input]
      }
    }
    otelcol.exporter.otlp "otlp_gateway" {
      client {
        endpoint = "http://otlp-gateway.example.com"
        tls {
          insecure = false
          insecure_skip_verify = false
        }
      }
    
      retry_on_failure {
        enabled = true
        initial_interval = "5s"
        max_interval = "30s"
        max_elapsed_time = "5m"
      }
    
      sending_queue {
        enabled = true
      }
    }
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.6.1", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="applicationObservability", protocols="otlphttp", version="1.0.0"} 1
    # EOF
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator
rules:
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "serviceaccounts", "services"]
    verbs: ["*"]
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "statefulsets"]
    verbs: ["*"]
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["*"]
  # Rules which allow the management of Ingresses and NetworkPolicies.
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["*"]
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["*"]
  # Rules which allow the management of ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings.
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    verbs: ["*"]
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator-alloy-manager
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.12
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.4.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator

          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-receiver
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts:
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-receiver
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["collectors.grafana.com"]
    resources: ["alloys"]
    verbs: ["get", "list", "watch", "delete"]
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-add-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-add-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-add-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.6.1
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "15"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-add-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-add-finalizer
      containers:
        - name: add-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              kubectl patch \
                --namespace=default \
                --patch='{"metadata":{"finalizers":["k8s.grafana.com/finalizer"]}}' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.6.1
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      containers:
        - name: remove-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              echo "Deleting Alloy instance: alloy/k8smon-alloy-receiver..."
              kubectl delete alloy/k8smon-alloy-receiver --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-receiver --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-receiver or it may not exist."

              kubectl patch \
                --namespace=default \
                --type json \
                --patch='[{"op": "remove", "path": "/metadata/finalizers"}]' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
