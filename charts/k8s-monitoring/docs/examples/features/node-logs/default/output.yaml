---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Feature: Node Logs
    declare "node_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.relabel "journal" {
        rule {
          action = "keep"
          source_labels = ["__journal__systemd_unit"]
          regex = "kubelet.service|containerd.service"
        }
    
        // copy all journal labels and make the available to the pipeline stages as labels, there is a label
        // keep defined to filter out unwanted labels, these pipeline labels can be set as structured metadata
        // as well, the following labels are available:
        // - boot_id
        // - cap_effective
        // - cmdline
        // - comm
        // - exe
        // - gid
        // - hostname
        // - machine_id
        // - pid
        // - stream_id
        // - systemd_cgroup
        // - systemd_invocation_id
        // - systemd_slice
        // - systemd_unit
        // - transport
        // - uid
        //
        // More Info: https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html
        rule {
          action = "labelmap"
          regex = "__journal__(.+)"
        }
    
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "unit"
        }
    
        // the service_name label will be set automatically in loki if not set, and the unit label
        // will not allow service_name to be set automatically.
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "service_name"
        }
    
        forward_to = [] // No forward_to is used in this component, the defined rules are used in the loki.source.journal component
      }
    
      loki.source.journal "worker" {
        path = "/var/log/journal"
        format_as_json = false
        max_age = "8h"
        relabel_rules = loki.relabel.journal.rules
        labels = {
          job = "integrations/kubernetes/journal",
          instance = sys.env("HOSTNAME"),
        }
        forward_to = [loki.process.journal_logs.receiver]
      }
    
      loki.process "journal_logs" {
        stage.static_labels {
          values = {
            // add a static source label to the logs so they can be differentiated / restricted if necessary
            "source" = "journal",
            // default level to unknown
            level = "unknown",
          }
        }
    
        // Attempt to determine the log level, most k8s workers are either in logfmt or klog formats
        // check to see if the log line matches the klog format (https://github.com/kubernetes/klog)
        stage.match {
          // unescaped regex: ([IWED][0-9]{4}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+)
          selector = "{level=\"unknown\"} |~ \"([IWED][0-9]{4}\\\\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+)\""
    
          // extract log level, klog uses a single letter code for the level followed by the month and day i.e. I0119
          stage.regex {
            expression = "((?P<level>[A-Z])[0-9])"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(I)"
            replace = "INFO"
          }
    
          // if the extracted level is W set WARN
          stage.replace {
            source = "level"
            expression = "(W)"
            replace = "WARN"
          }
    
          // if the extracted level is E set ERROR
          stage.replace {
            source = "level"
            expression = "(E)"
            replace = "ERROR"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(D)"
            replace = "DEBUG"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // if the level is still unknown, do one last attempt at detecting it based on common levels
        stage.match {
          selector = "{level=\"unknown\"}"
    
          // unescaped regex: (?i)(?:"(?:level|loglevel|levelname|lvl|levelText|SeverityText)":\s*"|\s*(?:level|loglevel|levelText|lvl)="?|\s+\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))("|\s+|-|\s*\])
          stage.regex {
            expression = "(?i)(?:\"(?:level|loglevel|levelname|lvl|levelText|SeverityText)\":\\s*\"|\\s*(?:level|loglevel|levelText|lvl)=\"?|\\s+\\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))(\"|\\s+|-|\\s*\\])"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["instance","job","level","name","unit","service_name","source"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    node_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
    
    
    
    
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/api/push"
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "node-logs-cluster",
        "k8s_cluster_name" = "node-logs-cluster",
      }
    }
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-editor-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-viewer-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-role
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  # Rules which allow the management of Alloys.
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups:
      - ""
    resources:
      - configmaps
      - serviceaccounts
      - services
    verbs:
      - '*'
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - statefulsets
    verbs:
      - '*'
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - '*'
  # Rules which allow for the management of Prometheus Operator objects.
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
    verbs:
      - get
      - list
      - watch
  # Rules which allow the management of Ingresses.
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - '*'
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - '*'
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules which allow the management of ClusterRoles and ClusterRoleBindings.
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - clusterroles
    verbs:
      - '*'
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:
    k8smon-alloy-operator-rolebinding
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name:
    k8smon-alloy-operator-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:
    k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:
    k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name:
    k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.0
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.1.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.1.0"
          imagePullPolicy: IfNotPresent
          args:
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            runAsNonRoot: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-logs
  namespace: default
spec: 
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: true
      extra: []
      varlog: true
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations:
    - effect: NoSchedule
      operator: Exists
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-logs
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
