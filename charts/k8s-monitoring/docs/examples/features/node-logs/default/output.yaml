---
# Source: k8s-monitoring/charts/alloy-logs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-0.12.6
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Feature: Node Logs
    declare "node_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.relabel "journal" {
        rule {
          action = "keep"
          source_labels = ["__journal__systemd_unit"]
          regex = "kubelet.service|containerd.service"
        }
    
        // copy all journal labels and make the available to the pipeline stages as labels, there is a label
        // keep defined to filter out unwanted labels, these pipeline labels can be set as structured metadata
        // as well, the following labels are available:
        // - boot_id
        // - cap_effective
        // - cmdline
        // - comm
        // - exe
        // - gid
        // - hostname
        // - machine_id
        // - pid
        // - stream_id
        // - systemd_cgroup
        // - systemd_invocation_id
        // - systemd_slice
        // - systemd_unit
        // - transport
        // - uid
        //
        // More Info: https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html
        rule {
          action = "labelmap"
          regex = "__journal__(.+)"
        }
    
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "unit"
        }
    
        // the service_name label will be set automatically in loki if not set, and the unit label
        // will not allow service_name to be set automatically.
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "service_name"
        }
    
        forward_to = [] // No forward_to is used in this component, the defined rules are used in the loki.source.journal component
      }
    
      loki.source.journal "worker" {
        path = "/var/log/journal"
        format_as_json = false
        max_age = "8h"
        relabel_rules = loki.relabel.journal.rules
        labels = {
          job = "integrations/kubernetes/journal",
          instance = sys.env("HOSTNAME"),
        }
        forward_to = [loki.process.journal_logs.receiver]
      }
    
      loki.process "journal_logs" {
        stage.static_labels {
          values = {
            // add a static source label to the logs so they can be differentiated / restricted if necessary
            "source" = "journal",
            // default level to unknown
            level = "unknown",
          }
        }
    
        // Attempt to determine the log level, most k8s workers are either in logfmt or klog formats
        // check to see if the log line matches the klog format (https://github.com/kubernetes/klog)
        stage.match {
          // unescaped regex: ([IWED][0-9]{4}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+)
          selector = "{level=\"unknown\"} |~ \"([IWED][0-9]{4}\\\\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+)\""
    
          // extract log level, klog uses a single letter code for the level followed by the month and day i.e. I0119
          stage.regex {
            expression = "((?P<level>[A-Z])[0-9])"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(I)"
            replace = "INFO"
          }
    
          // if the extracted level is W set WARN
          stage.replace {
            source = "level"
            expression = "(W)"
            replace = "WARN"
          }
    
          // if the extracted level is E set ERROR
          stage.replace {
            source = "level"
            expression = "(E)"
            replace = "ERROR"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(D)"
            replace = "DEBUG"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // if the level is still unknown, do one last attempt at detecting it based on common levels
        stage.match {
          selector = "{level=\"unknown\"}"
    
          // unescaped regex: (?i)(?:"(?:level|loglevel|levelname|lvl|levelText|SeverityText)":\s*"|\s*(?:level|loglevel|levelText|lvl)="?|\s+\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))("|\s+|-|\s*\])
          stage.regex {
            expression = "(?i)(?:\"(?:level|loglevel|levelname|lvl|levelText|SeverityText)\":\\s*\"|\\s*(?:level|loglevel|levelText|lvl)=\"?|\\s+\\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))(\"|\\s+|-|\\s*\\])"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["instance","job","level","name","unit","service_name","source"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    node_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
    
    
    
    
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/api/push"
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "node-logs-cluster",
        "k8s_cluster_name" = "node-logs-cluster",
      }
    }
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.12.6
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.12.6
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-logs
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-logs
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-logs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-0.12.6
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-logs/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-0.12.6
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-logs
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-logs
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-logs
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.7.5
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - SETPCAP
              - NET_BIND_SERVICE
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: dockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
        - name: config-reloader
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          args:
            - --watched-dir=/etc/alloy
            - --reload-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-logs
        - name: varlog
          hostPath:
            path: /var/log
        - name: dockercontainers
          hostPath:
            path: /var/lib/docker/containers
