---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "loki-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  tenantId: "MQ=="
  username: "bG9raQ=="
  password: "bG9raXBhc3N3b3Jk"
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-singleton
  namespace: default
data:
  config.alloy: |
    // Feature: Kubernetes Manifests
    declare "kubernetes_manifests" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
    
      otelcol.receiver.filelog "pod_manifests" {
        include = ["/var/kubernetes-manifests/pods/*/*.json"]
        include_file_path_resolved = true
        start_at = "beginning"
        delete_after_read = true
    
        output {
          logs = [otelcol.processor.transform.pod_manifests.input]
        }
      }
    
      otelcol.processor.transform "pod_manifests" {
        error_mode = "ignore"
    
        log_statements {
          context = "log"
          statements = [
            `merge_maps(attributes, ExtractPatterns(log.attributes["log.file.path_resolved"], "^/var/kubernetes-manifests/pods/(?P<namespace>[^/]+)/(?P<pod>[^.]+)\\.json$"), "upsert")`,
            `set(resource.attributes["k8s.namespace.name"], attributes["namespace"])`,
            `set(resource.attributes["k8s.pod.name"], attributes["pod"])`,
            `set(resource.attributes["service.name"], "k8s.grafana.com/manifest-collector")`,
            `set(resource.attributes["service.namespace"], "default")`,
          ]
        }
    
        output {
          logs = [otelcol.processor.k8sattributes.pod_manifests.input]
        }
      }
    
      otelcol.processor.k8sattributes "pod_manifests" {
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.name"
          }
          source {
            from = "resource_attribute"
            name = "k8s.namespace.name"
          }
        }
    
        extract {
          metadata = [
            "k8s.cronjob.name",
            "k8s.daemonset.name",
            "k8s.deployment.name",
            "k8s.job.name",
            "k8s.node.name",
            "k8s.pod.start_time",
            "k8s.replicaset.name",
            "k8s.statefulset.name",
          ]
        }
    
        output {
          logs = argument.logs_destinations.value
        }
      }
    }
    kubernetes_manifests "feature" {
      logs_destinations = [
        otelcol.exporter.loki.loki.input,
      ]
    }
    
    
    
    
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/loki/api/v1/push"
        retry_on_http_429 = true
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.loki.data["tenantId"])
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.loki.data["username"])
          password = remote.kubernetes.secret.loki.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "kubernetes-manifest-cluster",
        "k8s_cluster_name" = "kubernetes-manifest-cluster",
      }
    }
    
    remote.kubernetes.secret "loki" {
      name      = "loki-k8smon-k8s-monitoring"
      namespace = "default"
    }
  collect-manifests.sh: |
    #!/bin/bash
    set -o pipefail
    
    script_name="${0##*/}"
    if [[ "${script_name}" == "bash" || "${script_name}" == "-bash" ]]; then
      script_name="script.sh"
    fi
    
    DefaultWatchTimeout=30s
    ManifestRequestThrottling=0.1s
    WatchRestartDelay=5
    
    usage() {
      echo "Usage: ${script_name} [OPTIONS]"
      echo ""
      echo "Collects Kubernetes manifests and saves them as files."
      echo ""
      echo "Resource manifests are stored at \${MANIFEST_DIR}/<kind>/<namespace>/<name>.json"
      echo ""
      echo "Requires the MANIFEST_DIR environment variable to be set to the target directory."
      echo ""
      echo "Options:"
      echo "  -k, --kind <kind>        Kubernetes resource kind passed to \"kubectl get\"."
      echo "                           Default: pods"
      echo "  -n, --namespace <name>   Namespace to scan. When omitted, all namespaces"
      echo "                           are scanned."
      echo "  -f, --filters <list>     Comma or space separated list of jq selectors to drop"
      echo "                           from the resource JSON. Default: \".status\""
      echo "  --watch-timeout <time>   How long to keep a watch open before restarting."
      echo "                           Default: ${DefaultWatchTimeout}s."
      echo "  -h, --help               Show this help message."
    }
    
    kind="pods"
    kindDir="pods"
    namespace=""
    filters=(".status")
    watchTimeout="${DefaultWatchTimeout}"
    
    while [[ $# -gt 0 ]]; do
      case "$1" in
        -k|--kind)
          if [[ $# -lt 2 ]]; then
            echo "Error: --kind requires an argument." >&2
            usage
            exit 1
          fi
          kind="$2"
          kindDir="${kind,,}"  # Forces lowercase
          kindDir="${kindDir//[^a-z0-9._-]/_}"  # Replace special characters with _
          shift 2
          ;;
        -n|--namespace)
          if [[ $# -lt 2 ]]; then
            echo "Error: --namespace requires an argument." >&2
            usage
            exit 1
          fi
          namespace="$2"
          shift 2
          ;;
        -f|--filters)
          if [[ $# -lt 2 ]]; then
            echo "Error: --filters requires an argument." >&2
            usage
            exit 1
          fi
    
          filters=()
          sanitized="${2//$'\n'/ }"
          sanitized="${sanitized//,/ }"
          read -ra parsedFilters <<< "${sanitized}"
          for filter in "${parsedFilters[@]}"; do
            [[ -n "${filter}" ]] || continue
            filters+=("${filter}")
          done
          jqFilters="$(build_jq_filter "${filters[@]}")"
    
          shift 2
          ;;
        --watch-timeout)
          if [[ $# -lt 2 ]]; then
            echo "Error: --watch-timeout requires an argument." >&2
            usage
            exit 1
          fi
          watchTimeout="$2"
          shift 2
          ;;
        -h|--help)
          usage
          exit 0
          ;;
        *)
          echo "Unknown option: $1" >&2
          usage
          exit 1
          ;;
      esac
    done
    
    if [[ -z "${MANIFEST_DIR:-}" ]]; then
      echo "Error: MANIFEST_DIR environment variable must be set." >&2
      exit 1
    fi
    
    mkdir -p "${MANIFEST_DIR}"
    
    build_jq_filter() {
      local program="."
      for filter in "$@"; do
        [[ -n "${filter}" ]] || continue
        program+=" | del(${filter})"
      done
      printf '%s' "${program}"
    }
    
    jqFilters="$(build_jq_filter "${filters[@]}")"
    
    collect_manifest() {
      local namespace="$1"
      local resourceName="$2"
    
      [[ -n "${namespace}" && -n "${resourceName}" ]] || return 0
    
      local namespaceDir="${MANIFEST_DIR}/${kindDir}/${namespace}"
      mkdir -p "${namespaceDir}"
    
      local outputFile="${namespaceDir}/${resourceName}.json"
      local tmpFile="${outputFile}.tmp"
    
      if kubectl get "${kind}" --namespace "${namespace}" "${resourceName}" -o json \
        | jq --compact-output "${jqFilters}" > "${tmpFile}"; then
        if [[ ! -f "${outputFile}" ]] || ! cmp -s "${tmpFile}" "${outputFile}"; then
          echo "[INFO] ${kind}: Saving manifest for \"${namespace}/${resourceName}\""
          mv "${tmpFile}" "${outputFile}"
        else
          echo "[DEBUG] ${kind}: No changes to manifest for \"${namespace}/${resourceName}\""
          rm -f "${tmpFile}"
        fi
      else
        echo "[ERROR] ${kind}: Failed to collect manifest for ${kind} ${namespace}/${resourceName}" >&2
        rm -f "${tmpFile}"
      fi
    }
    
    remove_manifest() {
      local namespace="$1"
      local resourceName="$2"
    
      [[ -n "${namespace}" && -n "${resourceName}" ]] || return
    
      local outputFile="${MANIFEST_DIR}/${kindDir}/${namespace}/${resourceName}.json"
      if [[ -f "${outputFile}" ]]; then
        rm -f "${outputFile}"
        echo "[INFO] ${kind}: Removed manifest for \"${namespace}/${resourceName}\""
      fi
    }
    
    handle_watch_event() {
      local eventType="$1"
      local namespace="$2"
      local resourceName="$3"
    
      [[ -n "${eventType}" && -n "${namespace}" && -n "${resourceName}" ]] || return
    
      echo "[DEBUG] ${kind}: ${eventType} event for ${namespace}/${resourceName}"
      case "${eventType}" in
        ADDED|MODIFIED)
          collect_manifest "${namespace}" "${resourceName}"
          ;;
        DELETED)
          remove_manifest "${namespace}" "${resourceName}"
          ;;
        *)
          ;;
      esac
    }
    
    watch_resources() {
      local kubectlArgs=()
      if [[ -n "${namespace}" ]]; then
        kubectlArgs=(--namespace "${namespace}")
        echo "[INFO] ${kind}: Watching namespace ${namespace}"
      else
        kubectlArgs=(--all-namespaces)
        echo "[INFO] ${kind}: Watching all namespaces"
      fi
    
      while true; do
        if ! timeout --foreground "${watchTimeout}" kubectl get "${kind}" "${kubectlArgs[@]}" --watch --output-watch-events -o json \
          | jq --unbuffered -r 'select(.object.metadata.namespace != null and .object.metadata.name != null and .type != null) | "\(.type) \(.object.metadata.namespace) \(.object.metadata.name)"' \
          | while read -r eventType namespace resourceName; do
              handle_watch_event "${eventType}" "${namespace}" "${resourceName}"
              sleep "${ManifestRequestThrottling}"  # Throttle the requests
            done; then
          echo "[WARN] ${kind}: Watch ended. Restarting in ${WatchRestartDelay} seconds." >&2
          sleep "${WatchRestartDelay}"
        fi
      done
    }
    
    watch_resources
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator
rules:
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "serviceaccounts", "services"]
    verbs: ["*"]
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "statefulsets"]
    verbs: ["*"]
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["*"]
  # Rules which allow the management of Ingresses and NetworkPolicies.
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["*"]
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["*"]
  # Rules which allow the management of ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings.
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    verbs: ["*"]
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator-alloy-manager
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/templates/kubernetes-manifest-rules.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-alloy-singleton-kubernetes-manifests
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/templates/kubernetes-manifest-rules.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-alloy-singleton-kubernetes-manifests
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8smon-alloy-singleton-kubernetes-manifests
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-singleton
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.4.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.5.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.4.0
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.5.2"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      securityContext:
        runAsNonRoot: true
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.5.2"
          imagePullPolicy: IfNotPresent
          args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator

          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-singleton
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableHttpServerPort: true
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    initialDelaySeconds: 10
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra:
      - mountPath: /var/kubernetes-manifests
        name: kubernetes-manifests
        readOnly: false
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: public-preview
    storagePath: /tmp/alloy
    timeoutSeconds: 1
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers:
    - command:
      - /bin/bash
      - -c
      - |
        set -euo pipefail
        pids=()
        bash /etc/alloy/collect-manifests.sh --kind pods --namespace "default" --watch-timeout "1d" &
        pids+=("$!")
        trap 'for pid in "${pids[@]}"; do kill "${pid}" 2>/dev/null || true; done' EXIT
        wait -n "${pids[@]}"
      env:
      - name: MANIFEST_DIR
        value: /var/kubernetes-manifests
      image: ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.3
      imagePullPolicy: IfNotPresent
      name: kubernetes-manifest-collector
      volumeMounts:
      - mountPath: /etc/alloy
        name: config
      - mountPath: /var/kubernetes-manifests
        name: kubernetes-manifests
        readOnly: false
    extraLabels: {}
    hostNetwork: false
    hostPID: false
    initContainers: []
    minReadySeconds: 10
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: deployment
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra:
      - emptyDir:
          medium: Memory
        name: kubernetes-manifests
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-singleton
  networkPolicy:
    egress:
    - {}
    enabled: false
    flavor: kubernetes
    ingress:
    - {}
    policyTypes:
    - Ingress
    - Egress
  rbac:
    clusterRules:
    - apiGroups:
      - ""
      resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      verbs:
      - get
      - list
      - watch
    - nonResourceURLs:
      - /metrics
      verbs:
      - get
    create: true
    namespaces: []
    rules:
    - apiGroups:
      - ""
      - discovery.k8s.io
      - networking.k8s.io
      resources:
      - endpoints
      - endpointslices
      - ingresses
      - pods
      - services
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - pods
      - pods/log
      - namespaces
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.grafana.com
      resources:
      - podlogs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - prometheusrules
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - alertmanagerconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - events
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - configmaps
      - secrets
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - apps
      - extensions
      resources:
      - replicasets
      verbs:
      - get
      - list
      - watch
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["collectors.grafana.com"]
    resources: ["alloys"]
    verbs: ["get", "list", "watch", "delete"]
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-add-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-add-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-add-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.7.2
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "15"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-add-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-add-finalizer
      containers:
        - name: add-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              kubectl patch \
                --namespace=default \
                --patch='{"metadata":{"finalizers":["k8s.grafana.com/finalizer"]}}' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.7.2
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      containers:
        - name: remove-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              echo "Deleting Alloy instance: alloy/k8smon-alloy-singleton..."
              kubectl delete alloy/k8smon-alloy-singleton --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-singleton --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-singleton or it may not exist."

              kubectl patch \
                --namespace=default \
                --type json \
                --patch='[{"op": "remove", "path": "/metadata/finalizers"}]' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
