---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    declare "loki_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "loki_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=loki\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: http-metrics)"
          optional = true
        }
    
        // loki service discovery for all of the pods
        discovery.kubernetes "loki_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=loki"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
    
        }
    
        // loki relabelings (pre-scrape)
        discovery.relabel "loki_pods" {
          targets = discovery.kubernetes.loki_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "http-metrics") + "@Running@true@false"
            action = "keep"
          }
    
          // the loki-mixin expects the job label to be namespace/component
          rule {
            source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_component"]
            separator = "/"
            target_label = "job"
          }
    
    
    
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }
    
          // set the workload to the controller kind and name
          rule {
            action = "lowercase"
            source_labels = ["__meta_kubernetes_pod_controller_kind"]
            target_label  = "workload_type"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_controller_name"]
            target_label  = "workload"
          }
    
          // remove the hash from the ReplicaSet
          rule {
            source_labels = [
              "workload_type",
              "workload",
            ]
            separator = "/"
            regex = "replicaset/(.+)-.+$"
            target_label  = "workload"
          }
    
          // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
          rule {
            action = "replace"
            source_labels = [
              "__meta_kubernetes_pod_label_app_kubernetes_io_name",
              "__meta_kubernetes_pod_label_k8s_app",
              "__meta_kubernetes_pod_label_app",
            ]
            separator = ";"
            regex = "^(?:;*)?([^;]+).*$"
            replacement = "$1"
            target_label = "app"
          }
    
          // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
          rule {
            action = "replace"
            source_labels = [
              "__meta_kubernetes_pod_label_app_kubernetes_io_component",
              "__meta_kubernetes_pod_label_k8s_component",
              "__meta_kubernetes_pod_label_component",
            ]
            regex = "^(?:;*)?([^;]+).*$"
            replacement = "$1"
            target_label = "component"
          }
    
          // set a source label
          rule {
            action = "replace"
            replacement = "kubernetes"
            target_label = "source"
          }
    
        }
    
        export "output" {
          value = discovery.relabel.loki_pods.output
        }
      }
    
      declare "loki_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Loki metrics (default: integrations/loki)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "loki" {
          job_name = coalesce(argument.job_label.value, "integrations/loki")
          forward_to = [prometheus.relabel.loki.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // loki metric relabelings (post-scrape)
        prometheus.relabel "loki" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "")
            action = "drop"
          }
    
          // keep only metrics that match the keep_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.keep_metrics.value, "(.+)")
            action = "keep"
          }
    
          // the loki-mixin expects the instance label to be the node name
          rule {
            source_labels = ["node"]
            target_label = "instance"
            replacement = "$1"
          }
          rule {
            action = "labeldrop"
            regex = "node"
          }
    
          // set the memcached exporter container name from container="exporter" to container="memcached"
          rule {
            source_labels = ["component", "container"]
            separator = ";"
            regex = "memcached-[^;]+;exporter"
            target_label = "container"
            replacement = "memcached"
          }
        }
      }
    
      loki_integration_discovery "loki" {
        namespaces = []
        label_selectors = ["app.kubernetes.io/name=loki"]
        port_name = "http-metrics"
      }
    
      loki_integration_scrape  "loki" {
        targets = loki_integration_discovery.loki.output
        job_label = "integrations/loki"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|go_gc_cycles_total_gc_cycles_total|go_gc_duration_seconds|go_gc_duration_seconds_count|go_gc_duration_seconds_sum|go_gc_pauses_seconds_bucket|go_goroutines|go_memstats_heap_inuse_bytes|loki_azure_blob_request_duration_seconds_bucket|loki_azure_blob_request_duration_seconds_count|loki_bigtable_request_duration_seconds_bucket|loki_bigtable_request_duration_seconds_count|loki_bloom_blocks_cache_added_total|loki_bloom_blocks_cache_entries|loki_bloom_blocks_cache_evicted_total|loki_bloom_blocks_cache_fetched_total|loki_bloom_blocks_cache_usage_bytes|loki_bloom_chunks_indexed_total|loki_bloom_gateway_block_query_latency_seconds_bucket|loki_bloom_gateway_dequeue_duration_seconds_bucket|loki_bloom_gateway_filtered_chunks_sum|loki_bloom_gateway_filtered_series_sum|loki_bloom_gateway_inflight_tasks|loki_bloom_gateway_process_duration_seconds_bucket|loki_bloom_gateway_process_duration_seconds_count|loki_bloom_gateway_querier_chunks_filtered_total|loki_bloom_gateway_querier_chunks_skipped_total|loki_bloom_gateway_querier_chunks_total|loki_bloom_gateway_querier_series_filtered_total|loki_bloom_gateway_querier_series_skipped_total|loki_bloom_gateway_querier_series_total|loki_bloom_gateway_queue_duration_seconds_bucket|loki_bloom_gateway_queue_duration_seconds_count|loki_bloom_gateway_queue_duration_seconds_sum|loki_bloom_gateway_queue_length|loki_bloom_gateway_requested_chunks_sum|loki_bloom_gateway_requested_series_sum|loki_bloom_gateway_tasks_dequeued_bucket|loki_bloom_gateway_tasks_dequeued_total|loki_bloom_gateway_tasks_processed_total|loki_bloom_inserts_total|loki_bloom_recorder_chunks_total|loki_bloom_recorder_series_total|loki_bloom_size_bucket|loki_bloom_store_blocks_fetched_size_bytes_bucket|loki_bloom_store_blocks_fetched_sum|loki_bloom_store_download_queue_size_sum|loki_bloom_store_metas_fetched_bucket|loki_bloom_store_metas_fetched_size_bytes_bucket|loki_bloom_store_metas_fetched_sum|loki_bloom_tokens_total|loki_bloombuilder_blocks_created_total|loki_bloombuilder_blocks_reused_total|loki_bloombuilder_bytes_per_task_bucket|loki_bloombuilder_chunk_series_size_sum|loki_bloombuilder_metas_created_total|loki_bloombuilder_processing_task|loki_bloombuilder_series_per_task_bucket|loki_bloomplanner_blocks_deleted_total|loki_bloomplanner_connected_builders|loki_bloomplanner_inflight_tasks|loki_bloomplanner_metas_deleted_total|loki_bloomplanner_queue_length|loki_bloomplanner_retention_running|loki_bloomplanner_retention_time_seconds_bucket|loki_bloomplanner_tenant_tasks_completed|loki_bloomplanner_tenant_tasks_planned|loki_boltdb_shipper_compact_tables_operation_duration_seconds|loki_boltdb_shipper_compact_tables_operation_last_successful_run_timestamp_seconds|loki_boltdb_shipper_compact_tables_operation_total|loki_boltdb_shipper_request_duration_seconds_bucket|loki_boltdb_shipper_request_duration_seconds_count|loki_boltdb_shipper_request_duration_seconds_sum|loki_boltdb_shipper_retention_marker_count_total|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_bucket|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_count|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_sum|loki_boltdb_shipper_retention_marker_table_processed_total|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_bucket|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_count|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_sum|loki_boltdb_shipper_retention_sweeper_marker_file_processing_current_time|loki_boltdb_shipper_retention_sweeper_marker_files_current|loki_build_info|loki_chunk_store_deduped_chunks_total|loki_chunk_store_index_entries_per_chunk_count|loki_chunk_store_index_entries_per_chunk_sum|loki_compactor_apply_retention_last_successful_run_timestamp_seconds|loki_compactor_apply_retention_operation_duration_seconds|loki_compactor_apply_retention_operation_total|loki_compactor_delete_requests_processed_total|loki_compactor_delete_requests_received_total|loki_compactor_deleted_lines|loki_compactor_load_pending_requests_attempts_total|loki_compactor_locked_table_successive_compaction_skips|loki_compactor_oldest_pending_delete_request_age_seconds|loki_compactor_pending_delete_requests_count|loki_consul_request_duration_seconds_bucket|loki_discarded_samples_total|loki_distributor_bytes_received_total|loki_distributor_ingester_append_failures_total|loki_distributor_lines_received_total|loki_distributor_structured_metadata_bytes_received_total|loki_dynamo_consumed_capacity_total|loki_dynamo_dropped_requests_total|loki_dynamo_failures_total|loki_dynamo_query_pages_count|loki_dynamo_request_duration_seconds_bucket|loki_dynamo_request_duration_seconds_count|loki_dynamo_throttled_total|loki_embeddedcache_entries|loki_embeddedcache_memory_bytes|loki_gcs_request_duration_seconds_bucket|loki_gcs_request_duration_seconds_count|loki_index_gateway_postfilter_chunks_sum|loki_index_gateway_prefilter_chunks_sum|loki_index_request_duration_seconds_bucket|loki_index_request_duration_seconds_count|loki_index_request_duration_seconds_sum|loki_ingester_chunk_age_seconds_bucket|loki_ingester_chunk_age_seconds_count|loki_ingester_chunk_age_seconds_sum|loki_ingester_chunk_bounds_hours_bucket|loki_ingester_chunk_bounds_hours_count|loki_ingester_chunk_bounds_hours_sum|loki_ingester_chunk_entries_bucket|loki_ingester_chunk_entries_count|loki_ingester_chunk_entries_sum|loki_ingester_chunk_size_bytes_bucket|loki_ingester_chunk_utilization_bucket|loki_ingester_chunk_utilization_count|loki_ingester_chunk_utilization_sum|loki_ingester_chunks_flushed_total|loki_ingester_flush_queue_length|loki_ingester_memory_chunks|loki_ingester_memory_streams|loki_ingester_streams_created_total|loki_memcache_request_duration_seconds_bucket|loki_memcache_request_duration_seconds_count|loki_panic_total|loki_prometheus_rule_group_rules|loki_request_duration_seconds_bucket|loki_request_duration_seconds_count|loki_request_duration_seconds_sum|loki_ruler_wal_appender_ready|loki_ruler_wal_disk_size|loki_ruler_wal_prometheus_remote_storage_highest_timestamp_in_seconds|loki_ruler_wal_prometheus_remote_storage_queue_highest_sent_timestamp_seconds|loki_ruler_wal_prometheus_remote_storage_samples_pending|loki_ruler_wal_prometheus_remote_storage_samples_total|loki_ruler_wal_samples_appended_total|loki_ruler_wal_storage_created_series_total|loki_s3_request_duration_seconds_bucket|loki_s3_request_duration_seconds_count"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    loki_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "loki-integration-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "loki-integration-cluster"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }

  self-reporting-metric.prom: |
    
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.0.0", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="integrations", sources="loki", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
    
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
    
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
    
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
    
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.pod.name
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
    
        // set resource attributes
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex = "(.+)"
          target_label = "app_kubernetes_io_name"
        }
        // add static label of integration="loki" and instance="name" to pods that match the selector so they can be identified in the loki.process stages
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:loki)"
          target_label = "integration"
          replacement = "loki"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:loki)"
          target_label = "instance"
          replacement = "loki"
        }
        // override the job label to be namespace/component so it aligns to the loki-mixin
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name","__meta_kubernetes_namespace","__meta_kubernetes_pod_label_component"]
          separator = ";"
          regex = "(?:loki);([^;]+);([^;]+)"
          target_label = "job"
          replacement = "$1/$2"
        }
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
    
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
    
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
    
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
    
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
    
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
    
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
    
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
    
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        stage.structured_metadata {
          values = {
            "k8s_pod_name" = "k8s_pod_name",
            "pod" = "pod",
          }
        }
        // Integration: Loki
        stage.match {
          selector = "{integration=\"loki\",instance=\"loki\"}"
    
          // extract some of the fields from the log line
          stage.logfmt {
            mapping = {
              "ts" = "",
              "level" = "",
            }
          }
    
          // set the level as a label
          stage.labels {
            values = {
              level = "level",
            }
          }
          // reset the timestamp to the extracted value
          stage.timestamp {
            source = "ts"
            format = "RFC3339Nano"
          }
          // remove the timestamp from the log line
          stage.replace {
            expression = `(?:^|\s+)(ts=\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[^ ]*\s+)`
            replace = ""
          }
          // drop certain log levels
          stage.drop {
            source = "level"
            expression = "(?i)(debug)"
            drop_counter_reason = "loki-drop-log-level"
          }
    
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["app_kubernetes_io_name","container","instance","job","level","namespace","service_name","service_namespace","deployment_environment","deployment_environment_name","k8s_namespace_name","k8s_deployment_name","k8s_statefulset_name","k8s_daemonset_name","k8s_cronjob_name","k8s_job_name","k8s_node_name"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
    
    
    
    
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/api/push"
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "loki-integration-cluster",
        "k8s_cluster_name" = "loki-integration-cluster",
      }
    }
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-editor-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-viewer-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-role
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  # Rules which allow the management of Alloys.
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups:
      - ""
    resources:
      - configmaps
      - serviceaccounts
      - services
    verbs:
      - '*'
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - statefulsets
    verbs:
      - '*'
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - '*'
  # Rules which allow for the management of Prometheus Operator objects.
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
    verbs:
      - get
      - list
      - watch
  # Rules which allow the management of Ingresses.
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - '*'
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - '*'
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules which allow the management of ClusterRoles and ClusterRoleBindings.
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - clusterroles
    verbs:
      - '*'
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:
    k8smon-alloy-operator-rolebinding
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name:
    k8smon-alloy-operator-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:
    k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:
    k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name:
    k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.0
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.1.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.1.0"
          imagePullPolicy: IfNotPresent
          args:
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            runAsNonRoot: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-metrics
  namespace: default
spec: 
  alloy:
    clustering:
      enabled: true
      name: alloy-metrics
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: statefulset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-metrics
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-logs
  namespace: default
spec: 
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: true
      extra: []
      varlog: true
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations:
    - effect: NoSchedule
      operator: Exists
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-logs
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
