// Feature: Pod Logs
declare "pod_logs" {
  argument "logs_destinations" {
    comment = "Must be a list of log destinations where collected logs should be forwarded to"
  }

  discovery.relabel "filtered_pods" {
    targets = discovery.kubernetes.pods.targets
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      action = "replace"
      target_label = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      action = "replace"
      target_label = "pod"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      action = "replace"
      target_label = "container"
    }
    rule {
      source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      action = "replace"
      replacement = "$1"
      target_label = "job"
    }

    // set the container runtime as a label
    rule {
      action = "replace"
      source_labels = ["__meta_kubernetes_pod_container_id"]
      regex = "^(\\S+):\\/\\/.+$"
      replacement = "$1"
      target_label = "tmp_container_runtime"
    }

    // make all labels on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_label_(.+)"
    }

    // make all annotations on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_(.+)"
    }

    // explicitly set service_name. if not set, loki will automatically try to populate a default.
    // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
    //
    // choose the first value found from the following ordered list:
    // - pod.annotation[resource.opentelemetry.io/service.name]
    // - pod.label[app.kubernetes.io/name]
    // - k8s.pod.name
    // - k8s.container.name
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_name",
        "__meta_kubernetes_pod_container_name",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "service_name"
    }

    // set resource attributes
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
      regex = "(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      regex = "(.+)"
      target_label = "app_kubernetes_io_name"
    }
    // add static label of integration="mimir" and instance="name" to pods that match the selector so they can be identified in the mimir.process stages
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      separator = ";"
      regex = "(?:mimir)"
      target_label = "integration"
      replacement = "mimir"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      separator = ";"
      regex = "(?:mimir)"
      target_label = "instance"
      replacement = "mimir"
    }
    // override the job label to be namespace/component so it aligns to the mimir-mixin
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name","__meta_kubernetes_namespace","__meta_kubernetes_pod_label_component"]
      separator = ";"
      regex = "(?:mimir);([^;]+);([^;]+)"
      target_label = "job"
      replacement = "$1/$2"
    }
  }

  discovery.kubernetes "pods" {
    role = "pod"
    selectors {
      role = "pod"
      field = "spec.nodeName=" + sys.env("HOSTNAME")
    }
  }

  discovery.relabel "filtered_pods_with_paths" {
    targets = discovery.relabel.filtered_pods.output

    rule {
      source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      action = "replace"
      replacement = "/var/log/pods/*$1/*.log"
      target_label = "__path__"
    }
  }

  local.file_match "pod_logs" {
    path_targets = discovery.relabel.filtered_pods_with_paths.output
  }

  loki.source.file "pod_logs" {
    targets    = local.file_match.pod_logs.targets
    forward_to = [loki.process.pod_logs.receiver]
  }

  loki.process "pod_logs" {
    stage.match {
      selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
      // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
      stage.cri {}

      // Set the extract flags and stream values as labels
      stage.labels {
        values = {
          flags  = "",
          stream  = "",
        }
      }
    }

    stage.match {
      selector = "{tmp_container_runtime=\"docker\"}"
      // the docker processing stage extracts the following k/v pairs: log, stream, time
      stage.docker {}

      // Set the extract stream value as a label
      stage.labels {
        values = {
          stream  = "",
        }
      }
    }

    // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
    // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
    // container runtime label as it is no longer needed.
    stage.label_drop {
      values = [
        "filename",
        "tmp_container_runtime",
      ]
    }
    // Integration: Mimir
    stage.match {
      selector = "{integration=\"mimir\",instance=\"mimir\"}"

      // extract some of the fields from the log line
      stage.logfmt {
        mapping = {
          "ts" = "",
          "level" = "",
        }
      }

      // set the level as a label
      stage.labels {
        values = {
          level = "level",
        }
      }
      // reset the timestamp to the extracted value
      stage.timestamp {
        source = "ts"
        format = "RFC3339Nano"
      }
      // remove the timestamp from the log line
      stage.replace {
        expression = `(?:^|\s+)(ts=\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[^ ]*\s+)`
        replace = ""
      }
      // drop certain log levels
      stage.drop {
        source = "level"
        expression = "(?i)(debug)"
        drop_counter_reason = "mimir-drop-log-level"
      }

    }

    // Only keep the labels that are defined in the `keepLabels` list.
    stage.label_keep {
      values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","service_namespace","deployment_environment","deployment_environment_name","k8s_pod_name","k8s_namespace_name","k8s_deployment_name","k8s_statefulset_name","k8s_daemonset_name","k8s_cronjob_name","k8s_job_name","k8s_node_name"]
    }

    forward_to = argument.logs_destinations.value
  }
}
pod_logs "feature" {
  logs_destinations = [
    loki.write.loki.receiver,
  ]
}




// Destination: prometheus (prometheus)
otelcol.exporter.prometheus "prometheus" {
  add_metric_suffixes = true
  forward_to = [prometheus.remote_write.prometheus.receiver]
}

prometheus.remote_write "prometheus" {
  endpoint {
    url = "http://prometheus.prometheus.svc:9090/api/v1/write"
    headers = {
    }
    tls_config {
      insecure_skip_verify = false
    }
    send_native_histograms = false

    queue_config {
      capacity = 10000
      min_shards = 1
      max_shards = 50
      max_samples_per_send = 2000
      batch_send_deadline = "5s"
      min_backoff = "30ms"
      max_backoff = "5s"
      retry_on_http_429 = true
      sample_age_limit = "0s"
    }

    write_relabel_config {
      source_labels = ["cluster"]
      regex = ""
      replacement = "mimir-integration-cluster"
      target_label = "cluster"
    }
    write_relabel_config {
      source_labels = ["k8s_cluster_name"]
      regex = ""
      replacement = "mimir-integration-cluster"
      target_label = "k8s_cluster_name"
    }
  }

  wal {
    truncate_frequency = "2h"
    min_keepalive_time = "5m"
    max_keepalive_time = "8h"
  }
}
// Destination: loki (loki)
otelcol.exporter.loki "loki" {
  forward_to = [loki.write.loki.receiver]
}

loki.write "loki" {
  endpoint {
    url = "http://loki.loki.svc:3100/api/push"
    tls_config {
      insecure_skip_verify = false
    }
    min_backoff_period = "500ms"
    max_backoff_period = "5m"
    max_backoff_retries = "10"
  }
  external_labels = {
    "cluster" = "mimir-integration-cluster",
    "k8s_cluster_name" = "mimir-integration-cluster",
  }
}
