---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.7.3
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.0.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Feature: Auto-Instrumentation
    declare "auto_instrumentation" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      discovery.kubernetes "beyla_pods" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=beyla"
        }
      }
    
      discovery.relabel "beyla_pods" {
        targets = discovery.kubernetes.beyla_pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "beyla_applications" {
        targets         = discovery.relabel.beyla_pods.output
        honor_labels    = true
        scrape_interval = "60s"
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    
      prometheus.scrape "beyla_internal" {
        targets         = discovery.relabel.beyla_pods.output
        metrics_path    = "/internal/metrics"
        job_name        = "integrations/beyla"
        honor_labels    = true
        scrape_interval = "60s"
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    auto_instrumentation "feature" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "auto-instrumentation-with-rules-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "auto-instrumentation-with-rules-cluster"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }

  self-reporting-metric.prom: |
    
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.0.0", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="autoInstrumentation", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/beyla-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.7.3
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.0.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: config
data:
  beyla-config.yml: |-
    attributes:
      kubernetes:
        cluster_name: auto-instrumentation-with-rules-cluster
        enable: true
    discovery:
      exclude_services:
      - k8s_namespace: kube-system
      services:
      - k8s_pod_labels:
          instrument: beyla
    filter:
      network:
        k8s_dst_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
        k8s_src_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
    internal_metrics:
      prometheus:
        path: /internal/metrics
        port: 9090
    prometheus_export:
      features:
      - application
      - network
      - application_service_graph
      - application_span
      path: /metrics
      port: 9090
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-editor-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-alloy-viewer-role
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys/status
    verbs:
      - get
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name:
    k8smon-alloy-operator-role
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  # Rules which allow the management of Alloys.
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups:
      - ""
    resources:
      - configmaps
      - serviceaccounts
      - services
    verbs:
      - '*'
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - statefulsets
    verbs:
      - '*'
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - '*'
  # Rules which allow for the management of Prometheus Operator objects.
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
    verbs:
      - get
      - list
      - watch
  # Rules which allow the management of Ingresses.
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - '*'
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - '*'
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules which allow the management of ClusterRoles and ClusterRoleBindings.
  - apiGroups:
      - rbac.authorization.k8s.io
    resources:
      - clusterrolebindings
      - clusterroles
    verbs:
      - '*'
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-beyla
  labels:
    helm.sh/chart: beyla-1.7.3
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.0.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
rules:
  - apiGroups: [ "apps" ]
    resources: [ "replicasets" ]
    verbs: [ "list", "watch" ]
  - apiGroups: [ "" ]
    resources: [ "pods", "services", "nodes" ]
    verbs: [ "list", "watch", "get" ]
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:
    k8smon-alloy-operator-rolebinding
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name:
    k8smon-alloy-operator-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-beyla
  labels:
    helm.sh/chart: beyla-1.7.3
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.0.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: k8smon-beyla
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-beyla
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:
    k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name:
    k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name:
    k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/daemon-set.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.7.3
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.0.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: workload
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: beyla
      app.kubernetes.io/instance: k8smon
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        k8s.grafana.com/job: default/beyla
        k8s.grafana.com/logs.job: integrations/beyla
      labels:
        helm.sh/chart: beyla-1.7.3
        app.kubernetes.io/name: beyla
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.0.3"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: beyla
        app.kubernetes.io/component: workload
    spec:
      serviceAccountName: k8smon-beyla
      hostPID: true
      containers:
        - name: beyla
          image: docker.io/grafana/beyla:2.0.3
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          ports:
          - name: metrics
            containerPort: 9090
            protocol: TCP
          env:
            - name: BEYLA_CONFIG_PATH
              value: "/etc/beyla/config/beyla-config.yml"
          volumeMounts:
            - mountPath: /etc/beyla/config
              name: beyla-config
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: beyla-config
          configMap:
            name: k8smon-beyla
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.0
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.0
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.1.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.1.0"
          imagePullPolicy: IfNotPresent
          args:
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            runAsNonRoot: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-metrics
  namespace: default
spec: 
  alloy:
    clustering:
      enabled: true
      name: alloy-metrics
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: statefulset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-metrics
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
