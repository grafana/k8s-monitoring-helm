---
# Source: k8s-monitoring/charts/alloy-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-metrics
  namespace: default
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "annotation-autodiscovery-prom-annotations-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "annotation-autodiscovery-prom-annotations-cluster"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    // Feature: Annotation Autodiscovery
    declare "annotation_autodiscovery" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
      }
    
      discovery.relabel "annotation_autodiscovery_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
          regex = "true"
          action = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
          action = "replace"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
          action = "replace"
          target_label = "instance"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
          action = "replace"
          target_label = "__metrics_path__"
        }
    
        // Choose the pod port
        // The discovery generates a target for each declared container port of the pod.
        // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
    
        // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
        // one of the declared ports on that Pod.
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
          replacement = "[$2]:$1" // IPv6
          target_label = "__address__"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
          regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
          replacement = "$2:$1"
          target_label = "__address__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
          action = "replace"
          target_label = "__scheme__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
          action = "replace"
          target_label = "__scrape_interval__"
        }
      }
    
      discovery.kubernetes "services" {
        role = "service"
      }
    
      discovery.relabel "annotation_autodiscovery_services" {
        targets = discovery.kubernetes.services.targets
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
          regex = "true"
          action = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
          action = "replace"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
          action = "replace"
          target_label = "instance"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
          action = "replace"
          target_label = "__metrics_path__"
        }
    
        // Choose the service port
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_port_number"]
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
          regex = "(.+)"
          target_label = "__tmp_port"
        }
        rule {
          source_labels = ["__meta_kubernetes_service_port_number"]
          action = "keepequal"
          target_label = "__tmp_port"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
          action = "replace"
          target_label = "__scheme__"
        }
    
        rule {
          source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
          action = "replace"
          target_label = "__scrape_interval__"
        }
      }
    
      discovery.relabel "annotation_autodiscovery_http" {
        targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
        rule {
          source_labels = ["__scheme__"]
          regex = "https"
          action = "drop"
        }
      }
    
      discovery.relabel "annotation_autodiscovery_https" {
        targets = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
        rule {
          source_labels = ["__scheme__"]
          regex = "https"
          action = "keep"
        }
      }
    
      prometheus.scrape "annotation_autodiscovery_http" {
        targets = discovery.relabel.annotation_autodiscovery_http.output
        scrape_interval = "60s"
        honor_labels = true
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        clustering {
          enabled = true
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      prometheus.scrape "annotation_autodiscovery_https" {
        targets = discovery.relabel.annotation_autodiscovery_https.output
        scrape_interval = "60s"
        honor_labels = true
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        tls_config {
          insecure_skip_verify = true
        }
        clustering {
          enabled = true
        }
    
        forward_to = argument.metrics_destinations.value
      }
    }
    annotation_autodiscovery "feature" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "1h"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.0-rc.6", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="annotationAutodiscovery", version="1.0.0"} 1
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-metrics
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-metrics
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-metrics-cluster
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  clusterIP: 'None'
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  podManagementPolicy: Parallel
  minReadySeconds: 10
  serviceName: k8smon-alloy-metrics
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-metrics
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-metrics
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-metrics
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.5.1
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --cluster.enabled=true
            - --cluster.join-addresses=k8smon-alloy-metrics-cluster
            - --cluster.name=alloy-metrics
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - SETPCAP
              - NET_BIND_SERVICE
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-metrics
