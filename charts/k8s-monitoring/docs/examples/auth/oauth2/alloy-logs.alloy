// Destination: otel-endpoint (otlp)
otelcol.receiver.prometheus "otel_endpoint" {
  output {
    metrics = [otelcol.processor.attributes.otel_endpoint.input]
  }
}
otelcol.receiver.loki "otel_endpoint" {
  output {
    logs = [otelcol.processor.attributes.otel_endpoint.input]
  }
}
otelcol.auth.oauth2 "otel_endpoint" {
  client_id = nonsensitive(remote.kubernetes.secret.otel_endpoint.data["clientId"])
  client_secret_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  endpoint_params = {
    client_assertion_type = ["urn:ietf:params:oauth:client-assertion-type:jwt-bearer"],
    grant_type = ["client_credentials"],
  }
  token_url = "https://my.idp/application/o/token/"
}

otelcol.processor.attributes "otel_endpoint" {
  output {
    metrics = [otelcol.processor.transform.otel_endpoint.input]
    logs = [otelcol.processor.transform.otel_endpoint.input]
    traces = [otelcol.processor.transform.otel_endpoint.input]
  }
}

otelcol.processor.transform "otel_endpoint" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  metric_statements {
    context = "datapoint"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }
  log_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  log_statements {
    context = "log"
    statements = [
      `delete_key(attributes, "loki.attribute.labels")`,
      `set(resource.attributes["service.name"], attributes["service_name"]) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
      `delete_key(attributes, "service_name") where attributes["service_name"] != nil`,
      `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
      `delete_key(attributes, "service_namespace") where attributes["service_namespace"] != nil`,
      `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
      `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] != nil`,
      `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
      `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] != nil`,
    ]
  }

  trace_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  output {
    metrics = [otelcol.processor.batch.otel_endpoint.input]
    logs = [otelcol.processor.batch.otel_endpoint.input]
    traces = [otelcol.processor.batch.otel_endpoint.input]
  }
}

otelcol.processor.batch "otel_endpoint" {
  timeout = "2s"
  send_batch_size = 8192
  send_batch_max_size = 0

  output {
    metrics = [otelcol.exporter.otlp.otel_endpoint.input]
    logs = [otelcol.exporter.otlp.otel_endpoint.input]
    traces = [otelcol.exporter.otlp.otel_endpoint.input]
  }
}
otelcol.exporter.otlp "otel_endpoint" {
  client {
    endpoint = "grpc.my.otel.endpoint:443"
    auth = otelcol.auth.oauth2.otel_endpoint.handler
    tls {
      insecure = false
      insecure_skip_verify = false
    }
  }
}

remote.kubernetes.secret "otel_endpoint" {
  name      = "otel-endpoint-k8smon-k8s-monitoring"
  namespace = "default"
}
// Feature: Node Logs
declare "node_logs" {
  argument "logs_destinations" {
    comment = "Must be a list of log destinations where collected logs should be forwarded to"
  }

  loki.relabel "journal" {

    // copy all journal labels and make the available to the pipeline stages as labels, there is a label
    // keep defined to filter out unwanted labels, these pipeline labels can be set as structured metadata
    // as well, the following labels are available:
    // - boot_id
    // - cap_effective
    // - cmdline
    // - comm
    // - exe
    // - gid
    // - hostname
    // - machine_id
    // - pid
    // - stream_id
    // - systemd_cgroup
    // - systemd_invocation_id
    // - systemd_slice
    // - systemd_unit
    // - transport
    // - uid
    //
    // More Info: https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html
    rule {
      action = "labelmap"
      regex = "__journal__(.+)"
    }

    rule {
      action = "replace"
      source_labels = ["__journal__systemd_unit"]
      replacement = "$1"
      target_label = "unit"
    }

    // the service_name label will be set automatically in loki if not set, and the unit label
    // will not allow service_name to be set automatically.
    rule {
      action = "replace"
      source_labels = ["__journal__systemd_unit"]
      replacement = "$1"
      target_label = "service_name"
    }

    forward_to = [] // No forward_to is used in this component, the defined rules are used in the loki.source.journal component
  }

  loki.source.journal "worker" {
    path = "/var/log/journal"
    format_as_json = false
    max_age = "8h"
    relabel_rules = loki.relabel.journal.rules
    labels = {
      job = "integrations/kubernetes/journal",
      instance = sys.env("HOSTNAME"),
    }
    forward_to = [loki.process.journal_logs.receiver]
  }

  loki.process "journal_logs" {
    stage.static_labels {
      values = {
        // add a static source label to the logs so they can be differentiated / restricted if necessary
        "source" = "journal",
        // default level to unknown
        level = "unknown",
      }
    }

    // Attempt to determine the log level, most k8s workers are either in logfmt or klog formats
    // check to see if the log line matches the klog format (https://github.com/kubernetes/klog)
    stage.match {
      // unescaped regex: ([IWED][0-9]{4}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+)
      selector = "{level=\"unknown\"} |~ \"([IWED][0-9]{4}\\\\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+)\""

      // extract log level, klog uses a single letter code for the level followed by the month and day i.e. I0119
      stage.regex {
        expression = "((?P<level>[A-Z])[0-9])"
      }

      // if the extracted level is I set INFO
      stage.replace {
        source = "level"
        expression = "(I)"
        replace = "INFO"
      }

      // if the extracted level is W set WARN
      stage.replace {
        source = "level"
        expression = "(W)"
        replace = "WARN"
      }

      // if the extracted level is E set ERROR
      stage.replace {
        source = "level"
        expression = "(E)"
        replace = "ERROR"
      }

      // if the extracted level is I set INFO
      stage.replace {
        source = "level"
        expression = "(D)"
        replace = "DEBUG"
      }

      // set the extracted level to be a label
      stage.labels {
        values = {
          level = "",
        }
      }
    }

    // if the level is still unknown, do one last attempt at detecting it based on common levels
    stage.match {
      selector = "{level=\"unknown\"}"

      // unescaped regex: (?i)(?:"(?:level|loglevel|levelname|lvl|levelText|SeverityText)":\s*"|\s*(?:level|loglevel|levelText|lvl)="?|\s+\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))("|\s+|-|\s*\])
      stage.regex {
        expression = "(?i)(?:\"(?:level|loglevel|levelname|lvl|levelText|SeverityText)\":\\s*\"|\\s*(?:level|loglevel|levelText|lvl)=\"?|\\s+\\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))(\"|\\s+|-|\\s*\\])"
      }

      // set the extracted level to be a label
      stage.labels {
        values = {
          level = "",
        }
      }
    }

    // Only keep the labels that are defined in the `keepLabels` list.
    stage.label_keep {
      values = ["instance","job","level","name","unit","service_name","source"]
    }

    forward_to = argument.logs_destinations.value
  }
}
node_logs "feature" {
  logs_destinations = [
    otelcol.receiver.loki.otel_endpoint.receiver,
  ]
}
// Feature: Pod Logs
declare "pod_logs" {
  argument "logs_destinations" {
    comment = "Must be a list of log destinations where collected logs should be forwarded to"
  }

  discovery.kubernetes "volume_gathering_pods" {
    role = "pod"
    selectors {
      role = "pod"
      field = "spec.nodeName=" + sys.env("HOSTNAME")
    }
  }

  discovery.relabel "volume_gathering_pods" {
    targets = discovery.kubernetes.volume_gathering_pods.targets
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      action = "replace"
      target_label = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label = "pod"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label = "container"
    }

    rule {
      source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      replacement = "$1"
      target_label = "job"
    }

    // set the container runtime as a label
    rule {
      source_labels = ["__meta_kubernetes_pod_container_id"]
      regex = "^(\\S+):\\/\\/.+$"
      target_label = "tmp_container_runtime"
    }

    // make all labels on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_label_(.+)"
    }

    // make all annotations on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_(.+)"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      target_label = "app_kubernetes_io_name"
    }

    // explicitly set service_name. if not set, loki will automatically try to populate a default.
    // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
    //
    // choose the first value found from the following ordered list:
    // - pod.annotation[resource.opentelemetry.io/service.name]
    // - pod.label[app.kubernetes.io/name]
    // - k8s.pod.name
    // - k8s.container.name
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_name",
        "__meta_kubernetes_pod_container_name",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "service_name"
    }

    // set resource attributes
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      replacement = "/var/log/pods/*$1/*.log"
      target_label = "__path__"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
      regex = "(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      regex = "(.+)"
      target_label = "app_kubernetes_io_name"
    }
  }

  local.file_match "volume_gathering_pods" {
    path_targets = discovery.relabel.volume_gathering_pods.output
  }

  loki.source.file "volume_gathering_pods" {
    targets    = local.file_match.volume_gathering_pods.targets
    forward_to = [loki.process.pod_log_processor.receiver]
  }

  loki.process "pod_log_processor" {
    stage.match {
      selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
      // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
      stage.cri {}

      // Set the extract flags and stream values as labels
      stage.labels {
        values = {
          flags  = "",
          stream  = "",
        }
      }
    }

    stage.match {
      selector = "{tmp_container_runtime=\"docker\"}"
      // the docker processing stage extracts the following k/v pairs: log, stream, time
      stage.docker {}

      // Set the extract stream value as a label
      stage.labels {
        values = {
          stream  = "",
        }
      }
    }

    // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
    // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
    // container runtime label as it is no longer needed.
    stage.label_drop {
      values = [
        "filename",
        "tmp_container_runtime",
      ]
    }

    // Only keep the labels that are defined in the `keepLabels` list.
    stage.label_keep {
      values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","service_namespace","deployment_environment","deployment_environment_name","integration"]
    }

    forward_to = argument.logs_destinations.value
  }
}
pod_logs "feature" {
  logs_destinations = [
    otelcol.receiver.loki.otel_endpoint.receiver,
  ]
}
