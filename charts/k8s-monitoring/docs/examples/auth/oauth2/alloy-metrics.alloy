// Feature: Annotation Autodiscovery
declare "annotation_autodiscovery" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  discovery.kubernetes "pods" {
    role = "pod"
  }

  discovery.relabel "annotation_autodiscovery_pods" {
    targets = discovery.kubernetes.pods.targets
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
      regex = "true"
      action = "keep"
    }
    // Only keep pods that are running, ready, and not init containers.
    rule {
      source_labels = [
        "__meta_kubernetes_pod_phase",
        "__meta_kubernetes_pod_ready",
        "__meta_kubernetes_pod_container_init",
      ]
      regex = "Running;true;false"
      action = "keep"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label = "pod"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label = "container"
    }
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
      target_label = "job"
    }
    rule {
      source_labels = ["job", "__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["job", "__meta_kubernetes_pod_label_app"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["job", "container"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
      target_label = "instance"
    }

    // Rules to choose the right container
    rule {
      source_labels = ["container"]
      target_label = "__tmp_container"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_container"]
      regex = "(.+)"
      target_label = "__tmp_container"
    }
    rule {
      source_labels = ["container"]
      action = "keepequal"
      target_label = "__tmp_container"
    }
    rule {
      action = "labeldrop"
      regex = "__tmp_container"
    }

    // Set metrics path
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
      regex = "(.+)"
      target_label = "__metrics_path__"
    }

    // Set metrics scraping URL parameters
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_param_(.+)"
      replacement = "__param_$1"
    }

    // Choose the pod port
    // The discovery generates a target for each declared container port of the pod.
    // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      action = "keepequal"
      target_label = "__tmp_port"
    }
    rule {
      action = "labeldrop"
      regex = "__tmp_port"
    }

    // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
    // one of the declared ports on that Pod.
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
      regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
      replacement = "[$2]:$1" // IPv6
      target_label = "__address__"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
      regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
      replacement = "$2:$1"
      target_label = "__address__"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
      regex = "(.+)"
      target_label = "__scheme__"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
      regex = "(.+)"
      target_label = "__scrape_interval__"
    }
    rule {
      source_labels = ["__scrape_interval__"]
      regex = ""
      replacement = "60s"
      target_label = "__scrape_interval__"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeTimeout"]
      regex = "(.+)"
      target_label = "__scrape_timeout__"
    }
    rule {
      source_labels = ["__scrape_timeout__"]
      regex = ""
      replacement = "10s"
      target_label = "__scrape_timeout__"
    }
  }

  discovery.kubernetes "services" {
    role = "service"
  }

  discovery.relabel "annotation_autodiscovery_services" {
    targets = discovery.kubernetes.services.targets
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
      regex = "true"
      action = "keep"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_name"]
      target_label = "service"
    }
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
      target_label = "job"
    }
    rule {
      source_labels = ["job", "__meta_kubernetes_service_label_app_kubernetes_io_name"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["job", "__meta_kubernetes_service_label_app"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["job", "service"]
      regex = ";(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
      target_label = "instance"
    }

    // Set metrics path
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
      target_label = "__metrics_path__"
    }

    // Set metrics scraping URL parameters
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_param_(.+)"
      replacement = "__param_$1"
    }

    // Choose the service port
    rule {
      source_labels = ["__meta_kubernetes_service_port_name"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_port_name"]
      action = "keepequal"
      target_label = "__tmp_port"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_port_number"]
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
      regex = "(.+)"
      target_label = "__tmp_port"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_port_number"]
      action = "keepequal"
      target_label = "__tmp_port"
    }
    rule {
      action = "labeldrop"
      regex = "__tmp_port"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
      regex = "(.+)"
      target_label = "__scheme__"
    }

    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
      regex = "(.+)"
      target_label = "__scrape_interval__"
    }
    rule {
      source_labels = ["__scrape_interval__"]
      regex = ""
      replacement = "60s"
      target_label = "__scrape_interval__"
    }
    rule {
      source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeTimeout"]
      regex = "(.+)"
      target_label = "__scrape_timeout__"
    }
    rule {
      source_labels = ["__scrape_timeout__"]
      regex = ""
      replacement = "10s"
      target_label = "__scrape_timeout__"
    }
  }

  discovery.relabel "annotation_autodiscovery_http" {
    targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
    rule {
      source_labels = ["__scheme__"]
      regex = "https"
      action = "drop"
    }
  }

  discovery.relabel "annotation_autodiscovery_https" {
    targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
    rule {
      source_labels = ["__scheme__"]
      regex = "https"
      action = "keep"
    }
  }

  prometheus.scrape "annotation_autodiscovery_http" {
    targets = discovery.relabel.annotation_autodiscovery_http.output
    honor_labels = true
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    clustering {
      enabled = true
    }

    forward_to = argument.metrics_destinations.value
  }

  prometheus.scrape "annotation_autodiscovery_https" {
    targets = discovery.relabel.annotation_autodiscovery_https.output
    honor_labels = true
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    tls_config {
      insecure_skip_verify = true
    }
    clustering {
      enabled = true
    }

    forward_to = argument.metrics_destinations.value
  }
}
annotation_autodiscovery "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otel_endpoint.receiver,
  ]
}
// Feature: Cluster Metrics
declare "cluster_metrics" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }
  discovery.kubernetes "nodes" {
    role = "node"
  }

  discovery.relabel "nodes" {
    targets = discovery.kubernetes.nodes.targets
    rule {
      source_labels = ["__meta_kubernetes_node_name"]
      target_label  = "node"
    }

    rule {
      replacement = "kubernetes"
      target_label = "source"
    }

  }

  // Kubelet
  discovery.relabel "kubelet" {
    targets = discovery.relabel.nodes.output
  }

  prometheus.scrape "kubelet" {
    targets  = discovery.relabel.kubelet.output
    job_name = "integrations/kubernetes/kubelet"
    scheme   = "https"
    scrape_interval = "60s"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.kubelet.receiver]
  }

  prometheus.relabel "kubelet" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
      action = "keep"
    }

    forward_to = argument.metrics_destinations.value
  }

  // Kubelet Resources
  discovery.relabel "kubelet_resources" {
    targets = discovery.relabel.nodes.output
    rule {
      replacement   = "/metrics/resource"
      target_label  = "__metrics_path__"
    }
  }

  prometheus.scrape "kubelet_resources" {
    targets = discovery.relabel.kubelet_resources.output
    job_name = "integrations/kubernetes/resources"
    scheme   = "https"
    scrape_interval = "60s"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.kubelet_resources.receiver]
  }

  prometheus.relabel "kubelet_resources" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
      action = "keep"
    }

    forward_to = argument.metrics_destinations.value
  }

  // cAdvisor
  discovery.relabel "cadvisor" {
    targets = discovery.relabel.nodes.output
    rule {
      replacement   = "/metrics/cadvisor"
      target_label  = "__metrics_path__"
    }
  }

  prometheus.scrape "cadvisor" {
    targets = discovery.relabel.cadvisor.output
    job_name = "integrations/kubernetes/cadvisor"
    scheme = "https"
    scrape_interval = "60s"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.cadvisor.receiver]
  }

  prometheus.relabel "cadvisor" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
      action = "keep"
    }
    // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","container"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
      action = "drop"
    }
    // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","image"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
      action = "drop"
    }
    // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
    rule {
      source_labels = ["__name__", "boot_id"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "boot_id"
      replacement = "NA"
    }
    rule {
      source_labels = ["__name__", "system_uuid"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "system_uuid"
      replacement = "NA"
    }
    // Filter out non-physical devices/interfaces
    rule {
      source_labels = ["__name__", "device"]
      separator = "@"
      regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_fs_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_fs_.*"
      target_label = "__keepme"
      replacement = ""
    }
    rule {
      source_labels = ["__name__", "interface"]
      separator = "@"
      regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_network_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_network_.*"
      target_label = "__keepme"
      replacement = ""
    }
    forward_to = argument.metrics_destinations.value
  }
  discovery.kubernetes "kube_state_metrics" {
    role = "endpoints"

    selectors {
      role = "endpoints"
      label = "app.kubernetes.io/name=kube-state-metrics,release=k8smon"
    }
    namespaces {
      names = ["default"]
    }
  }

  discovery.relabel "kube_state_metrics" {
    targets = discovery.kubernetes.kube_state_metrics.targets

    // only keep targets with a matching port name
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      regex = "http"
      action = "keep"
    }

    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }

  }

  prometheus.scrape "kube_state_metrics" {
    targets = discovery.relabel.kube_state_metrics.output
    job_name = "integrations/kubernetes/kube-state-metrics"
    scrape_interval = "60s"
    scheme = "http"
    bearer_token_file = ""
    tls_config {
      insecure_skip_verify = true
    }

    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.kube_state_metrics.receiver]
  }

  prometheus.relabel "kube_state_metrics" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|kube_configmap_info|kube_configmap_metadata_resource_version|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }

  // Node Exporter
  discovery.kubernetes "node_exporter" {
    role = "pod"

    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=node-exporter,release=k8smon"
    }
    namespaces {
      names = ["default"]
    }
  }

  discovery.relabel "node_exporter" {
    targets = discovery.kubernetes.node_exporter.targets

    // keep only the specified metrics port name, and pods that are Running and ready
    rule {
      source_labels = [
        "__meta_kubernetes_pod_container_port_name",
        "__meta_kubernetes_pod_container_init",
        "__meta_kubernetes_pod_phase",
        "__meta_kubernetes_pod_ready",
      ]
      separator = "@"
      regex = "metrics@false@Running@true"
      action = "keep"
    }

    // Set the instance label to the node name
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }

    // set the namespace label
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    // set the pod label
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    // set the container label
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set a workload label
    rule {
      source_labels = [
        "__meta_kubernetes_pod_controller_kind",
        "__meta_kubernetes_pod_controller_name",
      ]
      separator = "/"
      target_label  = "workload"
    }
    // remove the hash from the ReplicaSet
    rule {
      source_labels = ["workload"]
      regex = "(ReplicaSet/.+)-.+"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
  }

  prometheus.scrape "node_exporter" {
    targets = discovery.relabel.node_exporter.output
    job_name = "integrations/node_exporter"
    scrape_interval = "60s"
    scheme = "http"
    bearer_token_file = ""
    tls_config {
      insecure_skip_verify = true
    }

    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.node_exporter.receiver]
  }

  prometheus.relabel "node_exporter" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
      action = "keep"
    }
    // Drop metrics for certain file systems
    rule {
      source_labels = ["__name__", "fstype"]
      separator = "@"
      regex = "node_filesystem.*@(ramfs|tmpfs)"
      action = "drop"
    }
    forward_to = argument.metrics_destinations.value
  }

  discovery.kubernetes "windows_exporter_pods" {
    role = "pod"
    namespaces {
      names = ["default"]
    }
    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=windows-exporter,release=k8smon"
    }
  }

  discovery.relabel "windows_exporter" {
    targets = discovery.kubernetes.windows_exporter_pods.targets
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }

  prometheus.scrape "windows_exporter" {
    job_name   = "integrations/windows-exporter"
    targets  = discovery.relabel.windows_exporter.output
    scrape_interval = "60s"
    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.windows_exporter.receiver]
  }

  prometheus.relabel "windows_exporter" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|windows_.*|node_cpu_seconds_total|node_filesystem_size_bytes|node_filesystem_avail_bytes|container_cpu_usage_seconds_total"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }
}
cluster_metrics "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otel_endpoint.receiver,
  ]
}
// Feature: Prometheus Operator Objects
declare "prometheus_operator_objects" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  // Prometheus Operator PodMonitor objects
  prometheus.operator.podmonitors "pod_monitors" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }

  // Prometheus Operator Probe objects
  prometheus.operator.probes "probes" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }

  // Prometheus Operator ServiceMonitor objects
  prometheus.operator.servicemonitors "service_monitors" {
    clustering {
      enabled = true
    }
    scrape {
      default_scrape_interval = "60s"
    }
    forward_to = argument.metrics_destinations.value
  }
}
prometheus_operator_objects "feature" {
  metrics_destinations = [
    otelcol.receiver.prometheus.otel_endpoint.receiver,
  ]
}




// Destination: otel-endpoint (otlp)
otelcol.receiver.prometheus "otel_endpoint" {
  output {
    metrics = [otelcol.processor.attributes.otel_endpoint.input]
  }
}
otelcol.receiver.loki "otel_endpoint" {
  output {
    logs = [otelcol.processor.attributes.otel_endpoint.input]
  }
}

otelcol.processor.attributes "otel_endpoint" {
  output {
    metrics = [otelcol.processor.transform.otel_endpoint.input]
    logs = [otelcol.processor.transform.otel_endpoint.input]
    traces = [otelcol.processor.transform.otel_endpoint.input]
  }
}

otelcol.processor.transform "otel_endpoint" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  metric_statements {
    context = "datapoint"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
      `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
      `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
      `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
      `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
      `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
      `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
      `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
      `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
    ]
  }
  log_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  log_statements {
    context = "log"
    statements = [
      `delete_key(attributes, "loki.attribute.labels")`,
      `delete_key(attributes, "loki.resource.labels")`,
      `set(resource.attributes["k8s.container.name"], attributes["container"] ) where resource.attributes["k8s.container.name"] == nil and attributes["container"] != nil`,
      `delete_key(attributes, "container") where attributes["container"] == resource.attributes["k8s.container.name"]`,
      `set(resource.attributes["k8s.cronjob.name"], attributes["cronjob"] ) where resource.attributes["k8s.cronjob.name"] == nil and attributes["cronjob"] != nil`,
      `delete_key(attributes, "cronjob") where attributes["cronjob"] == resource.attributes["k8s.cronjob.name"]`,
      `set(resource.attributes["k8s.daemonset.name"], attributes["daemonset"] ) where resource.attributes["k8s.daemonset.name"] == nil and attributes["daemonset"] != nil`,
      `delete_key(attributes, "daemonset") where attributes["daemonset"] == resource.attributes["k8s.daemonset.name"]`,
      `set(resource.attributes["k8s.deployment.name"], attributes["deployment"] ) where resource.attributes["k8s.deployment.name"] == nil and attributes["deployment"] != nil`,
      `delete_key(attributes, "deployment") where attributes["deployment"] == resource.attributes["k8s.deployment.name"]`,
      `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
      `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
      `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
      `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
      `set(resource.attributes["k8s.job.name"], attributes["job_name"] ) where resource.attributes["k8s.job.name"] == nil and attributes["job_name"] != nil`,
      `delete_key(attributes, "job_name") where attributes["job_name"] == resource.attributes["k8s.job.name"]`,
      `set(resource.attributes["k8s.namespace.name"], attributes["namespace"] ) where resource.attributes["k8s.namespace.name"] == nil and attributes["namespace"] != nil`,
      `delete_key(attributes, "namespace") where attributes["namespace"] == resource.attributes["k8s.namespace.name"]`,
      `set(resource.attributes["k8s.pod.name"], attributes["pod"] ) where resource.attributes["k8s.pod.name"] == nil and attributes["pod"] != nil`,
      `delete_key(attributes, "pod") where attributes["pod"] == resource.attributes["k8s.pod.name"]`,
      `set(resource.attributes["k8s.replicaset.name"], attributes["replicaset"] ) where resource.attributes["k8s.replicaset.name"] == nil and attributes["replicaset"] != nil`,
      `delete_key(attributes, "replicaset") where attributes["replicaset"] == resource.attributes["k8s.replicaset.name"]`,
      `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
      `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
      `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
      `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
      `set(resource.attributes["k8s.statefulset.name"], attributes["statefulset"] ) where resource.attributes["k8s.statefulset.name"] == nil and attributes["statefulset"] != nil`,
      `delete_key(attributes, "statefulset") where attributes["statefulset"] == resource.attributes["k8s.statefulset.name"]`,
    ]
  }

  trace_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "oauth2-auth-example")`,
      `set(attributes["k8s.cluster.name"], "oauth2-auth-example")`,
    ]
  }

  output {
    metrics = [otelcol.processor.batch.otel_endpoint.input]
    logs = [otelcol.processor.batch.otel_endpoint.input]
    traces = [otelcol.processor.batch.otel_endpoint.input]
  }
}

otelcol.processor.batch "otel_endpoint" {
  timeout = "2s"
  send_batch_size = 8192
  send_batch_max_size = 0

  output {
    metrics = [otelcol.exporter.otlp.otel_endpoint.input]
    logs = [otelcol.exporter.otlp.otel_endpoint.input]
    traces = [otelcol.exporter.otlp.otel_endpoint.input]
  }
}
otelcol.exporter.otlp "otel_endpoint" {
  client {
    endpoint = "grpc.my.otel.endpoint:443"
    auth = otelcol.auth.oauth2.otel_endpoint.handler
    tls {
      insecure = false
      insecure_skip_verify = false
    }
  }

  retry_on_failure {
    enabled = true
    initial_interval = "5s"
    max_interval = "30s"
    max_elapsed_time = "5m"
  }
}

otelcol.auth.oauth2 "otel_endpoint" {
  client_id = convert.nonsensitive(remote.kubernetes.secret.otel_endpoint.data["clientId"])
  client_secret_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
  endpoint_params = {
    client_assertion_type = ["urn:ietf:params:oauth:client-assertion-type:jwt-bearer"],
    grant_type = ["client_credentials"],
  }
  token_url = "https://my.idp/application/o/token/"
}

remote.kubernetes.secret "otel_endpoint" {
  name      = "otel-endpoint-k8smon-k8s-monitoring"
  namespace = "default"
}

