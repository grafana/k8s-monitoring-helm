---
# Source: k8s-monitoring/charts/alloy-singleton/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-singleton
  namespace: default
  labels:
    helm.sh/chart: alloy-singleton-0.11.0
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.6.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.28.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.14.0"
    release: k8smon
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.43.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.8.2"
    release: k8smon
automountServiceAccountToken: false
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-singleton
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus (prometheus)
    otelcol.exporter.prometheus "prometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "loki-meta-monitoring-cluster"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "loki-meta-monitoring-cluster"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    // Destination: loki (loki)
    otelcol.exporter.loki "loki" {
      forward_to = [loki.write.loki.receiver]
    }
    
    loki.write "loki" {
      endpoint {
        url = "http://loki.loki.svc:3100/api/push"
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "loki-meta-monitoring-cluster",
        "k8s_cluster_name" = "loki-meta-monitoring-cluster",
      }
    }
    
    // Feature: Cluster Metrics
    declare "cluster_metrics" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      
      remote.kubernetes.configmap "kubernetes" {
        name = "k8smon-alloy-module-kubernetes"
        namespace = "default"
      }
      
      import.string "kubernetes" {
        content = remote.kubernetes.configmap.kubernetes.data["core_metrics.alloy"]
      }      
      
      kubernetes.cadvisor "scrape" {
        clustering = true
        job_label = "integrations/kubernetes/cadvisor"
        keep_metrics = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = [prometheus.relabel.cadvisor.receiver]
      }
      
      prometheus.relabel "cadvisor" {
        max_cache_size = 100000
        // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","container"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
          action = "drop"
        }
        // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","image"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
          action = "drop"
        }
        // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
        rule {
          source_labels = ["__name__", "boot_id"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "boot_id"
          replacement = "NA"
        }
        rule {
          source_labels = ["__name__", "system_uuid"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "system_uuid"
          replacement = "NA"
        }
        // Filter out non-physical devices/interfaces
        rule {
          source_labels = ["__name__", "device"]
          separator = "@"
          regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_fs_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_fs_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          source_labels = ["__name__", "interface"]
          separator = "@"
          regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_network_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_network_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          action = "keep"
          source_labels = ["namespace"]
          regex = "logs|metrics|o11y"
        }
        forward_to = argument.metrics_destinations.value
      }            
      
      remote.kubernetes.configmap "kube_state_metrics" {
        name = "k8smon-alloy-module-kubernetes"
        namespace = "default"
      }
      
      import.string "kube_state_metrics" {
        content = remote.kubernetes.configmap.kube_state_metrics.data["kube-state-metrics_metrics.alloy"]
      }
      
      kube_state_metrics.kubernetes "targets" {
        namespaces = ["default"]
        port_name = "http"
        label_selectors = [
          "app.kubernetes.io/name=kube-state-metrics",
          "release=k8smon",
        ]
      }
      
      kube_state_metrics.scrape "metrics" {
        targets = kube_state_metrics.kubernetes.targets.output
        clustering = true
        job_label = "integrations/kubernetes/kube-state-metrics"
        keep_metrics = "up|scrape_samples_scraped|(.+)"
        scheme = "http"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = [prometheus.relabel.kube_state_metrics.receiver]
      }
      
      prometheus.relabel "kube_state_metrics" {
        max_cache_size = 100000
        
        rule {
          action = "keep"
          source_labels = ["namespace"]
          regex = "logs|metrics|o11y"
        }
        forward_to = argument.metrics_destinations.value
      }  
      
      remote.kubernetes.configmap "node_exporter" {
        name = "k8smon-alloy-module-system"
        namespace = "default"
      }
      
      import.string "node_exporter" {
        content = remote.kubernetes.configmap.node_exporter.data["node-exporter_metrics.alloy"]
      }
      
      node_exporter.kubernetes "targets" {
        namespaces = ["default"]
        port_name = "metrics"
        label_selectors = [
          "app.kubernetes.io/name=node-exporter",
          "release=k8smon",
        ]
      }
      
      discovery.relabel "node_exporter" {
        targets = node_exporter.kubernetes.targets.output
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
      
      node_exporter.scrape "metrics" {
        targets = discovery.relabel.node_exporter.output
        job_label = "integrations/node_exporter"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
        scheme = "http"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }      
    }
    cluster_metrics "feature" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    // Feature: Cluster Events
    declare "cluster_events" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.source.kubernetes_events "cluster_events" {
        job_name   = "integrations/kubernetes/eventhandler"
        log_format = "logfmt"
        namespaces = ["logs","metrics","o11y"]
        forward_to = [loki.process.cluster_events.receiver]
      }
    
      loki.process "cluster_events" {
    
        // add a static source label to the logs so they can be differentiated / restricted if necessary
        stage.static_labels {
          values = {
            "source" = "kubernetes-events",
          }
        }
    
        // extract some of the fields from the log line, these could be used as labels, structured metadata, etc.
        stage.logfmt {
          mapping = {
            "component" = "sourcecomponent", // map the sourcecomponent field to component
            "kind" = "",
            "level" = "type", // most events don't have a level but they do have a "type" i.e. Normal, Warning, Error, etc.
            "name" = "",
            "node" = "sourcehost", // map the sourcehost field to node
          }
        }
        // set these values as labels, they may or may not be used as index labels in Loki as they can be dropped
        // prior to being written to Loki, but this makes them available
        stage.labels {
          values = {
            "component" = "",
            "kind" = "",
            "level" = "",
            "name" = "",
            "node" = "",
          }
        }
    
        // if kind=Node, set the node label by copying the instance label
        stage.match {
          selector = "{kind=\"Node\"}"
    
          stage.labels {
            values = {
              "node" = "name",
            }
          }
        }
    
        // set the level extracted key value as a normalized log level
        stage.match {
          selector = "{level=\"Normal\"}"
    
          stage.static_labels {
            values = {
              level = "Info",
            }
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["job","level","namespace","node","source"]
        }
        forward_to = argument.logs_destinations.value
      }
    }
    cluster_events "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
    
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
      
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
      
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
      
        // set the job label from the k8s.grafana.com/logs.job annotation if it exists
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
      
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
      
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:o11y);(?:grafana)"
          target_label = "job"
          replacement = "integrations/grafana"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:o11y);(?:grafana)"
          target_label = "instance"
          replacement = "grafana"
        }
        
        // add static label of integration="loki" and instance="name" to pods that match the selector so they can be identified in the loki.process stages
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:logs);(?:loki)"
          target_label = "integration"
          replacement = "loki"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:logs);(?:loki)"
          target_label = "instance"
          replacement = "loki"
        }
        // override the job label to be namespace/component so it aligns to the loki-mixin
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name","__meta_kubernetes_namespace","__meta_kubernetes_pod_label_component"]
          separator = ";"
          regex = "(?:logs);(?:loki);([^;]+);([^;]+)"
          target_label = "job"
          replacement = "$1/$2"
        }
        
        // add static label of integration="mimir" and instance="name" to pods that match the selector so they can be identified in the mimir.process stages
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:metrics);(?:mimir)"
          target_label = "integration"
          replacement = "mimir"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          separator = ";"
          regex = "(?:metrics);(?:mimir)"
          target_label = "instance"
          replacement = "mimir"
        }
        // override the job label to be namespace/component so it aligns to the mimir-mixin
        rule {
          source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_name","__meta_kubernetes_namespace","__meta_kubernetes_pod_label_component"]
          separator = ";"
          regex = "(?:metrics);(?:mimir);([^;]+);([^;]+)"
          target_label = "job"
          replacement = "$1/$2"
        }
      }
      
      discovery.kubernetes "pods" {
        role = "pod"
        namespaces {
          names = ["logs","metrics","o11y"]
        }
      }
      
      loki.source.kubernetes "pod_logs" {
        targets = discovery.relabel.filtered_pods.output
        forward_to = [loki.process.pod_logs.receiver]
      }
      
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
      
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
      
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
      
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
      
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
      
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","integration"]
        }
        // Integration: Loki
        stage.match {
          selector = "{job=\"integrations/grafana\",instance=\"grafana\",namespace=~\"o11y\"}"
        
          // extract some of the fields from the log line
          stage.logfmt {
            mapping = {
              "timestamp" = "t",
              "level" = "",
              "logger" = "",
              "type" = "",
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        
          // set the level as a label
          stage.labels {
            values = {
              level = "level",
            }
          }
          // reset the timestamp to the extracted value
          stage.timestamp {
            source = "timestamp"
            format = "RFC3339Nano"
          }
          // remove the timestamp from the log line
          stage.replace {
            expression = "( t=[^ ]+\\s+)"
            replace = ""
          }
          // set the structured metadata values
          stage.structured_metadata {
            values = {
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        } 
         
        // Integration: Loki
        stage.match {
          selector = "{integration=\"loki\",instance=\"loki\",namespace=~\"logs\"}"
        
          // extract some of the fields from the log line
          stage.logfmt {
            mapping = {
              "ts" = "",
              "level" = "",
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        
          // set the level as a label
          stage.labels {
            values = {
              level = "level",
            }
          }
          // reset the timestamp to the extracted value
          stage.timestamp {
            source = "ts"
            format = "RFC3339Nano"
          }
          // remove the timestamp from the log line
          stage.replace {
            expression = "(ts=[^ ]+\\s+)"
            replace = ""
          }
          // clean up the caller to remove the line
          stage.replace {
            source = "caller"
            expression = "(:[0-9]+$)"
            replace = ""
          }
          // set the structured metadata values
          stage.structured_metadata {
            values = {
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        
        } 
         
        // Integration: Mimir
        stage.match {
          selector = "{integration=\"mimir\",instance=\"mimir\",namespace=~\"metrics\"}"
        
          // extract some of the fields from the log line
          stage.logfmt {
            mapping = {
              "ts" = "",
              "level" = "",
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        
          // set the level as a label
          stage.labels {
            values = {
              level = "level",
            }
          }
          // reset the timestamp to the extracted value
          stage.timestamp {
            source = "ts"
            format = "RFC3339Nano"
          }
          // remove the timestamp from the log line
          stage.replace {
            expression = "(ts=[^ ]+\\s+)"
            replace = ""
          }
          // clean up the caller to remove the line
          stage.replace {
            source = "caller"
            expression = "(:[0-9]+$)"
            replace = ""
          }
          // set the structured metadata values
          stage.structured_metadata {
            values = {
              "caller" = "caller",
              "org_id" = "org_id",
              "tenant" = "tenant",
              "user" = "user",
            }
          }
        
        }
      
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.loki.receiver,
      ]
    }
    
    declare "alloy_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "alloy_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=alloy\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: http-metrics)"
          optional = true
        }
    
        // Alloy service discovery for all of the pods
        discovery.kubernetes "alloy_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=alloy"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
        }
    
        // alloy relabelings (pre-scrape)
        discovery.relabel "alloy_pods" {
          targets = discovery.kubernetes.alloy_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "metrics") + "@Running@true@false"
            action = "keep"
          }
    
          
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        // set the workload to the controller kind and name
        rule {
          action = "lowercase"
          source_labels = ["__meta_kubernetes_pod_controller_kind"]
          target_label  = "workload_type"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_controller_name"]
          target_label  = "workload"
        }
        
        // remove the hash from the ReplicaSet
        rule {
          source_labels = [
            "workload_type",
            "workload",
          ]
          separator = "/"
          regex = "replicaset/(.+)-.+$"
          target_label  = "workload"
        }
        
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
        
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
        
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
        }
    
        export "output" {
          value = discovery.relabel.alloy_pods.output
        }
      }
    
      declare "alloy_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Alloy metrics (default: integrations/alloy)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "alloy" {
          job_name = coalesce(argument.job_label.value, "integrations/alloy")
          forward_to = [prometheus.relabel.alloy.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // alloy metric relabelings (post-scrape)
        prometheus.relabel "alloy" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "")
            action = "drop"
          }
    
          // keep only metrics that match the keep_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.keep_metrics.value, ".*")
            action = "keep"
          }
    
          // remove the component_id label from any metric that starts with log_bytes or log_lines, these are custom metrics that are generated
          // as part of the log annotation modules in this repo
          rule {
            action = "replace"
            source_labels = ["__name__"]
            regex = "^log_(bytes|lines).+"
            replacement = ""
            target_label = "component_id"
          }
    
          // set the namespace label to that of the exported_namespace
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_namespace"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "namespace"
          }
    
          // set the pod label to that of the exported_pod
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_pod"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "pod"
          }
    
          // set the container label to that of the exported_container
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_container"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "container"
          }
    
          // set the job label to that of the exported_job
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_job"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "job"
          }
    
          // set the instance label to that of the exported_instance
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_instance"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "instance"
          }
    
          rule {
            action = "labeldrop"
            regex = "exported_(namespace|pod|container|job|instance)"
          }
        }
      }
      
      alloy_integration_discovery "alloy_in_logs" {
        port_name = "grafana"
        namespaces = ["logs"]
        label_selectors = ["app.kubernetes.io/name=alloy-singleton"]
      }
      
      alloy_integration_scrape  "alloy_in_logs" {
        targets = alloy_integration_discovery.alloy_in_logs.output
        job_label = "integrations/grafana"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
      
      alloy_integration_discovery "alloy" {
        port_name = "http-metrics"
        namespaces = ["collectors"]
        label_selectors = ["app.kubernetes.io/name in (alloy-singleton,alloy-metrics,alloy-logs,alloy-profiles,alloy-receiver)"]
      }
      
      alloy_integration_scrape  "alloy" {
        targets = alloy_integration_discovery.alloy.output
        job_label = "integrations/alloy"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    alloy_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    declare "grafana_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "grafana_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=grafana\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: grafana)"
          optional = true
        }
    
        // grafana service discovery for all of the pods
        discovery.kubernetes "grafana_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=grafana"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
        }
    
        // grafana relabelings (pre-scrape)
        discovery.relabel "grafana_pods" {
          targets = discovery.kubernetes.grafana_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "grafana") + "@Running@true@false"
            action = "keep"
          }
    
          
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        // set the workload to the controller kind and name
        rule {
          action = "lowercase"
          source_labels = ["__meta_kubernetes_pod_controller_kind"]
          target_label  = "workload_type"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_controller_name"]
          target_label  = "workload"
        }
        
        // remove the hash from the ReplicaSet
        rule {
          source_labels = [
            "workload_type",
            "workload",
          ]
          separator = "/"
          regex = "replicaset/(.+)-.+$"
          target_label  = "workload"
        }
        
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
        
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
        
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
        }
    
        export "output" {
          value = discovery.relabel.grafana_pods.output
        }
      }
    
      declare "grafana_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Loki metrics (default: integrations/grafana)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "grafana" {
          job_name = coalesce(argument.job_label.value, "integrations/grafana")
          forward_to = [prometheus.relabel.grafana.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // grafana metric relabelings (post-scrape)
        prometheus.relabel "grafana" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
            action = "drop"
          }
        }
      }
      
      grafana_integration_discovery "grafana" {
        namespaces = ["o11y"]
        label_selectors = ["app.kubernetes.io/name=grafana"]
        port_name = "grafana"
      }
      
      grafana_integration_scrape  "grafana" {
        targets = grafana_integration_discovery.grafana.output
        job_label = "integrations/grafana"
        clustering = true
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    grafana_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    declare "loki_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "loki_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=loki\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: http-metrics)"
          optional = true
        }
    
        // loki service discovery for all of the pods
        discovery.kubernetes "loki_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=loki"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
        }
    
        // loki relabelings (pre-scrape)
        discovery.relabel "loki_pods" {
          targets = discovery.kubernetes.loki_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "http-metrics") + "@Running@true@false"
            action = "keep"
          }
    
          // the loki-mixin expects the job label to be namespace/component
          rule {
            source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_component"]
            separator = "/"
            target_label = "job"
          }
    
          
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        // set the workload to the controller kind and name
        rule {
          action = "lowercase"
          source_labels = ["__meta_kubernetes_pod_controller_kind"]
          target_label  = "workload_type"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_controller_name"]
          target_label  = "workload"
        }
        
        // remove the hash from the ReplicaSet
        rule {
          source_labels = [
            "workload_type",
            "workload",
          ]
          separator = "/"
          regex = "replicaset/(.+)-.+$"
          target_label  = "workload"
        }
        
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
        
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
        
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
        }
    
        export "output" {
          value = discovery.relabel.loki_pods.output
        }
      }
    
      declare "loki_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Loki metrics (default: integrations/loki)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "loki" {
          job_name = coalesce(argument.job_label.value, "integrations/loki")
          forward_to = [prometheus.relabel.loki.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // loki metric relabelings (post-scrape)
        prometheus.relabel "loki" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "")
            action = "drop"
          }
    
          // keep only metrics that match the keep_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.keep_metrics.value, "(.+)")
            action = "keep"
          }
    
          // the loki-mixin expects the instance label to be the node name
          rule {
            source_labels = ["node"]
            target_label = "instance"
            replacement = "$1"
          }
          rule {
            action = "labeldrop"
            regex = "node"
          }
    
          // set the memcached exporter container name from container="exporter" to container="memcached"
          rule {
            source_labels = ["component", "container"]
            separator = ";"
            regex = "memcached-[^;]+;exporter"
            target_label = "container"
            replacement = "memcached"
          }
        }
      }
      
      loki_integration_discovery "loki" {
        namespaces = ["logs"]
        label_selectors = ["app.kubernetes.io/name=loki"]
        port_name = "http-metrics"
      }
      
      loki_integration_scrape  "loki" {
        targets = loki_integration_discovery.loki.output
        job_label = "integrations/loki"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|go_gc_cycles_total_gc_cycles_total|go_gc_duration_seconds|go_gc_duration_seconds_count|go_gc_duration_seconds_sum|go_gc_pauses_seconds_bucket|go_goroutines|go_memstats_heap_inuse_bytes|loki_azure_blob_request_duration_seconds_bucket|loki_azure_blob_request_duration_seconds_count|loki_bigtable_request_duration_seconds_bucket|loki_bigtable_request_duration_seconds_count|loki_bloom_blocks_cache_added_total|loki_bloom_blocks_cache_entries|loki_bloom_blocks_cache_evicted_total|loki_bloom_blocks_cache_fetched_total|loki_bloom_blocks_cache_usage_bytes|loki_bloom_chunks_indexed_total|loki_bloom_gateway_block_query_latency_seconds_bucket|loki_bloom_gateway_dequeue_duration_seconds_bucket|loki_bloom_gateway_filtered_chunks_sum|loki_bloom_gateway_filtered_series_sum|loki_bloom_gateway_inflight_tasks|loki_bloom_gateway_process_duration_seconds_bucket|loki_bloom_gateway_process_duration_seconds_count|loki_bloom_gateway_querier_chunks_filtered_total|loki_bloom_gateway_querier_chunks_skipped_total|loki_bloom_gateway_querier_chunks_total|loki_bloom_gateway_querier_series_filtered_total|loki_bloom_gateway_querier_series_skipped_total|loki_bloom_gateway_querier_series_total|loki_bloom_gateway_queue_duration_seconds_bucket|loki_bloom_gateway_queue_duration_seconds_count|loki_bloom_gateway_queue_duration_seconds_sum|loki_bloom_gateway_queue_length|loki_bloom_gateway_requested_chunks_sum|loki_bloom_gateway_requested_series_sum|loki_bloom_gateway_tasks_dequeued_bucket|loki_bloom_gateway_tasks_dequeued_total|loki_bloom_gateway_tasks_processed_total|loki_bloom_inserts_total|loki_bloom_recorder_chunks_total|loki_bloom_recorder_series_total|loki_bloom_size_bucket|loki_bloom_store_blocks_fetched_size_bytes_bucket|loki_bloom_store_blocks_fetched_sum|loki_bloom_store_download_queue_size_sum|loki_bloom_store_metas_fetched_bucket|loki_bloom_store_metas_fetched_size_bytes_bucket|loki_bloom_store_metas_fetched_sum|loki_bloom_tokens_total|loki_bloombuilder_blocks_created_total|loki_bloombuilder_blocks_reused_total|loki_bloombuilder_bytes_per_task_bucket|loki_bloombuilder_chunk_series_size_sum|loki_bloombuilder_metas_created_total|loki_bloombuilder_processing_task|loki_bloombuilder_series_per_task_bucket|loki_bloomplanner_blocks_deleted_total|loki_bloomplanner_connected_builders|loki_bloomplanner_inflight_tasks|loki_bloomplanner_metas_deleted_total|loki_bloomplanner_queue_length|loki_bloomplanner_retention_running|loki_bloomplanner_retention_time_seconds_bucket|loki_bloomplanner_tenant_tasks_completed|loki_bloomplanner_tenant_tasks_planned|loki_boltdb_shipper_compact_tables_operation_duration_seconds|loki_boltdb_shipper_compact_tables_operation_last_successful_run_timestamp_seconds|loki_boltdb_shipper_compact_tables_operation_total|loki_boltdb_shipper_request_duration_seconds_bucket|loki_boltdb_shipper_request_duration_seconds_count|loki_boltdb_shipper_request_duration_seconds_sum|loki_boltdb_shipper_retention_marker_count_total|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_bucket|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_count|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_sum|loki_boltdb_shipper_retention_marker_table_processed_total|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_bucket|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_count|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_sum|loki_boltdb_shipper_retention_sweeper_marker_file_processing_current_time|loki_boltdb_shipper_retention_sweeper_marker_files_current|loki_build_info|loki_chunk_store_deduped_chunks_total|loki_chunk_store_index_entries_per_chunk_count|loki_chunk_store_index_entries_per_chunk_sum|loki_compactor_apply_retention_last_successful_run_timestamp_seconds|loki_compactor_apply_retention_operation_duration_seconds|loki_compactor_apply_retention_operation_total|loki_compactor_delete_requests_processed_total|loki_compactor_delete_requests_received_total|loki_compactor_deleted_lines|loki_compactor_load_pending_requests_attempts_total|loki_compactor_locked_table_successive_compaction_skips|loki_compactor_oldest_pending_delete_request_age_seconds|loki_compactor_pending_delete_requests_count|loki_consul_request_duration_seconds_bucket|loki_discarded_samples_total|loki_distributor_bytes_received_total|loki_distributor_ingester_append_failures_total|loki_distributor_lines_received_total|loki_distributor_structured_metadata_bytes_received_total|loki_dynamo_consumed_capacity_total|loki_dynamo_dropped_requests_total|loki_dynamo_failures_total|loki_dynamo_query_pages_count|loki_dynamo_request_duration_seconds_bucket|loki_dynamo_request_duration_seconds_count|loki_dynamo_throttled_total|loki_embeddedcache_entries|loki_embeddedcache_memory_bytes|loki_gcs_request_duration_seconds_bucket|loki_gcs_request_duration_seconds_count|loki_index_gateway_postfilter_chunks_sum|loki_index_gateway_prefilter_chunks_sum|loki_index_request_duration_seconds_bucket|loki_index_request_duration_seconds_count|loki_index_request_duration_seconds_sum|loki_ingester_chunk_age_seconds_bucket|loki_ingester_chunk_age_seconds_count|loki_ingester_chunk_age_seconds_sum|loki_ingester_chunk_bounds_hours_bucket|loki_ingester_chunk_bounds_hours_count|loki_ingester_chunk_bounds_hours_sum|loki_ingester_chunk_entries_bucket|loki_ingester_chunk_entries_count|loki_ingester_chunk_entries_sum|loki_ingester_chunk_size_bytes_bucket|loki_ingester_chunk_utilization_bucket|loki_ingester_chunk_utilization_count|loki_ingester_chunk_utilization_sum|loki_ingester_chunks_flushed_total|loki_ingester_flush_queue_length|loki_ingester_memory_chunks|loki_ingester_memory_streams|loki_ingester_streams_created_total|loki_memcache_request_duration_seconds_bucket|loki_memcache_request_duration_seconds_count|loki_panic_total|loki_prometheus_rule_group_rules|loki_request_duration_seconds_bucket|loki_request_duration_seconds_count|loki_request_duration_seconds_sum|loki_ruler_wal_appender_ready|loki_ruler_wal_disk_size|loki_ruler_wal_prometheus_remote_storage_highest_timestamp_in_seconds|loki_ruler_wal_prometheus_remote_storage_queue_highest_sent_timestamp_seconds|loki_ruler_wal_prometheus_remote_storage_samples_pending|loki_ruler_wal_prometheus_remote_storage_samples_total|loki_ruler_wal_samples_appended_total|loki_ruler_wal_storage_created_series_total|loki_s3_request_duration_seconds_bucket|loki_s3_request_duration_seconds_count"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    loki_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    declare "mimir_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "mimir_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=mimir\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: http-metrics)"
          optional = true
        }
    
        // mimir service discovery for all of the pods
        discovery.kubernetes "mimir_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=mimir"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
        }
    
        // mimir relabelings (pre-scrape)
        discovery.relabel "mimir_pods" {
          targets = discovery.kubernetes.mimir_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "http-metrics") + "@Running@true@false"
            action = "keep"
          }
    
          // the mimir-mixin expects the job label to be namespace/component
          rule {
            source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_component"]
            separator = "/"
            target_label = "job"
          }
    
          
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        // set the workload to the controller kind and name
        rule {
          action = "lowercase"
          source_labels = ["__meta_kubernetes_pod_controller_kind"]
          target_label  = "workload_type"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_controller_name"]
          target_label  = "workload"
        }
        
        // remove the hash from the ReplicaSet
        rule {
          source_labels = [
            "workload_type",
            "workload",
          ]
          separator = "/"
          regex = "replicaset/(.+)-.+$"
          target_label  = "workload"
        }
        
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
        
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
        
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
        }
    
        export "output" {
          value = discovery.relabel.mimir_pods.output
        }
      }
    
      declare "mimir_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Mimir metrics (default: integrations/mimir)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "mimir" {
          job_name = coalesce(argument.job_label.value, "integrations/mimir")
          forward_to = [prometheus.relabel.mimir.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // mimir metric relabelings (post-scrape)
        prometheus.relabel "mimir" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "")
            action = "drop"
          }
    
          // keep only metrics that match the keep_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.keep_metrics.value, "(.+)")
            action = "keep"
          }
    
          // the mimir-mixin expects the instance label to be the node name
          rule {
            source_labels = ["node"]
            target_label = "instance"
            replacement = "$1"
          }
          rule {
            action = "labeldrop"
            regex = "node"
          }
    
          // set the memcached exporter container name from container="exporter" to container="memcached"
          rule {
            source_labels = ["component", "container"]
            separator = ";"
            regex = "memcached-[^;]+;exporter"
            target_label = "container"
            replacement = "memcached"
          }
        }
      }
      
      mimir_integration_discovery "mimir" {
        namespaces = ["metrics"]
        label_selectors = ["app.kubernetes.io/name=mimir"]
        port_name = "http-metrics"
      }
      
      mimir_integration_scrape  "mimir" {
        targets = mimir_integration_discovery.mimir.output
        job_label = "integrations/mimir"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|cortex_alertmanager_alerts|cortex_alertmanager_alerts_invalid_total|cortex_alertmanager_alerts_received_total|cortex_alertmanager_dispatcher_aggregation_groups|cortex_alertmanager_notification_latency_seconds_bucket|cortex_alertmanager_notification_latency_seconds_count|cortex_alertmanager_notification_latency_seconds_sum|cortex_alertmanager_notifications_failed_total|cortex_alertmanager_notifications_total|cortex_alertmanager_partial_state_merges_failed_total|cortex_alertmanager_partial_state_merges_total|cortex_alertmanager_ring_check_errors_total|cortex_alertmanager_silences|cortex_alertmanager_state_fetch_replica_state_failed_total|cortex_alertmanager_state_fetch_replica_state_total|cortex_alertmanager_state_initial_sync_completed_total|cortex_alertmanager_state_initial_sync_duration_seconds_bucket|cortex_alertmanager_state_initial_sync_duration_seconds_count|cortex_alertmanager_state_initial_sync_duration_seconds_sum|cortex_alertmanager_state_persist_failed_total|cortex_alertmanager_state_persist_total|cortex_alertmanager_state_replication_failed_total|cortex_alertmanager_state_replication_total|cortex_alertmanager_sync_configs_failed_total|cortex_alertmanager_sync_configs_total|cortex_alertmanager_tenants_discovered|cortex_alertmanager_tenants_owned|cortex_blockbuilder_consume_cycle_duration_seconds|cortex_blockbuilder_consumer_lag_records|cortex_blockbuilder_tsdb_compact_and_upload_failed_total|cortex_bucket_blocks_count|cortex_bucket_index_estimated_compaction_jobs|cortex_bucket_index_estimated_compaction_jobs_errors_total|cortex_bucket_index_last_successful_update_timestamp_seconds|cortex_bucket_store_block_drop_failures_total|cortex_bucket_store_block_drops_total|cortex_bucket_store_block_load_failures_total|cortex_bucket_store_block_loads_total|cortex_bucket_store_blocks_loaded|cortex_bucket_store_indexheader_lazy_load_duration_seconds_bucket|cortex_bucket_store_indexheader_lazy_load_duration_seconds_count|cortex_bucket_store_indexheader_lazy_load_duration_seconds_sum|cortex_bucket_store_indexheader_lazy_load_total|cortex_bucket_store_indexheader_lazy_unload_total|cortex_bucket_store_series_batch_preloading_load_duration_seconds_sum|cortex_bucket_store_series_batch_preloading_wait_duration_seconds_sum|cortex_bucket_store_series_blocks_queried_sum|cortex_bucket_store_series_data_size_fetched_bytes_sum|cortex_bucket_store_series_data_size_touched_bytes_sum|cortex_bucket_store_series_hash_cache_hits_total|cortex_bucket_store_series_hash_cache_requests_total|cortex_bucket_store_series_request_stage_duration_seconds_bucket|cortex_bucket_store_series_request_stage_duration_seconds_count|cortex_bucket_store_series_request_stage_duration_seconds_sum|cortex_bucket_stores_blocks_last_successful_sync_timestamp_seconds|cortex_bucket_stores_gate_duration_seconds_bucket|cortex_bucket_stores_gate_duration_seconds_count|cortex_bucket_stores_gate_duration_seconds_sum|cortex_bucket_stores_tenants_synced|cortex_build_info|cortex_cache_memory_hits_total|cortex_cache_memory_requests_total|cortex_compactor_block_cleanup_failures_total|cortex_compactor_block_cleanup_last_successful_run_timestamp_seconds|cortex_compactor_block_max_time_delta_seconds_bucket|cortex_compactor_block_max_time_delta_seconds_count|cortex_compactor_block_max_time_delta_seconds_sum|cortex_compactor_blocks_cleaned_total|cortex_compactor_blocks_marked_for_deletion_total|cortex_compactor_blocks_marked_for_no_compaction_total|cortex_compactor_disk_out_of_space_errors_total|cortex_compactor_group_compaction_runs_started_total|cortex_compactor_last_successful_run_timestamp_seconds|cortex_compactor_meta_sync_duration_seconds_bucket|cortex_compactor_meta_sync_duration_seconds_count|cortex_compactor_meta_sync_duration_seconds_sum|cortex_compactor_meta_sync_failures_total|cortex_compactor_meta_syncs_total|cortex_compactor_runs_completed_total|cortex_compactor_runs_failed_total|cortex_compactor_runs_started_total|cortex_compactor_tenants_discovered|cortex_compactor_tenants_processing_failed|cortex_compactor_tenants_processing_succeeded|cortex_compactor_tenants_skipped|cortex_config_hash|cortex_discarded_exemplars_total|cortex_discarded_requests_total|cortex_discarded_samples_total|cortex_distributor_deduped_samples_total|cortex_distributor_exemplars_in_total|cortex_distributor_inflight_push_requests|cortex_distributor_instance_limits|cortex_distributor_instance_rejected_requests_total|cortex_distributor_latest_seen_sample_timestamp_seconds|cortex_distributor_non_ha_samples_received_total|cortex_distributor_received_exemplars_total|cortex_distributor_received_requests_total|cortex_distributor_received_samples_total|cortex_distributor_replication_factor|cortex_distributor_requests_in_total|cortex_distributor_samples_in_total|cortex_inflight_requests|cortex_ingest_storage_reader_buffered_fetched_records|cortex_ingest_storage_reader_fetch_errors_total|cortex_ingest_storage_reader_fetches_total|cortex_ingest_storage_reader_missed_records_total|cortex_ingest_storage_reader_offset_commit_failures_total|cortex_ingest_storage_reader_offset_commit_requests_total|cortex_ingest_storage_reader_read_errors_total|cortex_ingest_storage_reader_receive_delay_seconds_count|cortex_ingest_storage_reader_receive_delay_seconds_sum|cortex_ingest_storage_reader_records_failed_total|cortex_ingest_storage_reader_records_total|cortex_ingest_storage_reader_requests_failed_total|cortex_ingest_storage_reader_requests_total|cortex_ingest_storage_strong_consistency_failures_total|cortex_ingest_storage_strong_consistency_requests_total|cortex_ingest_storage_writer_buffered_produce_bytes|cortex_ingest_storage_writer_buffered_produce_bytes_limit|cortex_ingester_active_native_histogram_buckets|cortex_ingester_active_native_histogram_buckets_custom_tracker|cortex_ingester_active_native_histogram_series|cortex_ingester_active_native_histogram_series_custom_tracker|cortex_ingester_active_series|cortex_ingester_active_series_custom_tracker|cortex_ingester_client_request_duration_seconds_bucket|cortex_ingester_client_request_duration_seconds_count|cortex_ingester_client_request_duration_seconds_sum|cortex_ingester_ingested_exemplars_total|cortex_ingester_ingested_samples_total|cortex_ingester_instance_limits|cortex_ingester_instance_rejected_requests_total|cortex_ingester_local_limits|cortex_ingester_memory_series|cortex_ingester_memory_series_created_total|cortex_ingester_memory_series_removed_total|cortex_ingester_memory_users|cortex_ingester_oldest_unshipped_block_timestamp_seconds|cortex_ingester_owned_series|cortex_ingester_queried_exemplars_bucket|cortex_ingester_queried_exemplars_count|cortex_ingester_queried_exemplars_sum|cortex_ingester_queried_samples_bucket|cortex_ingester_queried_samples_count|cortex_ingester_queried_samples_sum|cortex_ingester_queried_series_bucket|cortex_ingester_queried_series_count|cortex_ingester_queried_series_sum|cortex_ingester_shipper_last_successful_upload_timestamp_seconds|cortex_ingester_shipper_upload_failures_total|cortex_ingester_shipper_uploads_total|cortex_ingester_tsdb_checkpoint_creations_failed_total|cortex_ingester_tsdb_checkpoint_creations_total|cortex_ingester_tsdb_checkpoint_deletions_failed_total|cortex_ingester_tsdb_compaction_duration_seconds_bucket|cortex_ingester_tsdb_compaction_duration_seconds_count|cortex_ingester_tsdb_compaction_duration_seconds_sum|cortex_ingester_tsdb_compactions_failed_total|cortex_ingester_tsdb_compactions_total|cortex_ingester_tsdb_exemplar_exemplars_appended_total|cortex_ingester_tsdb_exemplar_exemplars_in_storage|cortex_ingester_tsdb_exemplar_last_exemplars_timestamp_seconds|cortex_ingester_tsdb_exemplar_series_with_exemplars_in_storage|cortex_ingester_tsdb_head_max_timestamp_seconds|cortex_ingester_tsdb_head_truncations_failed_total|cortex_ingester_tsdb_mmap_chunk_corruptions_total|cortex_ingester_tsdb_out_of_order_samples_appended_total|cortex_ingester_tsdb_storage_blocks_bytes|cortex_ingester_tsdb_symbol_table_size_bytes|cortex_ingester_tsdb_wal_corruptions_total|cortex_ingester_tsdb_wal_truncate_duration_seconds_count|cortex_ingester_tsdb_wal_truncate_duration_seconds_sum|cortex_ingester_tsdb_wal_truncations_failed_total|cortex_ingester_tsdb_wal_truncations_total|cortex_ingester_tsdb_wal_writes_failed_total|cortex_kv_request_duration_seconds_bucket|cortex_kv_request_duration_seconds_count|cortex_kv_request_duration_seconds_sum|cortex_lifecycler_read_only|cortex_limits_defaults|cortex_limits_overrides|cortex_partition_ring_partitions|cortex_prometheus_notifications_dropped_total|cortex_prometheus_notifications_errors_total|cortex_prometheus_notifications_queue_capacity|cortex_prometheus_notifications_queue_length|cortex_prometheus_notifications_sent_total|cortex_prometheus_rule_evaluation_duration_seconds_count|cortex_prometheus_rule_evaluation_duration_seconds_sum|cortex_prometheus_rule_evaluation_failures_total|cortex_prometheus_rule_evaluations_total|cortex_prometheus_rule_group_duration_seconds_count|cortex_prometheus_rule_group_duration_seconds_sum|cortex_prometheus_rule_group_iterations_missed_total|cortex_prometheus_rule_group_iterations_total|cortex_prometheus_rule_group_rules|cortex_querier_blocks_consistency_checks_failed_total|cortex_querier_blocks_consistency_checks_total|cortex_querier_request_duration_seconds_bucket|cortex_querier_request_duration_seconds_count|cortex_querier_request_duration_seconds_sum|cortex_querier_storegateway_instances_hit_per_query_bucket|cortex_querier_storegateway_instances_hit_per_query_count|cortex_querier_storegateway_instances_hit_per_query_sum|cortex_querier_storegateway_refetches_per_query_bucket|cortex_querier_storegateway_refetches_per_query_count|cortex_querier_storegateway_refetches_per_query_sum|cortex_query_frontend_queries_total|cortex_query_frontend_queue_duration_seconds_bucket|cortex_query_frontend_queue_duration_seconds_count|cortex_query_frontend_queue_duration_seconds_sum|cortex_query_frontend_queue_length|cortex_query_frontend_retries_bucket|cortex_query_frontend_retries_count|cortex_query_frontend_retries_sum|cortex_query_scheduler_connected_querier_clients|cortex_query_scheduler_querier_inflight_requests|cortex_query_scheduler_queue_duration_seconds_bucket|cortex_query_scheduler_queue_duration_seconds_count|cortex_query_scheduler_queue_duration_seconds_sum|cortex_query_scheduler_queue_length|cortex_request_duration_seconds|cortex_request_duration_seconds_bucket|cortex_request_duration_seconds_count|cortex_request_duration_seconds_sum|cortex_ring_members|cortex_ruler_managers_total|cortex_ruler_queries_failed_total|cortex_ruler_queries_total|cortex_ruler_ring_check_errors_total|cortex_ruler_write_requests_failed_total|cortex_ruler_write_requests_total|cortex_runtime_config_hash|cortex_runtime_config_last_reload_successful|cortex_tcp_connections|cortex_tcp_connections_limit|go_memstats_heap_inuse_bytes|keda_scaler_errors|keda_scaler_metrics_value|kube_deployment_spec_replicas|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_endpoint_address|kube_horizontalpodautoscaler_spec_target_metric|kube_horizontalpodautoscaler_status_condition|kube_pod_info|kube_statefulset_replicas|kube_statefulset_status_current_revision|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_statefulset_status_replicas_updated|kube_statefulset_status_update_revision|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_used_bytes|memberlist_client_cluster_members_count|memcached_limit_bytes|mimir_continuous_test_queries_failed_total|mimir_continuous_test_query_result_checks_failed_total|mimir_continuous_test_writes_failed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|process_memory_map_areas|process_memory_map_areas_limit|prometheus_tsdb_compaction_duration_seconds_bucket|prometheus_tsdb_compaction_duration_seconds_count|prometheus_tsdb_compaction_duration_seconds_sum|prometheus_tsdb_compactions_total|rollout_operator_last_successful_group_reconcile_timestamp_seconds|thanos_cache_hits_total|thanos_cache_operation_duration_seconds_bucket|thanos_cache_operation_duration_seconds_count|thanos_cache_operation_duration_seconds_sum|thanos_cache_operation_failures_total|thanos_cache_operations_total|thanos_cache_requests_total|thanos_objstore_bucket_last_successful_upload_time|thanos_objstore_bucket_operation_duration_seconds_bucket|thanos_objstore_bucket_operation_duration_seconds_count|thanos_objstore_bucket_operation_duration_seconds_sum|thanos_objstore_bucket_operation_failures_total|thanos_objstore_bucket_operations_total|thanos_store_index_cache_hits_total|thanos_store_index_cache_requests_total"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    mimir_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.4", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{deployments="kube-state-metrics,node-exporter", feature="clusterMetrics", sources="cadvisor,kube-state-metrics,node-exporter", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="clusterEvents", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="kubernetesApi", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="integrations", sources="alloy,loki", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-modules-configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-module-kubernetes
data:
  core_metrics.alloy: |
    /*
    Module: job-cadvisor
    Description: Scrapes cadvisor
    
    Note: Every argument except for "forward_to" is optional, and does have a defined default value.  However, the values for these
          arguments are not defined using the default = " ... " argument syntax, but rather using the coalesce(argument.value, " ... ").
          This is because if the argument passed in from another consuming module is set to null, the default = " ... " syntax will
          does not override the value passed in, where coalesce() will return the first non-null value.
    */
    declare "cadvisor" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all cadvisor metric (default: integrations/kubernetes/cadvisor)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.cadvisor.output
      }
    
      // cadvisor service discovery for all of the nodes
      discovery.kubernetes "cadvisor" {
        role = "node"
    
        selectors {
          role = "node"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // cadvisor relabelings (pre-scrape)
      discovery.relabel "cadvisor" {
        targets = discovery.kubernetes.cadvisor.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes cadvisor metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/cadvisor"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // cadvisor scrape job
      prometheus.scrape "cadvisor" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/cadvisor")
        forward_to = [prometheus.relabel.cadvisor.receiver]
        targets = discovery.relabel.cadvisor.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // cadvisor metric relabelings (post-scrape)
      prometheus.relabel "cadvisor" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(up|container_(cpu_(cfs_(periods|throttled_periods)_total|usage_seconds_total)|fs_(reads|writes)(_bytes)?_total|memory_(cache|rss|swap|working_set_bytes)|network_(receive|transmit)_(bytes|packets(_dropped)?_total))|machine_memory_bytes)")
          action = "keep"
        }
    
        // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","container"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
          action = "drop"
        }
    
        // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","image"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
          action = "drop"
        }
    
        // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
        rule {
          source_labels = ["__name__", "boot_id"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "boot_id"
          replacement = "NA"
        }
        rule {
          source_labels = ["__name__", "system_uuid"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "system_uuid"
          replacement = "NA"
        }
    
        // Filter out non-physical devices/interfaces
        rule {
          source_labels = ["__name__", "device"]
          separator = "@"
          regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_fs_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_fs_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          source_labels = ["__name__", "interface"]
          separator = "@"
          regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_network_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_network_.*"
          target_label = "__keepme"
          replacement = ""
        }
      }
    }
    
    declare "resources" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all resources metric (default: integrations/kubernetes/kube-resources)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.resources.output
      }
    
      // resources service discovery for all of the nodes
      discovery.kubernetes "resources" {
        role = "node"
    
        selectors {
          role = "node"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // resources relabelings (pre-scrape)
      discovery.relabel "resources" {
        targets = discovery.kubernetes.resources.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes resources metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/resource"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // resources scrape job
      prometheus.scrape "resources" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-resources")
        forward_to = [prometheus.relabel.resources.receiver]
        targets = discovery.relabel.resources.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // resources metric relabelings (post-scrape)
      prometheus.relabel "resources" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "kubelet" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kubelet metric (default: integrations/kubernetes/kube-kubelet)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.kubelet.output
      }
    
      // kubelet service discovery for all of the nodes
      discovery.kubernetes "kubelet" {
        role = "node"
    
        selectors {
          role = "node"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // kubelet relabelings (pre-scrape)
      discovery.relabel "kubelet" {
        targets = discovery.kubernetes.kubelet.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes kubelet metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // kubelet scrape job
      prometheus.scrape "kubelet" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kubelet")
        forward_to = [prometheus.relabel.kubelet.receiver]
        targets = discovery.relabel.kubelet.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // kubelet metric relabelings (post-scrape)
      prometheus.relabel "kubelet" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "apiserver" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: default)"
        optional = true
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "port_name" {
        comment = "The value of the label for the selector (default: https)"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kube-apiserver metrics (default: integrations/kubernetes/kube-apiserver)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      // drop metrics and les from kube-prometheus
      // https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "drop_les" {
        comment = "Regular expression of metric les label values to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.apiserver.output
      }
    
      // kube-apiserver service discovery
      discovery.kubernetes "apiserver" {
        role = "service"
    
        selectors {
          role = "service"
          field = string.join(coalesce(argument.field_selectors.value, ["metadata.name=kubernetes"]), ",")
          label = string.join(coalesce(argument.label_selectors.value, []), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, ["default"])
        }
      }
    
      // apiserver relabelings (pre-scrape)
      discovery.relabel "apiserver" {
        targets = discovery.kubernetes.apiserver.targets
    
        // only keep targets with a matching port name
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          regex = coalesce(argument.port_name.value, "https")
          action = "keep"
        }
    
        // set the namespace
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
    
        // set the service_name
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_service_name"]
          target_label = "service"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_service_label_app_kubernetes_io_name",
            "__meta_kubernetes_service_label_k8s_app",
            "__meta_kubernetes_service_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // kube-apiserver scrape job
      prometheus.scrape "apiserver" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-apiserver")
        forward_to = [prometheus.relabel.apiserver.receiver]
        targets = discovery.relabel.apiserver.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // apiserver metric relabelings (post-scrape)
      prometheus.relabel "apiserver" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(((go|process)_.+)|kubelet_(pod_(worker|start)_latency_microseconds|cgroup_manager_latency_microseconds|pleg_relist_(latency|interval)_microseconds|runtime_operations(_latency_microseconds|_errors)?|eviction_stats_age_microseconds|device_plugin_(registration_count|alloc_latency_microseconds)|network_plugin_operations_latency_microseconds)|scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_(predicate|priority|preemption)_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)|apiserver_(request_(count|latencies(_summary)?)|dropped_requests|storage_(data_key_generation|transformation_(failures_total|latencies_microseconds))|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)|kubelet_docker_(operations(_latency_microseconds|_errors|_timeout)?)|reflector_(items_per_(list|watch)|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)|etcd_(helper_(cache_(hit|miss)_count|cache_entry_count|object_counts)|request_(cache_(get|add)_latencies_summary|latencies_summary)|debugging.*|disk.*|server.*)|transformation_(latencies_microseconds|failures_total)|(admission_quota_controller|APIServiceOpenAPIAggregationControllerQueue1|APIServiceRegistrationController|autoregister|AvailableConditionController|crd_(autoregistration_controller|Establishing|finalizer|naming_condition_controller|openapi_controller)|DiscoveryController|non_structural_schema_condition_controller|kubeproxy_sync_proxy_rules|rest_client_request_latency|storage_operation_(errors_total|status_count))(_.*)|apiserver_admission_(controller_admission|step_admission)_latencies_seconds_.*)")
          action = "drop"
        }
    
        // drop metrics whose name and le label match the drop_les regex
        rule {
          source_labels = [
            "__name__",
            "le",
          ]
          regex = coalesce(argument.drop_les.value, "apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "probes" {
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      argument "job_label" {
        comment = "The job label to add for all probes metric (default: integrations/kubernetes/kube-probes)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.probes.output
      }
    
      // probes service discovery for all of the nodes
      discovery.kubernetes "probes" {
        role = "node"
    
        selectors {
          role = "node"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // probes relabelings (pre-scrape)
      discovery.relabel "probes" {
        targets = discovery.kubernetes.probes.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes probes metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/probes"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // probes scrape job
      prometheus.scrape "probes" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-probes")
        forward_to = [prometheus.relabel.probes.receiver]
        targets = discovery.relabel.probes.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // probes metric relabelings (post-scrape)
      prometheus.relabel "probes" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "kube_dns" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
      // arguments for kubernetes discovery
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: [\"kube-system\"])"
        optional = true
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"k8s-app=kube-dns\"])"
        optional = true
      }
      argument "port_name" {
        comment = "The of the port to scrape metrics from (default: metrics)"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kube_dns metric (default: integrations/kubernetes/kube-dns)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.kube_dns.output
      }
    
      // kube_dns service discovery for all of the nodes
      discovery.kubernetes "kube_dns" {
        role = "endpoints"
    
        selectors {
          role = "endpoints"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, ["k8s-app=kube-dns"]), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, ["kube-system"])
        }
      }
    
      // kube_dns relabelings (pre-scrape)
      discovery.relabel "kube_dns" {
        targets = discovery.kubernetes.kube_dns.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = coalesce(argument.port_name.value, "metrics") + "@Running@true"
          action = "keep"
        }
    
        // drop any init containers
        rule {
          source_labels = ["__meta_kubernetes_pod_container_init"]
          regex = "true"
          action = "drop"
        }
    
        // set the namespace label
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
    
        // set the pod label
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
    
        // set the container label
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
    
        // set a workload label
        rule {
          source_labels = [
            "__meta_kubernetes_pod_controller_kind",
            "__meta_kubernetes_pod_controller_name",
          ]
          separator = "/"
          target_label  = "workload"
        }
        // remove the hash from the ReplicaSet
        rule {
          source_labels = ["workload"]
          regex = "(ReplicaSet/.+)-.+"
          target_label  = "workload"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set the service label
        rule {
          source_labels = ["__meta_kubernetes_service_name"]
          target_label  = "service"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // kube_dns scrape job
      prometheus.scrape "kube_dns" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-dns")
        forward_to = [prometheus.relabel.kube_dns.receiver]
        targets = discovery.relabel.kube_dns.output
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // kube_dns metric relabelings (post-scrape)
      prometheus.relabel "kube_dns" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process|promhttp)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
  kube-state-metrics_metrics.alloy: |
    /*
    Module: job-kube-state-metrics
    Description: Scrapes Kube-State-Metrics, this is a separate scrape job, if you are also using annotation based scraping, you will want to explicitly
                 disable kube-state-metrics from being scraped by this module and annotations by setting the following annotation on the kube-state-metrics
                 metrics.agent.grafana.com/scrape: "false"
    
    Note: Every argument except for "forward_to" is optional, and does have a defined default value.  However, the values for these
          arguments are not defined using the default = " ... " argument syntax, but rather using the coalesce(argument.value, " ... ").
          This is because if the argument passed in from another consuming module is set to null, the default = " ... " syntax will
          does not override the value passed in, where coalesce() will return the first non-null value.
    */
    declare "kubernetes" {
      // arguments for kubernetes discovery
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: [] is all namespaces)"
        optional = true
      }
    
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
    
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=kube-state-metrics\"])"
        optional = true
      }
    
      argument "port_name" {
        comment = "The of the port to scrape metrics from (default: http)"
        optional = true
      }
    
      // kube state metrics service discovery for all of the endpoints
      discovery.kubernetes "ksm" {
        role = "endpoints"
    
        selectors {
          role = "endpoints"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=kube-state-metrics"]), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, [])
        }
      }
    
      // kube-state-metrics relabelings (pre-scrape)
      discovery.relabel "ksm" {
        targets = discovery.kubernetes.ksm.targets
    
        // only keep targets with a matching port name
        rule {
          source_labels = ["__meta_kubernetes_endpoint_port_name"]
          regex = coalesce(argument.port_name.value, "http")
          action = "keep"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      export "output" {
        value = discovery.relabel.ksm.output
      }
    }
    
    declare "scrape" {
      argument "targets" {
        comment = "Must be a list() of targets"
      }
    
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
    
      argument "job_label" {
        comment = "The job label to add for all kube-kube_state_metrics metrics (default: integrations/kubernetes/kube-state-metrics)"
        optional = true
      }
    
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
    
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
    
      argument "scheme" {
        comment = "The scheme to use when scraping metrics (default: http)"
        optional = true
      }
    
      argument "bearer_token_file" {
        comment = "The bearer token file (default: none)"
        optional = true
      }
    
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
    
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
    
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
    
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      // kube-state-metrics scrape job
      prometheus.scrape "kube_state_metrics" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-state-metrics")
        forward_to = [prometheus.relabel.kube_state_metrics.receiver]
        targets = argument.targets.value
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
    
        scheme = coalesce(argument.scheme.value, "http")
        bearer_token_file = coalesce(argument.bearer_token_file.value, "")
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // kube_state_metrics metric relabelings (post-scrape)
      prometheus.relabel "kube_state_metrics" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(up|kube_(daemonset.*|deployment_(metadata_generation|spec_replicas|status_(observed_generation|replicas_(available|updated)))|horizontalpodautoscaler_(spec_(max|min)_replicas|status_(current|desired)_replicas)|job.*|namespace_status_phase|node.*|persistentvolumeclaim_resource_requests_storage_bytes|pod_(container_(info|resource_(limits|requests)|status_(last_terminated_reason|restarts_total|waiting_reason))|info|owner|start_time|status_(phase|reason))|replicaset.*|resourcequota|statefulset.*))")
          action = "keep"
        }
      }
    }
---
# Source: k8s-monitoring/templates/alloy-modules-configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-module-system
data:
  node-exporter_metrics.alloy: |
    /*
    Module: job-node_exporter
    Description: Scrapes node_exporter
    
    Note: Every argument except for "forward_to" is optional, and does have a defined default value.  However, the values for these
          arguments are not defined using the default = " ... " argument syntax, but rather using the coalesce(argument.value, " ... ").
          This is because if the argument passed in from another consuming module is set to null, the default = " ... " syntax will
          does not override the value passed in, where coalesce() will return the first non-null value.
    */
    declare "kubernetes" {
      // arguments for kubernetes discovery
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: [] is all namespaces)"
        optional = true
      }
    
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
    
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=prometheus-node-exporter\"])"
        optional = true
      }
    
      argument "port_name" {
        comment = "The of the port to scrape metrics from (default: metrics)"
        optional = true
      }
    
      // node_exporter service discovery for all of the pods
      discovery.kubernetes "node_exporter" {
        role = "pod"
    
        selectors {
          role = "pod"
          field = string.join(coalesce(argument.field_selectors.value, []), ",")
          label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=prometheus-node-exporter"]), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, [])
        }
      }
    
      // node_exporter relabelings (pre-scrape)
      discovery.relabel "kubernetes" {
        targets = discovery.kubernetes.node_exporter.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = coalesce(argument.port_name.value, "metrics") + "@Running@true"
          action = "keep"
        }
    
        // drop any init containers
        rule {
          source_labels = ["__meta_kubernetes_pod_container_init"]
          regex = "true"
          action = "drop"
        }
    
        // set the namespace label
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
    
        // set the pod label
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
    
        // set the container label
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
    
        // set a workload label
        rule {
          source_labels = [
            "__meta_kubernetes_pod_controller_kind",
            "__meta_kubernetes_pod_controller_name",
          ]
          separator = "/"
          target_label  = "workload"
        }
        // remove the hash from the ReplicaSet
        rule {
          source_labels = ["workload"]
          regex = "(ReplicaSet/.+)-.+"
          target_label  = "workload"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      export "output" {
        value = discovery.relabel.kubernetes.output
      }
    }
    
    declare "local" {
      argument "port" {
        comment = "The port to use (default: 9100)"
        optional = true
      }
    
      // arguments for local (static)
      discovery.relabel "local" {
        targets = [
          {
            "__address__" = "localhost" + string.format("%s", coalesce(argument.port.value, "9100")),
            "source" = "local",
          },
        ]
      }
    
      export "output" {
        value = discovery.relabel.local.output
      }
    }
    
    declare "scrape" {
      argument "targets" {
        comment = "Must be a list() of targets"
      }
    
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
      }
    
      argument "job_label" {
        comment = "The job label to add for all node_exporter metric (default: integrations/node_exporter)"
        optional = true
      }
    
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
    
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
    
      argument "scheme" {
        comment = "The scheme to use when scraping metrics (default: http)"
        optional = true
      }
    
      argument "bearer_token_file" {
        comment = "The bearer token file (default: none)"
        optional = true
      }
    
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
    
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
    
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
    
      argument "clustering" {
        // Docs: https://node_exporter.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      // node_exporter scrape job
      prometheus.scrape "node_exporter" {
        job_name = coalesce(argument.job_label.value, "integrations/node_exporter")
        forward_to = [prometheus.relabel.node_exporter.receiver]
        targets = argument.targets.value
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
    
        scheme = coalesce(argument.scheme.value, "http")
        bearer_token_file = coalesce(argument.bearer_token_file.value, "")
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // node_exporter metric relabelings (post-scrape)
      prometheus.relabel "node_exporter" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(up|scrape_(duration_seconds|series_added|samples_(post_metric_relabeling|scraped))|node_(arp_entries|boot_time_seconds|context_switches_total|cpu_seconds_total|disk_(io_time_seconds_total|io_time_weighted_seconds_total|read_(bytes_total|time_seconds_total)|reads_completed_total|write_time_seconds_total|writes_completed_total|written_bytes_total)|file(fd_(allocated|maximum)|system_(avail_bytes|device_error|files(_free)?|readonly|size_bytes))|intr_total|load(1|15|5)|md_disks(_required)?|memory_(Active_(anon_bytes|bytes|file_bytes)|Anon(HugePages_bytes|Pages_bytes)|Bounce_bytes|Buffers_bytes|Cached_bytes|CommitLimit_bytes|Committed_AS_bytes|DirectMap(1G|2M|4k)_bytes|Dirty_bytes|HugePages_(Free|Rsvd|Surp|Total)|Hugepagesize_bytes|Inactive_(anon_bytes|bytes|file_bytes)|Mapped_bytes|Mem(Available|Free|Total)_bytes|S(Reclaimable|Unreclaim)_bytes|Shmem(HugePages_bytes|PmdMapped_bytes|_bytes)|Slab_bytes|SwapTotal_bytes|Vmalloc(Chunk|Total|Used)_bytes|Writeback(Tmp|)_bytes)|netstat_(Icmp6_(InErrors|InMsgs|OutMsgs)|Icmp_(InErrors|InMsgs|OutMsgs)|IpExt_(InOctets|OutOctets)|TcpExt_(Listen(Drops|Overflows)|TCPSynRetrans)|Tcp_(InErrs|InSegs|OutRsts|OutSegs|RetransSegs)|Udp6_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors)|Udp(Lite|)_(InDatagrams|InErrors|NoPorts|OutDatagrams|RcvbufErrors|SndbufErrors))|network_(carrier|info|mtu_bytes|receive_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total)|speed_bytes|transmit_(bytes_total|compressed_total|drop_total|errs_total|fifo_total|multicast_total|packets_total|queue_length)|up)|nf_conntrack_(entries(_limit)?|limit)|os_info|sockstat_(FRAG6|FRAG|RAW6|RAW|TCP6|TCP_(alloc|inuse|mem(_bytes)?|orphan|tw)|UDP6|UDPLITE6|UDPLITE|UDP_(inuse|mem(_bytes)?)|sockets_used)|softnet_(dropped_total|processed_total|times_squeezed_total)|systemd_unit_state|textfile_scrape_error|time_zone_offset_seconds|timex_(estimated_error_seconds|maxerror_seconds|offset_seconds|sync_status)|uname_info|vmstat_(oom_kill|pgfault|pgmajfault|pgpgin|pgpgout|pswpin|pswpout)|process_(max_fds|open_fds)))")
          action = "keep"
        }
    
        // Drop metrics for certain file systems
        rule {
          source_labels = ["__name__", "fstype"]
          separator = "@"
          regex = "node_filesystem.*@(tempfs)"
          action = "drop"
        }
      }
    }
---
# Source: k8s-monitoring/charts/alloy-singleton/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-singleton
  labels:
    helm.sh/chart: alloy-singleton-0.11.0
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.6.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.28.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.14.0"
    release: k8smon
  name: k8smon-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-singleton/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-singleton
  labels:
    helm.sh/chart: alloy-singleton-0.11.0
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.6.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-singleton
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-singleton
    namespace: default
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.28.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.14.0"
    release: k8smon
  name: k8smon-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/alloy-singleton/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-singleton
  labels:
    helm.sh/chart: alloy-singleton-0.11.0
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.6.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.28.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.14.0"
    release: k8smon
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.43.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.8.2"
    release: k8smon
  annotations:
    prometheus.io/scrape: "false"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.43.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.8.2"
    release: k8smon
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/instance: k8smon
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        k8s.grafana.com/logs.job: integrations/node_exporter
      labels:
        helm.sh/chart: node-exporter-4.43.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: node-exporter
        app.kubernetes.io/name: node-exporter
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.8.2"
        release: k8smon
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: k8smon-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.8.2
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                - fargate
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: k8s-monitoring/charts/alloy-singleton/templates/controllers/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-singleton
  labels:
    helm.sh/chart: alloy-singleton-0.11.0
    app.kubernetes.io/name: alloy-singleton
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.6.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-singleton
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-singleton
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-singleton
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.6.1
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - SETPCAP
              - NET_BIND_SERVICE
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.14.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-singleton
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.28.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.14.0"
    release: k8smon
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: k8smon
  replicas: 1
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.28.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.14.0"
        release: k8smon
      annotations:
      
        k8s.grafana.com/logs.job: integrations/kubernetes/kube-state-metrics
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: k8smon-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --metric-labels-allowlist=nodes=[agentpool,alpha.eksctl.io/cluster-name,alpha.eksctl.io/nodegroup-name,beta.kubernetes.io/instance-type,cloud.google.com/gke-nodepool,cluster_name,ec2_amazonaws_com_Name,ec2_amazonaws_com_aws_autoscaling_groupName,ec2_amazonaws_com_aws_autoscaling_group_name,ec2_amazonaws_com_name,eks_amazonaws_com_nodegroup,k8s_io_cloud_provider_aws,karpenter.sh/nodepool,kubernetes.azure.com/cluster,kubernetes.io/arch,kubernetes.io/hostname,kubernetes.io/os,node.kubernetes.io/instance-type,topology.kubernetes.io/region,topology.kubernetes.io/zone]
        - --namespaces=logs,o11y
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.14.0
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
