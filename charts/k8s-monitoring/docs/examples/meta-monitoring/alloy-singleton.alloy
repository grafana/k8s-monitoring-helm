// Destination: prometheus (prometheus)
otelcol.exporter.prometheus "prometheus" {
  add_metric_suffixes = true
  forward_to = [prometheus.remote_write.prometheus.receiver]
}

prometheus.remote_write "prometheus" {
  endpoint {
    url = "http://prometheus.prometheus.svc:9090/api/v1/write"
    headers = {
    }
    tls_config {
      insecure_skip_verify = false
    }
    send_native_histograms = false

    queue_config {
      capacity = 10000
      min_shards = 1
      max_shards = 50
      max_samples_per_send = 2000
      batch_send_deadline = "5s"
      min_backoff = "30ms"
      max_backoff = "5s"
      retry_on_http_429 = true
      sample_age_limit = "0s"
    }

    write_relabel_config {
      source_labels = ["cluster"]
      regex = ""
      replacement = "loki-meta-monitoring-cluster"
      target_label = "cluster"
    }
    write_relabel_config {
      source_labels = ["k8s.cluster.name"]
      regex = ""
      replacement = "loki-meta-monitoring-cluster"
      target_label = "cluster"
    }
  }

  wal {
    truncate_frequency = "2h"
    min_keepalive_time = "5m"
    max_keepalive_time = "8h"
  }
}
// Destination: loki (loki)
otelcol.exporter.loki "loki" {
  forward_to = [loki.write.loki.receiver]
}

loki.write "loki" {
  endpoint {
    url = "http://loki.loki.svc:3100/api/push"
    tls_config {
      insecure_skip_verify = false
    }
  }
  external_labels = {
    cluster = "loki-meta-monitoring-cluster",
    "k8s_cluster_name" = "loki-meta-monitoring-cluster",
  }
}
// Destination: loki (otlp)

otelcol.processor.attributes "loki" {
  output {
    metrics = [otelcol.processor.transform.loki.input]
    logs = [otelcol.processor.transform.loki.input]
    traces = [otelcol.processor.transform.loki.input]
  }
}

otelcol.processor.transform "loki" {
  error_mode = "ignore"
  metric_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "loki-meta-monitoring-cluster")`,
      `set(attributes["k8s.cluster.name"], "loki-meta-monitoring-cluster")`,
    ]
  }

  metric_statements {
    context = "datapoint"
    statements = [
      `set(attributes["cluster"], "loki-meta-monitoring-cluster")`,
      `set(attributes["k8s.cluster.name"], "loki-meta-monitoring-cluster")`,
    ]
  }
  log_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "loki-meta-monitoring-cluster")`,
      `set(attributes["k8s.cluster.name"], "loki-meta-monitoring-cluster")`,
    ]
  }

  log_statements {
    context = "log"
    statements = [
      `delete_key(attributes, "loki.attribute.labels")`,
      `set(resource.attributes["service.name"], attributes["service_name"]) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
      `delete_key(attributes, "service_name") where attributes["service_name"] != nil`,
      `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
      `delete_key(attributes, "service_namespace") where attributes["service_namespace"] != nil`,
      `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
      `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] != nil`,
      `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
      `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] != nil`,
    ]
  }

  trace_statements {
    context = "resource"
    statements = [
      `set(attributes["cluster"], "loki-meta-monitoring-cluster")`,
      `set(attributes["k8s.cluster.name"], "loki-meta-monitoring-cluster")`,
    ]
  }

  output {
    metrics = [otelcol.processor.batch.loki.input]
    logs = [otelcol.processor.batch.loki.input]
    traces = [otelcol.processor.batch.loki.input]
  }
}

otelcol.processor.batch "loki" {
  timeout = "2s"
  send_batch_size = 8192
  send_batch_max_size = 0

  output {
    metrics = [otelcol.exporter.otlphttp.loki.input]
    logs = [otelcol.exporter.otlphttp.loki.input]
    traces = [otelcol.exporter.otlphttp.loki.input]
  }
}
otelcol.exporter.otlphttp "loki" {
  client {
    endpoint = "http://otlp-gateway.svc:443/otlp"
    tls {
      insecure = false
      insecure_skip_verify = false
    }
  }
}
// Feature: Cluster Metrics
declare "cluster_metrics" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }
  discovery.kubernetes "nodes" {
    role = "node"
  }

  // cAdvisor
  discovery.relabel "cadvisor" {
    targets = discovery.kubernetes.nodes.targets
    rule {
      replacement   = "/metrics/cadvisor"
      target_label  = "__metrics_path__"
    }
    rule {
      source_labels = ["__meta_kubernetes_node_name"]
      target_label  = "node"
    }
    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_node_label_app_kubernetes_io_name",
        "__meta_kubernetes_node_label_k8s_app",
        "__meta_kubernetes_node_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
  }

  prometheus.scrape "cadvisor" {
    targets = discovery.relabel.cadvisor.output
    job_name = "integrations/kubernetes/cadvisor"
    scheme = "https"
    scrape_interval = "60s"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.cadvisor.receiver]
  }

  prometheus.relabel "cadvisor" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
      action = "keep"
    }
    // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","container"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
      action = "drop"
    }
    // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","image"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
      action = "drop"
    }
    // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
    rule {
      source_labels = ["__name__", "boot_id"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "boot_id"
      replacement = "NA"
    }
    rule {
      source_labels = ["__name__", "system_uuid"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "system_uuid"
      replacement = "NA"
    }
    // Filter out non-physical devices/interfaces
    rule {
      source_labels = ["__name__", "device"]
      separator = "@"
      regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_fs_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_fs_.*"
      target_label = "__keepme"
      replacement = ""
    }
    rule {
      source_labels = ["__name__", "interface"]
      separator = "@"
      regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_network_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_network_.*"
      target_label = "__keepme"
      replacement = ""
    }
    rule {
      action = "keep"
      source_labels = ["namespace"]
      regex = "collectors|logs|metrics|o11y"
    }
    forward_to = argument.metrics_destinations.value
  }
  discovery.kubernetes "kube_state_metrics" {
    role = "endpoints"

    selectors {
      role = "endpoints"
      label = "app.kubernetes.io/name=kube-state-metrics,release=k8smon"
    }
  }

  discovery.relabel "kube_state_metrics" {
    targets = discovery.kubernetes.kube_state_metrics.targets

    // only keep targets with a matching port name
    rule {
      source_labels = ["__meta_kubernetes_endpoint_port_name"]
      regex = "http"
      action = "keep"
    }

    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }

  }

  prometheus.scrape "kube_state_metrics" {
    targets = discovery.relabel.kube_state_metrics.output
    job_name = "integrations/kubernetes/kube-state-metrics"
    scrape_interval = "60s"
    scheme = "http"
    bearer_token_file = ""
    tls_config {
      insecure_skip_verify = true
    }

    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.kube_state_metrics.receiver]
  }

  prometheus.relabel "kube_state_metrics" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|(.+)"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }

  remote.kubernetes.configmap "node_exporter" {
    name = "k8smon-alloy-module-system"
    namespace = "default"
  }

  import.string "node_exporter" {
    content = remote.kubernetes.configmap.node_exporter.data["node-exporter_metrics.alloy"]
  }

  node_exporter.kubernetes "targets" {
    namespaces = ["default"]
    port_name = "metrics"
    label_selectors = [
      "app.kubernetes.io/name=node-exporter",
      "release=k8smon",
    ]
  }

  discovery.relabel "node_exporter" {
    targets = node_exporter.kubernetes.targets.output
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }

  node_exporter.scrape "metrics" {
    targets = discovery.relabel.node_exporter.output
    job_label = "integrations/node_exporter"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes|node_arp_entries|node_boot_time_seconds|node_context_switches_total|node_cpu_seconds_total|node_disk_io_time_seconds_total|node_disk_io_time_weighted_seconds_total|node_disk_read_bytes_total|node_disk_read_time_seconds_total|node_disk_reads_completed_total|node_disk_write_time_seconds_total|node_disk_writes_completed_total|node_disk_written_bytes_total|node_filefd_allocated|node_filefd_maximum|node_filesystem_avail_bytes|node_filesystem_device_error|node_filesystem_files|node_filesystem_files_free|node_filesystem_readonly|node_filesystem_size_bytes|node_intr_total|node_load1|node_load15|node_load5|node_md_disks|node_md_disks_required|node_memory_Active_anon_bytes|node_memory_Active_bytes|node_memory_Active_file_bytes|node_memory_AnonHugePages_bytes|node_memory_AnonPages_bytes|node_memory_Bounce_bytes|node_memory_Buffers_bytes|node_memory_Cached_bytes|node_memory_CommitLimit_bytes|node_memory_Committed_AS_bytes|node_memory_DirectMap1G_bytes|node_memory_DirectMap2M_bytes|node_memory_DirectMap4k_bytes|node_memory_Dirty_bytes|node_memory_HugePages_Free|node_memory_HugePages_Rsvd|node_memory_HugePages_Surp|node_memory_HugePages_Total|node_memory_Hugepagesize_bytes|node_memory_Inactive_anon_bytes|node_memory_Inactive_bytes|node_memory_Inactive_file_bytes|node_memory_Mapped_bytes|node_memory_MemAvailable_bytes|node_memory_MemFree_bytes|node_memory_MemTotal_bytes|node_memory_Shmem_bytes|node_memory_ShmemHugePages_bytes|node_memory_Slab_bytes|node_memory_SReclaimable_bytes|node_memory_SUnreclaim_bytes|node_memory_SwapTotal_bytes|node_memory_VmallocChunk_bytes|node_memory_VmallocTotal_bytes|node_memory_VmallocUsed_bytes|node_memory_Writeback_bytes|node_memory_WritebackTmp_bytes|node_netstat_Icmp_InErrors|node_netstat_Icmp_InMsgs|node_netstat_Icmp_OutMsgs|node_netstat_Icmp6_InErrors|node_netstat_Icmp6_InMsgs|node_netstat_Icmp6_OutMsgs|node_netstat_IpExt_InOctets|node_netstat_IpExt_OutOctets|node_netstat_Tcp_InErrs|node_netstat_Tcp_InSegs|node_netstat_Tcp_OutRsts|node_netstat_Tcp_OutSegs|node_netstat_Tcp_RetransSegs|node_netstat_TcpExt_ListenDrops|node_netstat_TcpExt_ListenOverflows|node_netstat_TcpExt_TCPSynRetrans|node_netstat_Udp_InDatagrams|node_netstat_Udp_InErrors|node_netstat_Udp_NoPorts|node_netstat_Udp_OutDatagrams|node_netstat_Udp_RcvbufErrors|node_netstat_Udp_SndbufErrors|node_netstat_Udp6_InDatagrams|node_netstat_Udp6_InErrors|node_netstat_Udp6_NoPorts|node_netstat_Udp6_OutDatagrams|node_netstat_Udp6_RcvbufErrors|node_netstat_Udp6_SndbufErrors|node_netstat_UdpLite_InErrors|node_network_carrier|node_network_info|node_network_mtu_bytes|node_network_receive_compressed_total|node_network_receive_errs_total|node_network_receive_fifo_total|node_network_receive_multicast_total|node_network_receive_packets_total|node_network_speed_bytes|node_network_transmit_compressed_total|node_network_transmit_errs_total|node_network_transmit_fifo_total|node_network_transmit_multicast_total|node_network_transmit_packets_total|node_network_transmit_queue_length|node_network_up|node_nf_conntrack_entries|node_nf_conntrack_entries_limit|node_os_info|node_procs_running|node_sockstat_FRAG_inuse|node_sockstat_FRAG6_inuse|node_sockstat_RAW_inuse|node_sockstat_RAW6_inuse|node_sockstat_sockets_used|node_sockstat_TCP_alloc|node_sockstat_TCP_inuse|node_sockstat_TCP_mem|node_sockstat_TCP_mem_bytes|node_sockstat_TCP_orphan|node_sockstat_TCP_tw|node_sockstat_TCP6_inuse|node_sockstat_UDP_inuse|node_sockstat_UDP_mem|node_sockstat_UDP_mem_bytes|node_sockstat_UDP6_inuse|node_sockstat_UDPLITE_inuse|node_sockstat_UDPLITE6_inuse|node_softnet_dropped_total|node_softnet_processed_total|node_softnet_times_squeezed_total|node_systemd_service_restart_total|node_systemd_unit_state|node_textfile_scrape_error|node_time_zone_offset_seconds|node_timex_estimated_error_seconds|node_timex_maxerror_seconds|node_timex_offset_seconds|node_timex_sync_status|node_uname_info|node_vmstat_oom_kill|node_vmstat_pgfault|node_vmstat_pgmajfault|node_vmstat_pgpgin|node_vmstat_pgpgout|node_vmstat_pswpin|node_vmstat_pswpout|process_max_fds|process_open_fds"
    scheme = "http"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
cluster_metrics "feature" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
// Feature: Cluster Events
declare "cluster_events" {
  argument "logs_destinations" {
    comment = "Must be a list of log destinations where collected logs should be forwarded to"
  }

  loki.source.kubernetes_events "cluster_events" {
    job_name   = "integrations/kubernetes/eventhandler"
    log_format = "logfmt"
    namespaces = ["collectors","logs","metrics","o11y"]
    forward_to = [loki.process.cluster_events.receiver]
  }

  loki.process "cluster_events" {

    // add a static source label to the logs so they can be differentiated / restricted if necessary
    stage.static_labels {
      values = {
        "source" = "kubernetes-events",
      }
    }

    // extract some of the fields from the log line, these could be used as labels, structured metadata, etc.
    stage.logfmt {
      mapping = {
        "component" = "sourcecomponent", // map the sourcecomponent field to component
        "kind" = "",
        "level" = "type", // most events don't have a level but they do have a "type" i.e. Normal, Warning, Error, etc.
        "name" = "",
        "node" = "sourcehost", // map the sourcehost field to node
      }
    }
    // set these values as labels, they may or may not be used as index labels in Loki as they can be dropped
    // prior to being written to Loki, but this makes them available
    stage.labels {
      values = {
        "component" = "",
        "kind" = "",
        "level" = "",
        "name" = "",
        "node" = "",
      }
    }

    // if kind=Node, set the node label by copying the instance label
    stage.match {
      selector = "{kind=\"Node\"}"

      stage.labels {
        values = {
          "node" = "name",
        }
      }
    }

    // set the level extracted key value as a normalized log level
    stage.match {
      selector = "{level=\"Normal\"}"

      stage.static_labels {
        values = {
          level = "Info",
        }
      }
    }

    // Only keep the labels that are defined in the `keepLabels` list.
    stage.label_keep {
      values = ["job","level","namespace","node","source"]
    }
    forward_to = argument.logs_destinations.value
  }
}
cluster_events "feature" {
  logs_destinations = [
    loki.write.loki.receiver,
  ]
}
// Feature: Pod Logs
declare "pod_logs" {
  argument "logs_destinations" {
    comment = "Must be a list of log destinations where collected logs should be forwarded to"
  }

  discovery.kubernetes "volume_gathering_pods" {
    role = "pod"
    selectors {
      role = "pod"
      field = "spec.nodeName=" + sys.env("HOSTNAME")
    }
  }

  discovery.relabel "volume_gathering_pods" {
    targets = discovery.kubernetes.volume_gathering_pods.targets
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      action = "replace"
      target_label = "namespace"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label = "pod"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label = "container"
    }

    rule {
      source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      replacement = "$1"
      target_label = "job"
    }

    // set the container runtime as a label
    rule {
      source_labels = ["__meta_kubernetes_pod_container_id"]
      regex = "^(\\S+):\\/\\/.+$"
      target_label = "tmp_container_runtime"
    }

    // make all labels on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_label_(.+)"
    }

    // make all annotations on the pod available to the pipeline as labels,
    // they are omitted before write to loki via stage.label_keep unless explicitly set
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_(.+)"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      target_label = "app_kubernetes_io_name"
    }

    // explicitly set service_name. if not set, loki will automatically try to populate a default.
    // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
    //
    // choose the first value found from the following ordered list:
    // - pod.annotation[resource.opentelemetry.io/service.name]
    // - pod.label[app.kubernetes.io/name]
    // - k8s.pod.name
    // - k8s.container.name
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_name",
        "__meta_kubernetes_pod_container_name",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "service_name"
    }

    // set resource attributes
    rule {
      action = "labelmap"
      regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
      separator = "/"
      replacement = "/var/log/pods/*$1/*.log"
      target_label = "__path__"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
      regex = "(.+)"
      target_label = "job"
    }
    rule {
      source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
      regex = "(.+)"
      target_label = "app_kubernetes_io_name"
    }
  }

  local.file_match "volume_gathering_pods" {
    path_targets = discovery.relabel.volume_gathering_pods.output
  }

  loki.source.file "volume_gathering_pods" {
    targets    = local.file_match.volume_gathering_pods.targets
    forward_to = [loki.process.pod_log_processor.receiver]
  }

  loki.process "pod_log_processor" {
    stage.match {
      selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
      // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
      stage.cri {}

      // Set the extract flags and stream values as labels
      stage.labels {
        values = {
          flags  = "",
          stream  = "",
        }
      }
    }

    stage.match {
      selector = "{tmp_container_runtime=\"docker\"}"
      // the docker processing stage extracts the following k/v pairs: log, stream, time
      stage.docker {}

      // Set the extract stream value as a label
      stage.labels {
        values = {
          stream  = "",
        }
      }
    }

    // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
    // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
    // container runtime label as it is no longer needed.
    stage.label_drop {
      values = [
        "filename",
        "tmp_container_runtime",
      ]
    }
    // Integration: Grafana
    stage.match {
      selector = "{job=\"integrations/grafana\",instance=\"grafana\",namespace=~\"o11y\"}"

      // extract some of the fields from the log line
      stage.logfmt {
        mapping = {
          "ts" = "t",
          "level" = "",
        }
      }

      // set the level as a label
      stage.labels {
        values = {
          level = "level",
        }
      }
      // reset the timestamp to the extracted value
      stage.timestamp {
        source = "ts"
        format = "RFC3339Nano"
      }
      // remove the timestamp from the log line
      stage.replace {
        expression = `(?:^|\s+)(t=\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[^ ]*\s+)`
        replace = ""
      }
      // drop certain log levels
      stage.drop {
        source = "level"
        expression = "(?i)(debug)"
        drop_counter_reason = "grafana-drop-log-level"
      }

    }

    // Integration: Loki
    stage.match {
      selector = "{integration=\"loki\",instance=\"loki\",namespace=~\"logs\"}"

      // extract some of the fields from the log line
      stage.logfmt {
        mapping = {
          "ts" = "",
          "level" = "",
          "caller" = "caller",
          "org_id" = "org_id",
          "tenant" = "tenant",
          "user" = "user",
        }
      }

      // set the level as a label
      stage.labels {
        values = {
          level = "level",
        }
      }
      // reset the timestamp to the extracted value
      stage.timestamp {
        source = "ts"
        format = "RFC3339Nano"
      }
      // remove the timestamp from the log line
      stage.replace {
        expression = `(?:^|\s+)(ts=\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[^ ]*\s+)`
        replace = ""
      }
      // clean up the caller to remove the line
      stage.replace {
        source = "caller"
        expression = "(:[0-9]+$)"
        replace = ""
      }
      // set the structured metadata values
      stage.structured_metadata {
        values = {
          "caller" = "caller",
          "org_id" = "org_id",
          "tenant" = "tenant",
          "user" = "user",
        }
      }
      // drop certain log levels
      stage.drop {
        source = "level"
        expression = "(?i)(debug)"
        drop_counter_reason = "loki-drop-log-level"
      }

    }

    // Integration: Mimir
    stage.match {
      selector = "{integration=\"mimir\",instance=\"mimir\",namespace=~\"metrics\"}"

      // extract some of the fields from the log line
      stage.logfmt {
        mapping = {
          "ts" = "",
          "level" = "",
          "caller" = "caller",
          "org_id" = "org_id",
          "tenant" = "tenant",
          "user" = "user",
        }
      }

      // set the level as a label
      stage.labels {
        values = {
          level = "level",
        }
      }
      // reset the timestamp to the extracted value
      stage.timestamp {
        source = "ts"
        format = "RFC3339Nano"
      }
      // remove the timestamp from the log line
      stage.replace {
        expression = `(?:^|\s+)(ts=\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[^ ]*\s+)`
        replace = ""
      }
      // clean up the caller to remove the line
      stage.replace {
        source = "caller"
        expression = "(:[0-9]+$)"
        replace = ""
      }
      // set the structured metadata values
      stage.structured_metadata {
        values = {
          "caller" = "caller",
          "org_id" = "org_id",
          "tenant" = "tenant",
          "user" = "user",
        }
      }
      // drop certain log levels
      stage.drop {
        source = "level"
        expression = "(?i)(debug)"
        drop_counter_reason = "mimir-drop-log-level"
      }

    }

    // Only keep the labels that are defined in the `keepLabels` list.
    stage.label_keep {
      values = ["app","app_kubernetes_io_name","component","container","job","level","namespace","pod","service_name","integration"]
    }

    forward_to = argument.logs_destinations.value
  }
}
pod_logs "feature" {
  logs_destinations = [
    loki.write.loki.receiver,
  ]
}
declare "alloy_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "alloy_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=alloy\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: http-metrics)"
      optional = true
    }

    // Alloy service discovery for all of the pods
    discovery.kubernetes "alloy_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = string.join(coalesce(argument.field_selectors.value, []), ",")
        label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=alloy"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }
    }

    // alloy relabelings (pre-scrape)
    discovery.relabel "alloy_pods" {
      targets = discovery.kubernetes.alloy_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "metrics") + "@Running@true@false"
        action = "keep"
      }



    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set the workload to the controller kind and name
    rule {
      action = "lowercase"
      source_labels = ["__meta_kubernetes_pod_controller_kind"]
      target_label  = "workload_type"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_controller_name"]
      target_label  = "workload"
    }

    // remove the hash from the ReplicaSet
    rule {
      source_labels = [
        "workload_type",
        "workload",
      ]
      separator = "/"
      regex = "replicaset/(.+)-.+$"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
    }

    export "output" {
      value = discovery.relabel.alloy_pods.output
    }
  }

  declare "alloy_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all Alloy metrics (default: integrations/alloy)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "alloy" {
      job_name = coalesce(argument.job_label.value, "integrations/alloy")
      forward_to = [prometheus.relabel.alloy.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // alloy metric relabelings (post-scrape)
    prometheus.relabel "alloy" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, ".*")
        action = "keep"
      }

      // remove the component_id label from any metric that starts with log_bytes or log_lines, these are custom metrics that are generated
      // as part of the log annotation modules in this repo
      rule {
        action = "replace"
        source_labels = ["__name__"]
        regex = "^log_(bytes|lines).+"
        replacement = ""
        target_label = "component_id"
      }

      // set the namespace label to that of the exported_namespace
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_namespace"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "namespace"
      }

      // set the pod label to that of the exported_pod
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_pod"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "pod"
      }

      // set the container label to that of the exported_container
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_container"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "container"
      }

      // set the job label to that of the exported_job
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_job"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "job"
      }

      // set the instance label to that of the exported_instance
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_instance"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "instance"
      }

      rule {
        action = "labeldrop"
        regex = "exported_(namespace|pod|container|job|instance)"
      }
    }
  }

  alloy_integration_discovery "alloy_in_logs" {
    port_name = "http-metrics"
    namespaces = ["logs"]
    label_selectors = ["app.kubernetes.io/name=alloy-singleton"]
  }

  alloy_integration_scrape  "alloy_in_logs" {
    targets = alloy_integration_discovery.alloy_in_logs.output
    job_label = "integrations/alloy"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }

  alloy_integration_discovery "alloy" {
    port_name = "http-metrics"
    namespaces = ["collectors"]
    label_selectors = ["app.kubernetes.io/name in (alloy-singleton,alloy-metrics,alloy-logs,alloy-profiles,alloy-receiver)"]
  }

  alloy_integration_scrape  "alloy" {
    targets = alloy_integration_discovery.alloy.output
    job_label = "integrations/alloy"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
alloy_integration "integration" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
declare "grafana_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "grafana_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=grafana\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: grafana)"
      optional = true
    }

    // grafana service discovery for all of the pods
    discovery.kubernetes "grafana_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = string.join(coalesce(argument.field_selectors.value, []), ",")
        label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=grafana"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }
    }

    // grafana relabelings (pre-scrape)
    discovery.relabel "grafana_pods" {
      targets = discovery.kubernetes.grafana_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "grafana") + "@Running@true@false"
        action = "keep"
      }



    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set the workload to the controller kind and name
    rule {
      action = "lowercase"
      source_labels = ["__meta_kubernetes_pod_controller_kind"]
      target_label  = "workload_type"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_controller_name"]
      target_label  = "workload"
    }

    // remove the hash from the ReplicaSet
    rule {
      source_labels = [
        "workload_type",
        "workload",
      ]
      separator = "/"
      regex = "replicaset/(.+)-.+$"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
    }

    export "output" {
      value = discovery.relabel.grafana_pods.output
    }
  }

  declare "grafana_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all Grafana metrics (default: integrations/grafana)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "grafana" {
      job_name = coalesce(argument.job_label.value, "integrations/grafana")
      forward_to = [prometheus.relabel.grafana.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // grafana metric relabelings (post-scrape)
    prometheus.relabel "grafana" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, "(.+)")
        action = "keep"
      }

      // the grafana-mixin expects the instance label to be the node name
      rule {
        source_labels = ["node"]
        target_label = "instance"
        replacement = "$1"
      }
      rule {
        action = "labeldrop"
        regex = "node"
      }
    }
  }

  grafana_integration_discovery "grafana" {
    namespaces = ["o11y"]
    label_selectors = ["app.kubernetes.io/name=grafana"]
    port_name = "grafana"
  }

  grafana_integration_scrape  "grafana" {
    targets = grafana_integration_discovery.grafana.output
    job_label = "integrations/grafana"
    clustering = true
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
grafana_integration "integration" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
declare "loki_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "loki_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=loki\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: http-metrics)"
      optional = true
    }

    // loki service discovery for all of the pods
    discovery.kubernetes "loki_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = string.join(coalesce(argument.field_selectors.value, []), ",")
        label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=loki"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }
    }

    // loki relabelings (pre-scrape)
    discovery.relabel "loki_pods" {
      targets = discovery.kubernetes.loki_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "http-metrics") + "@Running@true@false"
        action = "keep"
      }

      // the loki-mixin expects the job label to be namespace/component
      rule {
        source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_component"]
        separator = "/"
        target_label = "job"
      }



    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set the workload to the controller kind and name
    rule {
      action = "lowercase"
      source_labels = ["__meta_kubernetes_pod_controller_kind"]
      target_label  = "workload_type"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_controller_name"]
      target_label  = "workload"
    }

    // remove the hash from the ReplicaSet
    rule {
      source_labels = [
        "workload_type",
        "workload",
      ]
      separator = "/"
      regex = "replicaset/(.+)-.+$"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
    }

    export "output" {
      value = discovery.relabel.loki_pods.output
    }
  }

  declare "loki_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all Loki metrics (default: integrations/loki)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "loki" {
      job_name = coalesce(argument.job_label.value, "integrations/loki")
      forward_to = [prometheus.relabel.loki.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // loki metric relabelings (post-scrape)
    prometheus.relabel "loki" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, "(.+)")
        action = "keep"
      }

      // the loki-mixin expects the instance label to be the node name
      rule {
        source_labels = ["node"]
        target_label = "instance"
        replacement = "$1"
      }
      rule {
        action = "labeldrop"
        regex = "node"
      }

      // set the memcached exporter container name from container="exporter" to container="memcached"
      rule {
        source_labels = ["component", "container"]
        separator = ";"
        regex = "memcached-[^;]+;exporter"
        target_label = "container"
        replacement = "memcached"
      }
    }
  }

  loki_integration_discovery "loki" {
    namespaces = ["logs"]
    label_selectors = ["app.kubernetes.io/name=loki"]
    port_name = "http-metrics"
  }

  loki_integration_scrape  "loki" {
    targets = loki_integration_discovery.loki.output
    job_label = "integrations/loki"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|go_gc_cycles_total_gc_cycles_total|go_gc_duration_seconds|go_gc_duration_seconds_count|go_gc_duration_seconds_sum|go_gc_pauses_seconds_bucket|go_goroutines|go_memstats_heap_inuse_bytes|loki_azure_blob_request_duration_seconds_bucket|loki_azure_blob_request_duration_seconds_count|loki_bigtable_request_duration_seconds_bucket|loki_bigtable_request_duration_seconds_count|loki_bloom_blocks_cache_added_total|loki_bloom_blocks_cache_entries|loki_bloom_blocks_cache_evicted_total|loki_bloom_blocks_cache_fetched_total|loki_bloom_blocks_cache_usage_bytes|loki_bloom_chunks_indexed_total|loki_bloom_gateway_block_query_latency_seconds_bucket|loki_bloom_gateway_dequeue_duration_seconds_bucket|loki_bloom_gateway_filtered_chunks_sum|loki_bloom_gateway_filtered_series_sum|loki_bloom_gateway_inflight_tasks|loki_bloom_gateway_process_duration_seconds_bucket|loki_bloom_gateway_process_duration_seconds_count|loki_bloom_gateway_querier_chunks_filtered_total|loki_bloom_gateway_querier_chunks_skipped_total|loki_bloom_gateway_querier_chunks_total|loki_bloom_gateway_querier_series_filtered_total|loki_bloom_gateway_querier_series_skipped_total|loki_bloom_gateway_querier_series_total|loki_bloom_gateway_queue_duration_seconds_bucket|loki_bloom_gateway_queue_duration_seconds_count|loki_bloom_gateway_queue_duration_seconds_sum|loki_bloom_gateway_queue_length|loki_bloom_gateway_requested_chunks_sum|loki_bloom_gateway_requested_series_sum|loki_bloom_gateway_tasks_dequeued_bucket|loki_bloom_gateway_tasks_dequeued_total|loki_bloom_gateway_tasks_processed_total|loki_bloom_inserts_total|loki_bloom_recorder_chunks_total|loki_bloom_recorder_series_total|loki_bloom_size_bucket|loki_bloom_store_blocks_fetched_size_bytes_bucket|loki_bloom_store_blocks_fetched_sum|loki_bloom_store_download_queue_size_sum|loki_bloom_store_metas_fetched_bucket|loki_bloom_store_metas_fetched_size_bytes_bucket|loki_bloom_store_metas_fetched_sum|loki_bloom_tokens_total|loki_bloombuilder_blocks_created_total|loki_bloombuilder_blocks_reused_total|loki_bloombuilder_bytes_per_task_bucket|loki_bloombuilder_chunk_series_size_sum|loki_bloombuilder_metas_created_total|loki_bloombuilder_processing_task|loki_bloombuilder_series_per_task_bucket|loki_bloomplanner_blocks_deleted_total|loki_bloomplanner_connected_builders|loki_bloomplanner_inflight_tasks|loki_bloomplanner_metas_deleted_total|loki_bloomplanner_queue_length|loki_bloomplanner_retention_running|loki_bloomplanner_retention_time_seconds_bucket|loki_bloomplanner_tenant_tasks_completed|loki_bloomplanner_tenant_tasks_planned|loki_boltdb_shipper_compact_tables_operation_duration_seconds|loki_boltdb_shipper_compact_tables_operation_last_successful_run_timestamp_seconds|loki_boltdb_shipper_compact_tables_operation_total|loki_boltdb_shipper_request_duration_seconds_bucket|loki_boltdb_shipper_request_duration_seconds_count|loki_boltdb_shipper_request_duration_seconds_sum|loki_boltdb_shipper_retention_marker_count_total|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_bucket|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_count|loki_boltdb_shipper_retention_marker_table_processed_duration_seconds_sum|loki_boltdb_shipper_retention_marker_table_processed_total|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_bucket|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_count|loki_boltdb_shipper_retention_sweeper_chunk_deleted_duration_seconds_sum|loki_boltdb_shipper_retention_sweeper_marker_file_processing_current_time|loki_boltdb_shipper_retention_sweeper_marker_files_current|loki_build_info|loki_chunk_store_deduped_chunks_total|loki_chunk_store_index_entries_per_chunk_count|loki_chunk_store_index_entries_per_chunk_sum|loki_compactor_apply_retention_last_successful_run_timestamp_seconds|loki_compactor_apply_retention_operation_duration_seconds|loki_compactor_apply_retention_operation_total|loki_compactor_delete_requests_processed_total|loki_compactor_delete_requests_received_total|loki_compactor_deleted_lines|loki_compactor_load_pending_requests_attempts_total|loki_compactor_locked_table_successive_compaction_skips|loki_compactor_oldest_pending_delete_request_age_seconds|loki_compactor_pending_delete_requests_count|loki_consul_request_duration_seconds_bucket|loki_discarded_samples_total|loki_distributor_bytes_received_total|loki_distributor_ingester_append_failures_total|loki_distributor_lines_received_total|loki_distributor_structured_metadata_bytes_received_total|loki_dynamo_consumed_capacity_total|loki_dynamo_dropped_requests_total|loki_dynamo_failures_total|loki_dynamo_query_pages_count|loki_dynamo_request_duration_seconds_bucket|loki_dynamo_request_duration_seconds_count|loki_dynamo_throttled_total|loki_embeddedcache_entries|loki_embeddedcache_memory_bytes|loki_gcs_request_duration_seconds_bucket|loki_gcs_request_duration_seconds_count|loki_index_gateway_postfilter_chunks_sum|loki_index_gateway_prefilter_chunks_sum|loki_index_request_duration_seconds_bucket|loki_index_request_duration_seconds_count|loki_index_request_duration_seconds_sum|loki_ingester_chunk_age_seconds_bucket|loki_ingester_chunk_age_seconds_count|loki_ingester_chunk_age_seconds_sum|loki_ingester_chunk_bounds_hours_bucket|loki_ingester_chunk_bounds_hours_count|loki_ingester_chunk_bounds_hours_sum|loki_ingester_chunk_entries_bucket|loki_ingester_chunk_entries_count|loki_ingester_chunk_entries_sum|loki_ingester_chunk_size_bytes_bucket|loki_ingester_chunk_utilization_bucket|loki_ingester_chunk_utilization_count|loki_ingester_chunk_utilization_sum|loki_ingester_chunks_flushed_total|loki_ingester_flush_queue_length|loki_ingester_memory_chunks|loki_ingester_memory_streams|loki_ingester_streams_created_total|loki_memcache_request_duration_seconds_bucket|loki_memcache_request_duration_seconds_count|loki_panic_total|loki_prometheus_rule_group_rules|loki_request_duration_seconds_bucket|loki_request_duration_seconds_count|loki_request_duration_seconds_sum|loki_ruler_wal_appender_ready|loki_ruler_wal_disk_size|loki_ruler_wal_prometheus_remote_storage_highest_timestamp_in_seconds|loki_ruler_wal_prometheus_remote_storage_queue_highest_sent_timestamp_seconds|loki_ruler_wal_prometheus_remote_storage_samples_pending|loki_ruler_wal_prometheus_remote_storage_samples_total|loki_ruler_wal_samples_appended_total|loki_ruler_wal_storage_created_series_total|loki_s3_request_duration_seconds_bucket|loki_s3_request_duration_seconds_count"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
loki_integration "integration" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
declare "mimir_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "mimir_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=mimir\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: http-metrics)"
      optional = true
    }

    // mimir service discovery for all of the pods
    discovery.kubernetes "mimir_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = string.join(coalesce(argument.field_selectors.value, []), ",")
        label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=mimir"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }
    }

    // mimir relabelings (pre-scrape)
    discovery.relabel "mimir_pods" {
      targets = discovery.kubernetes.mimir_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "http-metrics") + "@Running@true@false"
        action = "keep"
      }

      // the mimir-mixin expects the job label to be namespace/component
      rule {
        source_labels = ["__meta_kubernetes_namespace","__meta_kubernetes_pod_label_app_kubernetes_io_component"]
        separator = "/"
        target_label = "job"
      }



    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set the workload to the controller kind and name
    rule {
      action = "lowercase"
      source_labels = ["__meta_kubernetes_pod_controller_kind"]
      target_label  = "workload_type"
    }

    rule {
      source_labels = ["__meta_kubernetes_pod_controller_name"]
      target_label  = "workload"
    }

    // remove the hash from the ReplicaSet
    rule {
      source_labels = [
        "workload_type",
        "workload",
      ]
      separator = "/"
      regex = "replicaset/(.+)-.+$"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
    }

    export "output" {
      value = discovery.relabel.mimir_pods.output
    }
  }

  declare "mimir_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all Mimir metrics (default: integrations/mimir)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "mimir" {
      job_name = coalesce(argument.job_label.value, "integrations/mimir")
      forward_to = [prometheus.relabel.mimir.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // mimir metric relabelings (post-scrape)
    prometheus.relabel "mimir" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, "(.+)")
        action = "keep"
      }

      // the mimir-mixin expects the instance label to be the node name
      rule {
        source_labels = ["node"]
        target_label = "instance"
        replacement = "$1"
      }
      rule {
        action = "labeldrop"
        regex = "node"
      }

      // set the memcached exporter container name from container="exporter" to container="memcached"
      rule {
        source_labels = ["component", "container"]
        separator = ";"
        regex = "memcached-[^;]+;exporter"
        target_label = "container"
        replacement = "memcached"
      }
    }
  }

  mimir_integration_discovery "mimir" {
    namespaces = ["metrics"]
    label_selectors = ["app.kubernetes.io/name=mimir"]
    port_name = "http-metrics"
  }

  mimir_integration_scrape  "mimir" {
    targets = mimir_integration_discovery.mimir.output
    job_label = "integrations/mimir"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|cortex_alertmanager_alerts|cortex_alertmanager_alerts_invalid_total|cortex_alertmanager_alerts_received_total|cortex_alertmanager_dispatcher_aggregation_groups|cortex_alertmanager_notification_latency_seconds_bucket|cortex_alertmanager_notification_latency_seconds_count|cortex_alertmanager_notification_latency_seconds_sum|cortex_alertmanager_notifications_failed_total|cortex_alertmanager_notifications_total|cortex_alertmanager_partial_state_merges_failed_total|cortex_alertmanager_partial_state_merges_total|cortex_alertmanager_ring_check_errors_total|cortex_alertmanager_silences|cortex_alertmanager_state_fetch_replica_state_failed_total|cortex_alertmanager_state_fetch_replica_state_total|cortex_alertmanager_state_initial_sync_completed_total|cortex_alertmanager_state_initial_sync_duration_seconds_bucket|cortex_alertmanager_state_initial_sync_duration_seconds_count|cortex_alertmanager_state_initial_sync_duration_seconds_sum|cortex_alertmanager_state_persist_failed_total|cortex_alertmanager_state_persist_total|cortex_alertmanager_state_replication_failed_total|cortex_alertmanager_state_replication_total|cortex_alertmanager_sync_configs_failed_total|cortex_alertmanager_sync_configs_total|cortex_alertmanager_tenants_discovered|cortex_alertmanager_tenants_owned|cortex_blockbuilder_consume_cycle_duration_seconds|cortex_blockbuilder_consumer_lag_records|cortex_blockbuilder_tsdb_compact_and_upload_failed_total|cortex_bucket_blocks_count|cortex_bucket_index_estimated_compaction_jobs|cortex_bucket_index_estimated_compaction_jobs_errors_total|cortex_bucket_index_last_successful_update_timestamp_seconds|cortex_bucket_store_block_drop_failures_total|cortex_bucket_store_block_drops_total|cortex_bucket_store_block_load_failures_total|cortex_bucket_store_block_loads_total|cortex_bucket_store_blocks_loaded|cortex_bucket_store_indexheader_lazy_load_duration_seconds_bucket|cortex_bucket_store_indexheader_lazy_load_duration_seconds_count|cortex_bucket_store_indexheader_lazy_load_duration_seconds_sum|cortex_bucket_store_indexheader_lazy_load_total|cortex_bucket_store_indexheader_lazy_unload_total|cortex_bucket_store_series_batch_preloading_load_duration_seconds_sum|cortex_bucket_store_series_batch_preloading_wait_duration_seconds_sum|cortex_bucket_store_series_blocks_queried_sum|cortex_bucket_store_series_data_size_fetched_bytes_sum|cortex_bucket_store_series_data_size_touched_bytes_sum|cortex_bucket_store_series_hash_cache_hits_total|cortex_bucket_store_series_hash_cache_requests_total|cortex_bucket_store_series_request_stage_duration_seconds_bucket|cortex_bucket_store_series_request_stage_duration_seconds_count|cortex_bucket_store_series_request_stage_duration_seconds_sum|cortex_bucket_stores_blocks_last_successful_sync_timestamp_seconds|cortex_bucket_stores_gate_duration_seconds_bucket|cortex_bucket_stores_gate_duration_seconds_count|cortex_bucket_stores_gate_duration_seconds_sum|cortex_bucket_stores_tenants_synced|cortex_build_info|cortex_cache_memory_hits_total|cortex_cache_memory_requests_total|cortex_compactor_block_cleanup_failures_total|cortex_compactor_block_cleanup_last_successful_run_timestamp_seconds|cortex_compactor_block_max_time_delta_seconds_bucket|cortex_compactor_block_max_time_delta_seconds_count|cortex_compactor_block_max_time_delta_seconds_sum|cortex_compactor_blocks_cleaned_total|cortex_compactor_blocks_marked_for_deletion_total|cortex_compactor_blocks_marked_for_no_compaction_total|cortex_compactor_disk_out_of_space_errors_total|cortex_compactor_group_compaction_runs_started_total|cortex_compactor_last_successful_run_timestamp_seconds|cortex_compactor_meta_sync_duration_seconds_bucket|cortex_compactor_meta_sync_duration_seconds_count|cortex_compactor_meta_sync_duration_seconds_sum|cortex_compactor_meta_sync_failures_total|cortex_compactor_meta_syncs_total|cortex_compactor_runs_completed_total|cortex_compactor_runs_failed_total|cortex_compactor_runs_started_total|cortex_compactor_tenants_discovered|cortex_compactor_tenants_processing_failed|cortex_compactor_tenants_processing_succeeded|cortex_compactor_tenants_skipped|cortex_config_hash|cortex_discarded_exemplars_total|cortex_discarded_requests_total|cortex_discarded_samples_total|cortex_distributor_deduped_samples_total|cortex_distributor_exemplars_in_total|cortex_distributor_inflight_push_requests|cortex_distributor_instance_limits|cortex_distributor_instance_rejected_requests_total|cortex_distributor_latest_seen_sample_timestamp_seconds|cortex_distributor_non_ha_samples_received_total|cortex_distributor_received_exemplars_total|cortex_distributor_received_requests_total|cortex_distributor_received_samples_total|cortex_distributor_replication_factor|cortex_distributor_requests_in_total|cortex_distributor_samples_in_total|cortex_inflight_requests|cortex_ingest_storage_reader_buffered_fetched_records|cortex_ingest_storage_reader_fetch_errors_total|cortex_ingest_storage_reader_fetches_total|cortex_ingest_storage_reader_missed_records_total|cortex_ingest_storage_reader_offset_commit_failures_total|cortex_ingest_storage_reader_offset_commit_requests_total|cortex_ingest_storage_reader_read_errors_total|cortex_ingest_storage_reader_receive_delay_seconds_count|cortex_ingest_storage_reader_receive_delay_seconds_sum|cortex_ingest_storage_reader_records_failed_total|cortex_ingest_storage_reader_records_total|cortex_ingest_storage_reader_requests_failed_total|cortex_ingest_storage_reader_requests_total|cortex_ingest_storage_strong_consistency_failures_total|cortex_ingest_storage_strong_consistency_requests_total|cortex_ingest_storage_writer_buffered_produce_bytes|cortex_ingest_storage_writer_buffered_produce_bytes_limit|cortex_ingester_active_native_histogram_buckets|cortex_ingester_active_native_histogram_buckets_custom_tracker|cortex_ingester_active_native_histogram_series|cortex_ingester_active_native_histogram_series_custom_tracker|cortex_ingester_active_series|cortex_ingester_active_series_custom_tracker|cortex_ingester_client_request_duration_seconds_bucket|cortex_ingester_client_request_duration_seconds_count|cortex_ingester_client_request_duration_seconds_sum|cortex_ingester_ingested_exemplars_total|cortex_ingester_ingested_samples_total|cortex_ingester_instance_limits|cortex_ingester_instance_rejected_requests_total|cortex_ingester_local_limits|cortex_ingester_memory_series|cortex_ingester_memory_series_created_total|cortex_ingester_memory_series_removed_total|cortex_ingester_memory_users|cortex_ingester_oldest_unshipped_block_timestamp_seconds|cortex_ingester_owned_series|cortex_ingester_queried_exemplars_bucket|cortex_ingester_queried_exemplars_count|cortex_ingester_queried_exemplars_sum|cortex_ingester_queried_samples_bucket|cortex_ingester_queried_samples_count|cortex_ingester_queried_samples_sum|cortex_ingester_queried_series_bucket|cortex_ingester_queried_series_count|cortex_ingester_queried_series_sum|cortex_ingester_shipper_last_successful_upload_timestamp_seconds|cortex_ingester_shipper_upload_failures_total|cortex_ingester_shipper_uploads_total|cortex_ingester_tsdb_checkpoint_creations_failed_total|cortex_ingester_tsdb_checkpoint_creations_total|cortex_ingester_tsdb_checkpoint_deletions_failed_total|cortex_ingester_tsdb_compaction_duration_seconds_bucket|cortex_ingester_tsdb_compaction_duration_seconds_count|cortex_ingester_tsdb_compaction_duration_seconds_sum|cortex_ingester_tsdb_compactions_failed_total|cortex_ingester_tsdb_compactions_total|cortex_ingester_tsdb_exemplar_exemplars_appended_total|cortex_ingester_tsdb_exemplar_exemplars_in_storage|cortex_ingester_tsdb_exemplar_last_exemplars_timestamp_seconds|cortex_ingester_tsdb_exemplar_series_with_exemplars_in_storage|cortex_ingester_tsdb_head_max_timestamp_seconds|cortex_ingester_tsdb_head_truncations_failed_total|cortex_ingester_tsdb_mmap_chunk_corruptions_total|cortex_ingester_tsdb_out_of_order_samples_appended_total|cortex_ingester_tsdb_storage_blocks_bytes|cortex_ingester_tsdb_symbol_table_size_bytes|cortex_ingester_tsdb_wal_corruptions_total|cortex_ingester_tsdb_wal_truncate_duration_seconds_count|cortex_ingester_tsdb_wal_truncate_duration_seconds_sum|cortex_ingester_tsdb_wal_truncations_failed_total|cortex_ingester_tsdb_wal_truncations_total|cortex_ingester_tsdb_wal_writes_failed_total|cortex_kv_request_duration_seconds_bucket|cortex_kv_request_duration_seconds_count|cortex_kv_request_duration_seconds_sum|cortex_lifecycler_read_only|cortex_limits_defaults|cortex_limits_overrides|cortex_partition_ring_partitions|cortex_prometheus_notifications_dropped_total|cortex_prometheus_notifications_errors_total|cortex_prometheus_notifications_queue_capacity|cortex_prometheus_notifications_queue_length|cortex_prometheus_notifications_sent_total|cortex_prometheus_rule_evaluation_duration_seconds_count|cortex_prometheus_rule_evaluation_duration_seconds_sum|cortex_prometheus_rule_evaluation_failures_total|cortex_prometheus_rule_evaluations_total|cortex_prometheus_rule_group_duration_seconds_count|cortex_prometheus_rule_group_duration_seconds_sum|cortex_prometheus_rule_group_iterations_missed_total|cortex_prometheus_rule_group_iterations_total|cortex_prometheus_rule_group_rules|cortex_querier_blocks_consistency_checks_failed_total|cortex_querier_blocks_consistency_checks_total|cortex_querier_request_duration_seconds_bucket|cortex_querier_request_duration_seconds_count|cortex_querier_request_duration_seconds_sum|cortex_querier_storegateway_instances_hit_per_query_bucket|cortex_querier_storegateway_instances_hit_per_query_count|cortex_querier_storegateway_instances_hit_per_query_sum|cortex_querier_storegateway_refetches_per_query_bucket|cortex_querier_storegateway_refetches_per_query_count|cortex_querier_storegateway_refetches_per_query_sum|cortex_query_frontend_queries_total|cortex_query_frontend_queue_duration_seconds_bucket|cortex_query_frontend_queue_duration_seconds_count|cortex_query_frontend_queue_duration_seconds_sum|cortex_query_frontend_queue_length|cortex_query_frontend_retries_bucket|cortex_query_frontend_retries_count|cortex_query_frontend_retries_sum|cortex_query_scheduler_connected_querier_clients|cortex_query_scheduler_querier_inflight_requests|cortex_query_scheduler_queue_duration_seconds_bucket|cortex_query_scheduler_queue_duration_seconds_count|cortex_query_scheduler_queue_duration_seconds_sum|cortex_query_scheduler_queue_length|cortex_request_duration_seconds|cortex_request_duration_seconds_bucket|cortex_request_duration_seconds_count|cortex_request_duration_seconds_sum|cortex_ring_members|cortex_ruler_managers_total|cortex_ruler_queries_failed_total|cortex_ruler_queries_total|cortex_ruler_ring_check_errors_total|cortex_ruler_write_requests_failed_total|cortex_ruler_write_requests_total|cortex_runtime_config_hash|cortex_runtime_config_last_reload_successful|cortex_tcp_connections|cortex_tcp_connections_limit|go_memstats_heap_inuse_bytes|keda_scaler_errors|keda_scaler_metrics_value|kube_deployment_spec_replicas|kube_deployment_status_replicas_unavailable|kube_deployment_status_replicas_updated|kube_endpoint_address|kube_horizontalpodautoscaler_spec_target_metric|kube_horizontalpodautoscaler_status_condition|kube_pod_info|kube_statefulset_replicas|kube_statefulset_status_current_revision|kube_statefulset_status_replicas_current|kube_statefulset_status_replicas_ready|kube_statefulset_status_replicas_updated|kube_statefulset_status_update_revision|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_used_bytes|memberlist_client_cluster_members_count|memcached_limit_bytes|mimir_continuous_test_queries_failed_total|mimir_continuous_test_query_result_checks_failed_total|mimir_continuous_test_writes_failed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|process_memory_map_areas|process_memory_map_areas_limit|prometheus_tsdb_compaction_duration_seconds_bucket|prometheus_tsdb_compaction_duration_seconds_count|prometheus_tsdb_compaction_duration_seconds_sum|prometheus_tsdb_compactions_total|rollout_operator_last_successful_group_reconcile_timestamp_seconds|thanos_cache_hits_total|thanos_cache_operation_duration_seconds_bucket|thanos_cache_operation_duration_seconds_count|thanos_cache_operation_duration_seconds_sum|thanos_cache_operation_failures_total|thanos_cache_operations_total|thanos_cache_requests_total|thanos_objstore_bucket_last_successful_upload_time|thanos_objstore_bucket_operation_duration_seconds_bucket|thanos_objstore_bucket_operation_duration_seconds_count|thanos_objstore_bucket_operation_duration_seconds_sum|thanos_objstore_bucket_operation_failures_total|thanos_objstore_bucket_operations_total|thanos_store_index_cache_hits_total|thanos_store_index_cache_requests_total"
    scrape_interval = "60s"
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
mimir_integration "integration" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
// Self Reporting
prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
  set_collectors = ["textfile"]
  textfile {
    directory = "/etc/alloy"
  }
}

discovery.relabel "kubernetes_monitoring_telemetry" {
  targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
  rule {
    target_label = "instance"
    action = "replace"
    replacement = "k8smon"
  }
  rule {
    target_label = "job"
    action = "replace"
    replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
  }
}

prometheus.scrape "kubernetes_monitoring_telemetry" {
  job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
  targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
  scrape_interval = "60s"
  clustering {
    enabled = true
  }
  forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
}

prometheus.relabel "kubernetes_monitoring_telemetry" {
  rule {
    source_labels = ["__name__"]
    regex = "grafana_kubernetes_monitoring_.*"
    action = "keep"
  }
  forward_to = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
