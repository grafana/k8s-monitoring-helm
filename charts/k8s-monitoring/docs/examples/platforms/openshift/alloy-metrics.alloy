// Feature: Cluster Metrics
declare "cluster_metrics" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }
  discovery.kubernetes "nodes" {
    role = "node"
  }

  discovery.relabel "nodes" {
    targets = discovery.kubernetes.nodes.targets
    rule {
      source_labels = ["__meta_kubernetes_node_name"]
      target_label  = "node"
    }

    rule {
      replacement = "kubernetes"
      target_label = "source"
    }

  }

  // Kubelet
  discovery.relabel "kubelet" {
    targets = discovery.relabel.nodes.output
  }

  prometheus.scrape "kubelet" {
    targets  = discovery.relabel.kubelet.output
    job_name = "integrations/kubernetes/kubelet"
    scheme   = "https"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.kubelet.receiver]
  }

  prometheus.relabel "kubelet" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
      action = "keep"
    }

    forward_to = argument.metrics_destinations.value
  }

  // Kubelet Resources
  discovery.relabel "kubelet_resources" {
    targets = discovery.relabel.nodes.output
    rule {
      replacement   = "/metrics/resource"
      target_label  = "__metrics_path__"
    }
  }

  prometheus.scrape "kubelet_resources" {
    targets = discovery.relabel.kubelet_resources.output
    job_name = "integrations/kubernetes/resources"
    scheme   = "https"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.kubelet_resources.receiver]
  }

  prometheus.relabel "kubelet_resources" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
      action = "keep"
    }

    forward_to = argument.metrics_destinations.value
  }

  // cAdvisor
  discovery.relabel "cadvisor" {
    targets = discovery.relabel.nodes.output
    rule {
      replacement   = "/metrics/cadvisor"
      target_label  = "__metrics_path__"
    }
  }

  prometheus.scrape "cadvisor" {
    targets = discovery.relabel.cadvisor.output
    job_name = "integrations/kubernetes/cadvisor"
    scheme = "https"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"

    tls_config {
      ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
      insecure_skip_verify = true
      server_name = "kubernetes"
    }

    clustering {
      enabled = true
    }

    forward_to = [prometheus.relabel.cadvisor.receiver]
  }

  prometheus.relabel "cadvisor" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
      action = "keep"
    }
    // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","container"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
      action = "drop"
    }
    // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
    rule {
      source_labels = ["__name__","image"]
      separator = "@"
      regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
      action = "drop"
    }
    // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
    rule {
      source_labels = ["__name__", "boot_id"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "boot_id"
      replacement = "NA"
    }
    rule {
      source_labels = ["__name__", "system_uuid"]
      separator = "@"
      regex = "machine_memory_bytes@.*"
      target_label = "system_uuid"
      replacement = "NA"
    }
    // Filter out non-physical devices/interfaces
    rule {
      source_labels = ["__name__", "device"]
      separator = "@"
      regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_fs_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_fs_.*"
      target_label = "__keepme"
      replacement = ""
    }
    rule {
      source_labels = ["__name__", "interface"]
      separator = "@"
      regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
      target_label = "__keepme"
      replacement = "1"
    }
    rule {
      source_labels = ["__name__", "__keepme"]
      separator = "@"
      regex = "container_network_.*@"
      action = "drop"
    }
    rule {
      source_labels = ["__name__"]
      regex = "container_network_.*"
      target_label = "__keepme"
      replacement = ""
    }
    forward_to = argument.metrics_destinations.value
  }
  discovery.kubernetes "kube_state_metrics" {
    role = "endpoints"

    selectors {
      role = "endpoints"
      label = "app.kubernetes.io/name=kube-state-metrics"
    }
    namespaces {
      names = ["openshift-monitoring"]
    }
  }

  discovery.relabel "kube_state_metrics" {
    targets = discovery.kubernetes.kube_state_metrics.targets

    // only keep targets with a matching port name
    rule {
      source_labels = ["__meta_kubernetes_pod_container_port_name"]
      regex = "https-main"
      action = "keep"
    }

    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }

  }

  prometheus.scrape "kube_state_metrics" {
    targets = discovery.relabel.kube_state_metrics.output
    job_name = "integrations/kubernetes/kube-state-metrics"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    scheme = "https"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    tls_config {
      insecure_skip_verify = true
    }

    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.kube_state_metrics.receiver]
  }

  prometheus.relabel "kube_state_metrics" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_pod_init_.*|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }

  // Node Exporter
  discovery.kubernetes "node_exporter" {
    role = "pod"

    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=node-exporter"
    }
    namespaces {
      names = ["openshift-monitoring"]
    }
  }

  discovery.relabel "node_exporter" {
    targets = discovery.kubernetes.node_exporter.targets

    // keep only the specified metrics port name, and pods that are Running and ready
    rule {
      source_labels = [
        "__meta_kubernetes_pod_container_port_name",
        "__meta_kubernetes_pod_container_init",
        "__meta_kubernetes_pod_phase",
        "__meta_kubernetes_pod_ready",
      ]
      separator = "@"
      regex = "https@false@Running@true"
      action = "keep"
    }

    // Set the instance label to the node name
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }

    // set the namespace label
    rule {
      source_labels = ["__meta_kubernetes_namespace"]
      target_label  = "namespace"
    }

    // set the pod label
    rule {
      source_labels = ["__meta_kubernetes_pod_name"]
      target_label  = "pod"
    }

    // set the container label
    rule {
      source_labels = ["__meta_kubernetes_pod_container_name"]
      target_label  = "container"
    }

    // set a workload label
    rule {
      source_labels = [
        "__meta_kubernetes_pod_controller_kind",
        "__meta_kubernetes_pod_controller_name",
      ]
      separator = "/"
      target_label  = "workload"
    }
    // remove the hash from the ReplicaSet
    rule {
      source_labels = ["workload"]
      regex = "(ReplicaSet/.+)-.+"
      target_label  = "workload"
    }

    // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_name",
        "__meta_kubernetes_pod_label_k8s_app",
        "__meta_kubernetes_pod_label_app",
      ]
      separator = ";"
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "app"
    }

    // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
    rule {
      action = "replace"
      source_labels = [
        "__meta_kubernetes_pod_label_app_kubernetes_io_component",
        "__meta_kubernetes_pod_label_k8s_component",
        "__meta_kubernetes_pod_label_component",
      ]
      regex = "^(?:;*)?([^;]+).*$"
      replacement = "$1"
      target_label = "component"
    }

    // set a source label
    rule {
      action = "replace"
      replacement = "kubernetes"
      target_label = "source"
    }
  }

  prometheus.scrape "node_exporter" {
    targets = discovery.relabel.node_exporter.output
    job_name = "integrations/node_exporter"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    scheme = "https"
    bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    tls_config {
      insecure_skip_verify = true
    }

    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.node_exporter.receiver]
  }

  prometheus.relabel "node_exporter" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
      action = "keep"
    }
    // Drop metrics for certain file systems
    rule {
      source_labels = ["__name__", "fstype"]
      separator = "@"
      regex = "node_filesystem.*@(ramfs|tmpfs)"
      action = "drop"
    }
    forward_to = argument.metrics_destinations.value
  }

  // Windows Exporter
  discovery.kubernetes "windows_exporter_pods" {
    role = "pod"
    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=windows-exporter,release=k8smon"
    }
    namespaces {
      names = ["default"]
    }
  }

  discovery.relabel "windows_exporter" {
    targets = discovery.kubernetes.windows_exporter_pods.targets

    // keep only the specified metrics port name, and pods that are Running and ready
    rule {
      source_labels = [
        "__meta_kubernetes_pod_container_port_name",
        "__meta_kubernetes_pod_container_init",
        "__meta_kubernetes_pod_phase",
        "__meta_kubernetes_pod_ready",
      ]
      separator = "@"
      regex = "metrics@false@Running@true"
      action = "keep"
    }

    // Set the instance label to the node name
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      target_label = "instance"
    }
  }

  prometheus.scrape "windows_exporter" {
    targets  = discovery.relabel.windows_exporter.output
    job_name   = "integrations/windows-exporter"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.windows_exporter.receiver]
  }

  prometheus.relabel "windows_exporter" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|windows_.*|node_cpu_seconds_total|node_filesystem_size_bytes|node_filesystem_avail_bytes|container_cpu_usage_seconds_total"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }

  discovery.kubernetes "kepler" {
    role = "pod"
    namespaces {
      own_namespace = true
    }
    selectors {
      role = "pod"
      label = "app.kubernetes.io/name=kepler"
    }
  }

  discovery.relabel "kepler" {
    targets = discovery.kubernetes.kepler.targets
    rule {
      source_labels = ["__meta_kubernetes_pod_node_name"]
      action = "replace"
      target_label = "instance"
    }
  }

  prometheus.scrape "kepler" {
    targets      = discovery.relabel.kepler.output
    job_name     = "integrations/kepler"
    honor_labels = true
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    clustering {
      enabled = true
    }
    forward_to = [prometheus.relabel.kepler.receiver]
  }

  prometheus.relabel "kepler" {
    max_cache_size = 100000
    rule {
      source_labels = ["__name__"]
      regex = "up|scrape_samples_scraped|kepler_.*"
      action = "keep"
    }
    forward_to = argument.metrics_destinations.value
  }
}
cluster_metrics "feature" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}
declare "alloy_integration" {
  argument "metrics_destinations" {
    comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
  }

  declare "alloy_integration_discovery" {
    argument "namespaces" {
      comment = "The namespaces to look for targets in (default: [] is all namespaces)"
      optional = true
    }

    argument "field_selectors" {
      comment = "The field selectors to use to find matching targets (default: [])"
      optional = true
    }

    argument "label_selectors" {
      comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=alloy\"])"
      optional = true
    }

    argument "port_name" {
      comment = "The of the port to scrape metrics from (default: http-metrics)"
      optional = true
    }

    // Alloy service discovery for all of the pods
    discovery.kubernetes "alloy_pods" {
      role = "pod"

      selectors {
        role = "pod"
        field = string.join(coalesce(argument.field_selectors.value, []), ",")
        label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=alloy"]), ",")
      }

      namespaces {
        names = coalesce(argument.namespaces.value, [])
      }

    }

    // alloy relabelings (pre-scrape)
    discovery.relabel "alloy_pods" {
      targets = discovery.kubernetes.alloy_pods.targets

      // keep only the specified metrics port name, and pods that are Running and ready
      rule {
        source_labels = [
          "__meta_kubernetes_pod_container_port_name",
          "__meta_kubernetes_pod_phase",
          "__meta_kubernetes_pod_ready",
          "__meta_kubernetes_pod_container_init",
        ]
        separator = "@"
        regex = coalesce(argument.port_name.value, "metrics") + "@Running@true@false"
        action = "keep"
      }

      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }

      // set the workload to the controller kind and name
      rule {
        action = "lowercase"
        source_labels = ["__meta_kubernetes_pod_controller_kind"]
        target_label  = "workload_type"
      }

      rule {
        source_labels = ["__meta_kubernetes_pod_controller_name"]
        target_label  = "workload"
      }

      // remove the hash from the ReplicaSet
      rule {
        source_labels = [
          "workload_type",
          "workload",
        ]
        separator = "/"
        regex = "replicaset/(.+)-.+$"
        target_label  = "workload"
      }

      // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
      rule {
        action = "replace"
        source_labels = [
          "__meta_kubernetes_pod_label_app_kubernetes_io_name",
          "__meta_kubernetes_pod_label_k8s_app",
          "__meta_kubernetes_pod_label_app",
        ]
        separator = ";"
        regex = "^(?:;*)?([^;]+).*$"
        replacement = "$1"
        target_label = "app"
      }

      // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
      rule {
        action = "replace"
        source_labels = [
          "__meta_kubernetes_pod_label_app_kubernetes_io_component",
          "__meta_kubernetes_pod_label_k8s_component",
          "__meta_kubernetes_pod_label_component",
        ]
        regex = "^(?:;*)?([^;]+).*$"
        replacement = "$1"
        target_label = "component"
      }

      // set a source label
      rule {
        action = "replace"
        replacement = "kubernetes"
        target_label = "source"
      }

    }

    export "output" {
      value = discovery.relabel.alloy_pods.output
    }
  }

  declare "alloy_integration_scrape" {
    argument "targets" {
      comment = "Must be a list() of targets"
    }

    argument "forward_to" {
      comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
    }

    argument "job_label" {
      comment = "The job label to add for all Alloy metrics (default: integrations/alloy)"
      optional = true
    }

    argument "keep_metrics" {
      comment = "A regular expression of metrics to keep (default: see below)"
      optional = true
    }

    argument "drop_metrics" {
      comment = "A regular expression of metrics to drop (default: see below)"
      optional = true
    }

    argument "scrape_interval" {
      comment = "How often to scrape metrics from the targets (default: 60s)"
      optional = true
    }

    argument "scrape_timeout" {
      comment = "The timeout for scraping metrics from the targets (default: 10s)"
      optional = true
    }

    argument "scrape_protocols" {
      comment = "The scrape protocols to use for scraping metrics"
      optional = true
    }

    argument "scrape_classic_histograms" {
      comment = "Whether to scrape classic histograms (default: false)."
      optional = true
    }

    argument "scrape_native_histograms" {
      comment = "Whether to scrape native histograms (default: false)."
      optional = true
    }

    argument "max_cache_size" {
      comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
      optional = true
    }

    argument "clustering" {
      comment = "Whether or not clustering should be enabled (default: false)"
      optional = true
    }

    prometheus.scrape "alloy" {
      job_name = coalesce(argument.job_label.value, "integrations/alloy")
      forward_to = [prometheus.relabel.alloy.receiver]
      targets = argument.targets.value
      scrape_interval = coalesce(argument.scrape_interval.value, "60s")
      scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
      scrape_protocols = argument.scrape_protocols.value
      scrape_classic_histograms = argument.scrape_classic_histograms.value
      scrape_native_histograms = argument.scrape_native_histograms.value

      clustering {
        enabled = coalesce(argument.clustering.value, false)
      }
    }

    // alloy metric relabelings (post-scrape)
    prometheus.relabel "alloy" {
      forward_to = argument.forward_to.value
      max_cache_size = coalesce(argument.max_cache_size.value, 100000)

      // drop metrics that match the drop_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.drop_metrics.value, "")
        action = "drop"
      }

      // keep only metrics that match the keep_metrics regex
      rule {
        source_labels = ["__name__"]
        regex = coalesce(argument.keep_metrics.value, ".*")
        action = "keep"
      }

      // remove the component_id label from any metric that starts with log_bytes or log_lines, these are custom metrics that are generated
      // as part of the log annotation modules in this repo
      rule {
        action = "replace"
        source_labels = ["__name__"]
        regex = "^log_(bytes|lines).+"
        replacement = ""
        target_label = "component_id"
      }

      // set the namespace label to that of the exported_namespace
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_namespace"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "namespace"
      }

      // set the pod label to that of the exported_pod
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_pod"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "pod"
      }

      // set the container label to that of the exported_container
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_container"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "container"
      }

      // set the job label to that of the exported_job
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_job"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "job"
      }

      // set the instance label to that of the exported_instance
      rule {
        action = "replace"
        source_labels = ["__name__", "exported_instance"]
        separator = "@"
        regex = "^log_(bytes|lines).+@(.+)"
        replacement = "$2"
        target_label = "instance"
      }

      rule {
        action = "labeldrop"
        regex = "exported_(namespace|pod|container|job|instance)"
      }
    }
  }

  alloy_integration_discovery "alloy" {
    port_name = "http-metrics"
    label_selectors = ["app.kubernetes.io/name in (alloy-metrics,alloy-singleton,alloy-logs)"]
  }

  alloy_integration_scrape "alloy" {
    targets = alloy_integration_discovery.alloy.output
    job_label = "integrations/alloy"
    clustering = true
    keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
    scrape_interval = "60s"
    scrape_timeout = "10s"
    scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
    scrape_classic_histograms = false
    scrape_native_histograms = false
    max_cache_size = 100000
    forward_to = argument.metrics_destinations.value
  }
}
alloy_integration "integration" {
  metrics_destinations = [
    prometheus.remote_write.prometheus.receiver,
  ]
}




// Destination: prometheus (prometheus)
otelcol.exporter.prometheus "prometheus" {
  add_metric_suffixes = true
  resource_to_telemetry_conversion = false
  forward_to = [prometheus.remote_write.prometheus.receiver]
}

prometheus.remote_write "prometheus" {
  endpoint {
    url = "http://prometheus.prometheus.svc:9090/api/v1/write"
    headers = {
    }
    tls_config {
      insecure_skip_verify = false
    }
    send_native_histograms = false

    queue_config {
      capacity = 10000
      min_shards = 1
      max_shards = 50
      max_samples_per_send = 2000
      batch_send_deadline = "5s"
      min_backoff = "30ms"
      max_backoff = "5s"
      retry_on_http_429 = true
      sample_age_limit = "0s"
    }

    write_relabel_config {
      source_labels = ["cluster"]
      regex = ""
      replacement = "openshift-cluster"
      target_label = "cluster"
    }
    write_relabel_config {
      source_labels = ["k8s_cluster_name"]
      regex = ""
      replacement = "openshift-cluster"
      target_label = "k8s_cluster_name"
    }
  }

  wal {
    truncate_frequency = "2h"
    min_keepalive_time = "5m"
    max_keepalive_time = "8h"
  }
}

