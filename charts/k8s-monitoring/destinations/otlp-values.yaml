---
# -- The name for this OTLP destination.
# @section -- General
name: ""

# -- The protocol for the OTLP destination.
# Options are "grpc" (default), "http".
# @section -- General
protocol: "grpc"

metrics:
  # -- Whether to send metrics to the OTLP destination.
  # @section -- Telemetry
  enabled: true

logs:
  # -- Whether to send logs to the OTLP destination.
  # @section -- Telemetry
  enabled: true

traces:
  # -- Whether to send traces to the OTLP destination.
  # @section -- Telemetry
  enabled: true

# -- The URL for the OTLP destination.
# @section -- General
url: ""

# -- Raw config for accessing the URL.
# @section -- General
urlFrom: ""

# -- HTTP proxy to send requests through, only when using the `http` protocol.
# @section -- General
proxyURL: ""

# -- The tenant ID for the OTLP destination.
# @section -- General
tenantId: ""
# -- The key for storing the tenant ID in the secret.
# @section -- General
tenantIdKey: "tenantId"
# -- Raw config for accessing the tenant ID.
# @section -- General
tenantIdFrom: ""

# -- Extra headers to be set when sending data.
# All values are treated as strings and automatically quoted.
# @section -- General
extraHeaders: {}
# -- Extra headers to be set when sending data through a dynamic reference.
# All values are treated as raw strings and not quoted.
# @section -- General
extraHeadersFrom: {}

# -- Labels to be set with the cluster name as the value.
# @section -- General
clusterLabels: [cluster, k8s.cluster.name]

auth:
  # -- The type of authentication to do.
  # Options are "none" (default), "basic", "bearerToken", "oauth2".
  # @section -- Authentication
  type: none

  # -- The username for basic authentication.
  # @section -- Authentication - Basic
  username: ""
  # -- The key for storing the username in the secret.
  # @section -- Authentication - Basic
  usernameKey: username
  # -- Raw config for accessing the username.
  # @section -- Authentication - Basic
  usernameFrom: ""

  # -- The password for basic authentication.
  # @section -- Authentication - Basic
  password: ""
  # -- The key for storing the password in the secret.
  # @section -- Authentication - Basic
  passwordKey: password
  # -- Raw config for accessing the password.
  # @section -- Authentication - Basic
  passwordFrom: ""

  # -- The bearer token for bearer token authentication.
  # @section -- Authentication - Bearer Token
  bearerToken: ""
  # -- The key for storing the bearer token in the secret.
  # @section -- Authentication - Bearer Token
  bearerTokenKey: bearerToken
  # -- Raw config for accessing the bearer token.
  # @section -- Authentication - Bearer Token
  bearerTokenFrom: ""
  # -- Path to a file that containers the bearer token.
  # @section -- Authentication - Bearer Token
  bearerTokenFile: ""

  # Authenticate to Prometheus using OAuth2
  # @section -- Authentication - OAuth2
  oauth2:
    # -- OAuth2 client ID
    # @section -- Authentication - OAuth2
    clientId: ""
    # -- The key for the client ID property in the secret
    # @section -- Authentication - OAuth2
    clientIdKey: clientId
    # -- Raw config for accessing the client ID
    # @section -- Authentication - OAuth2
    clientIdFrom: ""
    # -- OAuth2 client secret
    # @section -- Authentication - OAuth2
    clientSecret: ""
    # -- The key for the client secret property in the secret
    # @section -- Authentication - OAuth2
    clientSecretKey: clientSecret
    # -- Raw config for accessing the client secret
    # @section -- Authentication - OAuth2
    clientSecretFrom: ""
    # -- File containing the OAuth2 client secret.
    # @section -- Authentication - OAuth2
    clientSecretFile: ""
    # -- OAuth2 endpoint parameters
    # @section -- Authentication - OAuth2
    endpointParams: {}
    # -- HTTP proxy to send requests through.
    # @section -- Authentication - OAuth2
    proxyURL: ""
    # -- Comma-separated list of IP addresses, CIDR notations, and domain names to exclude from proxying.
    # @section -- Authentication - OAuth2
    noProxy: ""
    # -- Use the proxy URL indicated by environment variables.
    # @section -- Authentication - OAuth2
    proxyFromEnvironment: false
    # -- Specifies headers to send to proxies during CONNECT requests.
    # @section -- Authentication - OAuth2
    proxyConnectHeader: {}
    # -- List of scopes to authenticate with.
    # @section -- Authentication - OAuth2
    scopes: []
    # -- URL to fetch the token from.
    # @section -- Authentication - OAuth2
    tokenURL: ""

secret:
  # -- Whether to create a secret for this Prometheus destination.
  # @section -- Secret
  create: true
  # -- If true, skip secret creation and embed the credentials directly into the configuration.
  # @section -- Secret
  embed: false
  # -- The name of the secret to create.
  # @section -- Secret
  name: ""
  # -- The namespace for the secret.
  # @section -- Secret
  namespace: ""

tls:
  # -- Whether to use TLS for the OTLP destination.
  # @section -- TLS
  insecure: false

  # -- Disables validation of the server certificate.
  # @section -- TLS
  insecureSkipVerify: false

  # -- The CA certificate for the server (as a string).
  # @section -- TLS
  ca: ""
  # -- The CA certificate for the server (as a path to a file).
  # @section -- TLS
  caFile: ""
  # -- Raw config for accessing the server CA certificate.
  # @section -- TLS
  caFrom: ""

  # -- The client certificate for the server (as a string).
  # @section -- TLS
  cert: ""
  # -- The client certificate for the server (as a path to a file).
  # @section -- TLS
  certFile: ""
  # -- Raw config for accessing the client certificate.
  # @section -- TLS
  certFrom: ""

  # -- The client key for the server (as a string).
  # @section -- TLS
  key: ""
  # -- The client key for the server (as a path to a file).
  # @section -- TLS
  keyFile: ""
  # -- Raw config for accessing the client key.
  # @section -- TLS
  keyFrom: ""

# -- Size of the read buffer the gRPC client to use for reading server responses.
# @section -- General
readBufferSize: ""

# -- Size of the write buffer the gRPC client to use for writing requests.
# @section -- General
writeBufferSize: ""

retryOnFailure:
  # -- Should failed requests be retried?
  # @section -- General
  enabled: true
  # -- The initial time to wait before retrying a failed request to the OTLP destination.
  # @section -- General
  initialInterval: 5s
  # -- The maximum time to wait before retrying a failed request to the OTLP destination.
  # @section -- General
  maxInterval: 30s
  # -- The maximum amount of time to wait before discarding a failed batch.
  # @section -- General
  maxElapsedTime: 5m

# Processors to apply to the data before delivering it to its destination.
processors:
  tailSampling:
    # -- Apply tail sampling policies to the traces before delivering them to this destination. This will create an
    # additional Alloy instance to handle the tail sampling, and traces sent to this destination will be automatically
    # forwarded, using a load balancer component, to the new sampling Alloy instance.
    # @section -- Tail Sampling
    enabled: false

    # -- The tail sampling otlp receiver configuration.
    # @section -- Tail Sampling
    receiver:
      otlp:
        grpc:
          maxReceivedMessageSize: 4MB

    # -- Wait time since the first span of a trace before making a sampling decision.
    # @section -- Tail Sampling
    decisionWait: 15s

    # -- The decision cache for the tail sampling. When you use decision_cache, configure it with a much higher value
    # than num_traces so decisions for trace IDs are kept longer than the span data for the trace.
    # @section -- Tail Sampling
    decisionCache:
      # -- Configures amount of trace IDs to be kept in an LRU cache, persisting the "keep" decisions for traces that
      # may have already been released from memory. By default, the size is 0 and the cache is inactive.
      # @section -- Tail Sampling
      sampledCacheSize: 0
      # -- Configures amount of trace IDs to be kept in an LRU cache, persisting the "drop" decisions for traces that
      # may have already been released from memory. By default, the size is 0 and the cache is inactive.
      # @section -- Tail Sampling
      nonSampledCacheSize: 0

    # --  Determines the buffer size of the trace delete channel which is composed of trace IDs that are being deleted.
    # Default is 0, which means no buffer is used.
    # @section -- Tail Sampling
    numTraces: 0

    # -- Expected number of new traces (helps in allocating data structures).
    # @section -- Tail Sampling
    expectedNewTracesPerSec: 0

    # -- Tail sampling policies to apply.
    # @section -- Tail Sampling
    policies: []

    # -- Settings for the Alloy instance that will handle tail sampling.
    # @section -- Tail Sampling
    collector:
      alloy: {}
      controller:
        type: statefulset
        replicas: 2

  attributes:
    # -- Attribute processor actions
    # Format: { key: "", value: "", action: "", pattern: "", fromAttribute: "", fromContext: "", convertedType: "" }
    # Can also use `valueFrom` instead of value to use a raw reference.
    # @section -- Attributes Processor
    actions: []

  transform:
    # Metric transforms
    metrics:
      # -- Metric resource transforms
      # @section -- Transform Processor
      resource: []
      # -- Metric transforms
      # @section -- Transform Processor
      metric: []
      # -- Metric datapoint transforms
      # @section -- Transform Processor
      datapoint: []

      # -- Promote certain metric datapoint attributes to resource attributes. This is helpful for translating metric
      # data from Prometheus sources to OTLP format.
      # Format: `{ <datapoint attribute name>: <resource attribute name> }`.
      # Will not copy if the resource attribute already exists.
      # @section -- Transform Processor
      datapointToResource:
        service_name: service.name
        service_namespace: service.namespace
        deployment_environment_name: deployment.environment.name
        deployment_environment: deployment.environment

    # Log transforms
    logs:
      # -- Log resource transforms
      # @section -- Transform Processor
      resource: []
      # -- Log transforms
      # @section -- Transform Processor
      log: []

      # -- Promote certain log attributes to resource attributes. This is helpful for translating log data from Loki
      # sources to OTLP format.
      # Format: `{ <log attribute name>: <resource attribute name> }`.
      # Will not copy if the resource attribute already exists.
      # @section -- Transform Processor
      logToResource:
        service_name: service.name
        service_namespace: service.namespace
        deployment_environment_name: deployment.environment.name
        deployment_environment: deployment.environment

        container: k8s.container.name
        cronjob: k8s.cronjob.name
        daemonset: k8s.daemonset.name
        deployment: k8s.deployment.name
        job_name: k8s.job.name
        namespace: k8s.namespace.name
        pod: k8s.pod.name
        replicaset: k8s.replicaset.name
        statefulset: k8s.statefulset.name


    # Trace transforms
    traces:
      # -- Trace resource transforms
      # @section -- Transform Processor
      resource: []
      # -- Trace span transforms
      # @section -- Transform Processor
      span: []
      # -- Trace spanevent transforms
      # @section -- Transform Processor
      spanevent: []

  filters:
    # -- Enable the filter processor. Any rules that evaluate to true will drop the matching telemetry data.
    # @section -- Filter Processor
    enabled: false

    # -- Metric filters
    # @section -- Filter Processor
    metrics:
      metric: []
      datapoint: []

    # -- Log filters
    # @section -- Filter Processor
    logs:
      logRecord: []

    # -- Trace filters
    # @section -- Filter Processor
    traces:
      span: []
      spanevent: []

  batch:
    # -- Whether to use a batch processor.
    # @section -- Batch Processor
    enabled: true
    # -- Number of spans, metric data points, or log records after which a batch will be sent regardless of the timeout.
    # This setting acts as a trigger and does not affect the size of the batch. If you need to enforce batch size limit,
    # use `maxSize`.
    # @section -- Batch Processor
    size: 8192
    # -- Maximum number of spans, metric data points, or log records to send in a single batch. This number must be
    # greater than or equal to the `size` setting. If set to 0, the batch processor will not enforce a maximum size.
    # @section -- Batch Processor
    maxSize: 0
    # -- How long to wait before flushing the batch.
    # @section -- Batch Processor
    timeout: 2s
  memoryLimiter:
    # -- Whether to use a memory limiter.
    # @section -- Memory Limiter
    enabled: false
    # -- How often to check memory usage.
    # @section -- Memory Limiter
    checkInterval: 1s
    # -- Maximum amount of memory targeted to be allocated by the process heap.
    # @section -- Memory Limiter
    limit: 0MiB
