---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.9.9
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kepler/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-kepler
  namespace: default
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
automountServiceAccountToken: false
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/windows-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-windows-exporter
  namespace: default
  labels:
    helm.sh/chart: windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: windows-exporter
    app.kubernetes.io/name: windows-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "0.31.3"
    release: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
data:
  CLOUD_PROVIDER_API_KEY: "b3AzbmNvNTdvcDNOY281N09QM05jbzU3b3AzbmNvNTdvcDNOY281Nw=="
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/windows-exporter/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-windows-exporter
  namespace: default
  labels:
    helm.sh/chart: windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: windows-exporter
    app.kubernetes.io/name: windows-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "0.31.3"
    release: k8smon
data:
  config.yml: |
    collectors:
      enabled: cpu,container,logical_disk,memory,net,os
    collector:
      service:
        include: "containerd|kubelet"
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |
    // Feature: Auto-Instrumentation
    declare "auto_instrumentation" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      discovery.kubernetes "beyla_pods" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=beyla"
        }
      }
    
      discovery.relabel "beyla_pods" {
        targets = discovery.kubernetes.beyla_pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "beyla_applications" {
        targets         = discovery.relabel.beyla_pods.output
        honor_labels    = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    
      prometheus.scrape "beyla_internal" {
        targets         = discovery.relabel.beyla_pods.output
        metrics_path    = "/internal/metrics"
        job_name        = "integrations/beyla"
        honor_labels    = true
        scrape_interval = "60s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        clustering {
          enabled = true
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    auto_instrumentation "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    // Feature: Cluster Metrics
    declare "cluster_metrics" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      discovery.kubernetes "nodes" {
        role = "node"
      }
    
      discovery.relabel "nodes" {
        targets = discovery.kubernetes.nodes.targets
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        rule {
          replacement = "kubernetes"
          target_label = "source"
        }
    
      }
    
      // Kubelet
      discovery.relabel "kubelet" {
        targets = discovery.relabel.nodes.output
      }
    
      prometheus.scrape "kubelet" {
        targets  = discovery.relabel.kubelet.output
        job_name = "integrations/kubernetes/kubelet"
        scheme   = "https"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.kubelet.receiver]
      }
    
      prometheus.relabel "kubelet" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
          action = "keep"
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      // Kubelet Resources
      discovery.relabel "kubelet_resources" {
        targets = discovery.relabel.nodes.output
        rule {
          replacement   = "/metrics/resource"
          target_label  = "__metrics_path__"
        }
      }
    
      prometheus.scrape "kubelet_resources" {
        targets = discovery.relabel.kubelet_resources.output
        job_name = "integrations/kubernetes/resources"
        scheme   = "https"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.kubelet_resources.receiver]
      }
    
      prometheus.relabel "kubelet_resources" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
          action = "keep"
        }
    
        forward_to = argument.metrics_destinations.value
      }
    
      // cAdvisor
      discovery.relabel "cadvisor" {
        targets = discovery.relabel.nodes.output
        rule {
          replacement   = "/metrics/cadvisor"
          target_label  = "__metrics_path__"
        }
      }
    
      prometheus.scrape "cadvisor" {
        targets = discovery.relabel.cadvisor.output
        job_name = "integrations/kubernetes/cadvisor"
        scheme = "https"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = true
        }
    
        forward_to = [prometheus.relabel.cadvisor.receiver]
      }
    
      prometheus.relabel "cadvisor" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
          action = "keep"
        }
        // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","container"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
          action = "drop"
        }
        // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","image"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
          action = "drop"
        }
        // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
        rule {
          source_labels = ["__name__", "boot_id"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "boot_id"
          replacement = "NA"
        }
        rule {
          source_labels = ["__name__", "system_uuid"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "system_uuid"
          replacement = "NA"
        }
        // Filter out non-physical devices/interfaces
        rule {
          source_labels = ["__name__", "device"]
          separator = "@"
          regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_fs_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_fs_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          source_labels = ["__name__", "interface"]
          separator = "@"
          regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_network_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_network_.*"
          target_label = "__keepme"
          replacement = ""
        }
        forward_to = argument.metrics_destinations.value
      }
      discovery.kubernetes "kube_state_metrics" {
        role = "endpoints"
    
        selectors {
          role = "endpoints"
          label = "app.kubernetes.io/name=kube-state-metrics,release=k8smon"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "kube_state_metrics" {
        targets = discovery.kubernetes.kube_state_metrics.targets
    
        // only keep targets with a matching port name
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_name"]
          regex = "http"
          action = "keep"
        }
    
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
    
      }
    
      prometheus.scrape "kube_state_metrics" {
        targets = discovery.relabel.kube_state_metrics.output
        job_name = "integrations/kubernetes/kube-state-metrics"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        scheme = "http"
        bearer_token_file = ""
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.kube_state_metrics.receiver]
      }
    
      prometheus.relabel "kube_state_metrics" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_pod_init_.*|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Node Exporter
      discovery.kubernetes "node_exporter" {
        role = "pod"
    
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=node-exporter,release=k8smon"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "node_exporter" {
        targets = discovery.kubernetes.node_exporter.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_container_init",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = "metrics@false@Running@true"
          action = "keep"
        }
    
        // Set the instance label to the node name
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
    
        // set the namespace label
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
    
        // set the pod label
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
    
        // set the container label
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
    
        // set a workload label
        rule {
          source_labels = [
            "__meta_kubernetes_pod_controller_kind",
            "__meta_kubernetes_pod_controller_name",
          ]
          separator = "/"
          target_label  = "workload"
        }
        // remove the hash from the ReplicaSet
        rule {
          source_labels = ["workload"]
          regex = "(ReplicaSet/.+)-.+"
          target_label  = "workload"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_component",
            "__meta_kubernetes_pod_label_k8s_component",
            "__meta_kubernetes_pod_label_component",
          ]
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "component"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      prometheus.scrape "node_exporter" {
        targets = discovery.relabel.node_exporter.output
        job_name = "integrations/node_exporter"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        scheme = "http"
        bearer_token_file = ""
        tls_config {
          insecure_skip_verify = true
        }
    
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.node_exporter.receiver]
      }
    
      prometheus.relabel "node_exporter" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
          action = "keep"
        }
        // Drop metrics for certain file systems
        rule {
          source_labels = ["__name__", "fstype"]
          separator = "@"
          regex = "node_filesystem.*@(ramfs|tmpfs)"
          action = "drop"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      // Windows Exporter
      discovery.kubernetes "windows_exporter_pods" {
        role = "pod"
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=windows-exporter,release=k8smon"
        }
        namespaces {
          names = ["default"]
        }
      }
    
      discovery.relabel "windows_exporter" {
        targets = discovery.kubernetes.windows_exporter_pods.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_container_init",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = "metrics@false@Running@true"
          action = "keep"
        }
    
        // Set the instance label to the node name
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "instance"
        }
      }
    
      prometheus.scrape "windows_exporter" {
        targets  = discovery.relabel.windows_exporter.output
        job_name   = "integrations/windows-exporter"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.windows_exporter.receiver]
      }
    
      prometheus.relabel "windows_exporter" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|windows_.*|node_cpu_seconds_total|node_filesystem_size_bytes|node_filesystem_avail_bytes|container_cpu_usage_seconds_total"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      discovery.kubernetes "kepler" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=kepler"
        }
      }
    
      discovery.relabel "kepler" {
        targets = discovery.kubernetes.kepler.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "kepler" {
        targets      = discovery.relabel.kepler.output
        job_name     = "integrations/kepler"
        honor_labels = true
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.kepler.receiver]
      }
    
      prometheus.relabel "kepler" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|kepler_.*"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    
      discovery.kubernetes "opencost" {
        role = "pod"
        namespaces {
          own_namespace = true
        }
        selectors {
          role = "pod"
          label = "app.kubernetes.io/name=opencost"
        }
      }
    
      discovery.relabel "opencost" {
        targets = discovery.kubernetes.opencost.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          action = "replace"
          target_label = "instance"
        }
      }
    
      prometheus.scrape "opencost" {
        targets      = discovery.relabel.opencost.output
        job_name     = "integrations/opencost"
        honor_labels = true
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        clustering {
          enabled = true
        }
        forward_to = [prometheus.relabel.opencost.receiver]
      }
    
      prometheus.relabel "opencost" {
        max_cache_size = 100000
        rule {
          source_labels = ["__name__"]
          regex = "up|scrape_samples_scraped|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
          action = "keep"
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    cluster_metrics "feature" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    declare "alloy_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
    
      declare "alloy_integration_discovery" {
        argument "namespaces" {
          comment = "The namespaces to look for targets in (default: [] is all namespaces)"
          optional = true
        }
    
        argument "field_selectors" {
          comment = "The field selectors to use to find matching targets (default: [])"
          optional = true
        }
    
        argument "label_selectors" {
          comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=alloy\"])"
          optional = true
        }
    
        argument "port_name" {
          comment = "The of the port to scrape metrics from (default: http-metrics)"
          optional = true
        }
    
        // Alloy service discovery for all of the pods
        discovery.kubernetes "alloy_pods" {
          role = "pod"
    
          selectors {
            role = "pod"
            field = string.join(coalesce(argument.field_selectors.value, []), ",")
            label = string.join(coalesce(argument.label_selectors.value, ["app.kubernetes.io/name=alloy"]), ",")
          }
    
          namespaces {
            names = coalesce(argument.namespaces.value, [])
          }
    
        }
    
        // alloy relabelings (pre-scrape)
        discovery.relabel "alloy_pods" {
          targets = discovery.kubernetes.alloy_pods.targets
    
          // keep only the specified metrics port name, and pods that are Running and ready
          rule {
            source_labels = [
              "__meta_kubernetes_pod_container_port_name",
              "__meta_kubernetes_pod_phase",
              "__meta_kubernetes_pod_ready",
              "__meta_kubernetes_pod_container_init",
            ]
            separator = "@"
            regex = coalesce(argument.port_name.value, "metrics") + "@Running@true@false"
            action = "keep"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }
    
          // set the workload to the controller kind and name
          rule {
            action = "lowercase"
            source_labels = ["__meta_kubernetes_pod_controller_kind"]
            target_label  = "workload_type"
          }
    
          rule {
            source_labels = ["__meta_kubernetes_pod_controller_name"]
            target_label  = "workload"
          }
    
          // remove the hash from the ReplicaSet
          rule {
            source_labels = [
              "workload_type",
              "workload",
            ]
            separator = "/"
            regex = "replicaset/(.+)-.+$"
            target_label  = "workload"
          }
    
          // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
          rule {
            action = "replace"
            source_labels = [
              "__meta_kubernetes_pod_label_app_kubernetes_io_name",
              "__meta_kubernetes_pod_label_k8s_app",
              "__meta_kubernetes_pod_label_app",
            ]
            separator = ";"
            regex = "^(?:;*)?([^;]+).*$"
            replacement = "$1"
            target_label = "app"
          }
    
          // set the component if specified as metadata labels "component:" or "app.kubernetes.io/component:" or "k8s-component:"
          rule {
            action = "replace"
            source_labels = [
              "__meta_kubernetes_pod_label_app_kubernetes_io_component",
              "__meta_kubernetes_pod_label_k8s_component",
              "__meta_kubernetes_pod_label_component",
            ]
            regex = "^(?:;*)?([^;]+).*$"
            replacement = "$1"
            target_label = "component"
          }
    
          // set a source label
          rule {
            action = "replace"
            replacement = "kubernetes"
            target_label = "source"
          }
    
        }
    
        export "output" {
          value = discovery.relabel.alloy_pods.output
        }
      }
    
      declare "alloy_integration_scrape" {
        argument "targets" {
          comment = "Must be a list() of targets"
        }
    
        argument "forward_to" {
          comment = "Must be a list(MetricsReceiver) where collected metrics should be forwarded to"
        }
    
        argument "job_label" {
          comment = "The job label to add for all Alloy metrics (default: integrations/alloy)"
          optional = true
        }
    
        argument "keep_metrics" {
          comment = "A regular expression of metrics to keep (default: see below)"
          optional = true
        }
    
        argument "drop_metrics" {
          comment = "A regular expression of metrics to drop (default: see below)"
          optional = true
        }
    
        argument "scrape_interval" {
          comment = "How often to scrape metrics from the targets (default: 60s)"
          optional = true
        }
    
        argument "scrape_timeout" {
          comment = "The timeout for scraping metrics from the targets (default: 10s)"
          optional = true
        }
    
        argument "scrape_protocols" {
          comment = "The scrape protocols to use for scraping metrics"
          optional = true
        }
    
        argument "scrape_classic_histograms" {
          comment = "Whether to scrape classic histograms (default: false)."
          optional = true
        }
    
        argument "scrape_native_histograms" {
          comment = "Whether to scrape native histograms (default: false)."
          optional = true
        }
    
        argument "max_cache_size" {
          comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
          optional = true
        }
    
        argument "clustering" {
          comment = "Whether or not clustering should be enabled (default: false)"
          optional = true
        }
    
        prometheus.scrape "alloy" {
          job_name = coalesce(argument.job_label.value, "integrations/alloy")
          forward_to = [prometheus.relabel.alloy.receiver]
          targets = argument.targets.value
          scrape_interval = coalesce(argument.scrape_interval.value, "60s")
          scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
          scrape_protocols = argument.scrape_protocols.value
          scrape_classic_histograms = argument.scrape_classic_histograms.value
          scrape_native_histograms = argument.scrape_native_histograms.value
    
          clustering {
            enabled = coalesce(argument.clustering.value, false)
          }
        }
    
        // alloy metric relabelings (post-scrape)
        prometheus.relabel "alloy" {
          forward_to = argument.forward_to.value
          max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
          // drop metrics that match the drop_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.drop_metrics.value, "")
            action = "drop"
          }
    
          // keep only metrics that match the keep_metrics regex
          rule {
            source_labels = ["__name__"]
            regex = coalesce(argument.keep_metrics.value, ".*")
            action = "keep"
          }
    
          // remove the component_id label from any metric that starts with log_bytes or log_lines, these are custom metrics that are generated
          // as part of the log annotation modules in this repo
          rule {
            action = "replace"
            source_labels = ["__name__"]
            regex = "^log_(bytes|lines).+"
            replacement = ""
            target_label = "component_id"
          }
    
          // set the namespace label to that of the exported_namespace
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_namespace"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "namespace"
          }
    
          // set the pod label to that of the exported_pod
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_pod"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "pod"
          }
    
          // set the container label to that of the exported_container
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_container"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "container"
          }
    
          // set the job label to that of the exported_job
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_job"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "job"
          }
    
          // set the instance label to that of the exported_instance
          rule {
            action = "replace"
            source_labels = ["__name__", "exported_instance"]
            separator = "@"
            regex = "^log_(bytes|lines).+@(.+)"
            replacement = "$2"
            target_label = "instance"
          }
    
          rule {
            action = "labeldrop"
            regex = "exported_(namespace|pod|container|job|instance)"
          }
        }
      }
    
      alloy_integration_discovery "alloy" {
        port_name = "http-metrics"
        label_selectors = ["app.kubernetes.io/name in (alloy-metrics,alloy-singleton,alloy-logs)"]
      }
    
      alloy_integration_scrape "alloy" {
        targets = alloy_integration_discovery.alloy.output
        job_label = "integrations/alloy"
        clustering = true
        keep_metrics = "up|scrape_samples_scraped|alloy_build_info|alloy_component_controller_running_components|alloy_component_dependencies_wait_seconds|alloy_component_dependencies_wait_seconds_bucket|alloy_component_evaluation_seconds|alloy_component_evaluation_seconds_bucket|alloy_component_evaluation_seconds_count|alloy_component_evaluation_seconds_sum|alloy_component_evaluation_slow_seconds|alloy_config_hash|alloy_resources_machine_rx_bytes_total|alloy_resources_machine_tx_bytes_total|alloy_resources_process_cpu_seconds_total|alloy_resources_process_resident_memory_bytes|alloy_tcp_connections|alloy_wal_samples_appended_total|alloy_wal_storage_active_series|cluster_node_gossip_health_score|cluster_node_gossip_proto_version|cluster_node_gossip_received_events_total|cluster_node_info|cluster_node_lamport_time|cluster_node_peers|cluster_node_update_observers|cluster_transport_rx_bytes_total|cluster_transport_rx_packet_queue_length|cluster_transport_rx_packets_failed_total|cluster_transport_rx_packets_total|cluster_transport_stream_rx_bytes_total|cluster_transport_stream_rx_packets_failed_total|cluster_transport_stream_rx_packets_total|cluster_transport_stream_tx_bytes_total|cluster_transport_stream_tx_packets_failed_total|cluster_transport_stream_tx_packets_total|cluster_transport_streams|cluster_transport_tx_bytes_total|cluster_transport_tx_packet_queue_length|cluster_transport_tx_packets_failed_total|cluster_transport_tx_packets_total|otelcol_exporter_send_failed_spans_total|otelcol_exporter_sent_spans_total|go_gc_duration_seconds_count|go_goroutines|go_memstats_heap_inuse_bytes|loki_process_dropped_lines_total|loki_write_batch_retries_total|loki_write_dropped_bytes_total|loki_write_dropped_entries_total|loki_write_encoded_bytes_total|loki_write_mutated_bytes_total|loki_write_mutated_entries_total|loki_write_request_duration_seconds_bucket|loki_write_sent_bytes_total|loki_write_sent_entries_total|process_cpu_seconds_total|process_start_time_seconds|otelcol_processor_batch_batch_send_size_bucket|otelcol_processor_batch_metadata_cardinality|otelcol_processor_batch_timeout_trigger_send_total|prometheus_remote_storage_bytes_total|prometheus_remote_storage_enqueue_retries_total|prometheus_remote_storage_highest_timestamp_in_seconds|prometheus_remote_storage_metadata_bytes_total|prometheus_remote_storage_queue_highest_sent_timestamp_seconds|prometheus_remote_storage_samples_dropped_total|prometheus_remote_storage_samples_failed_total|prometheus_remote_storage_samples_pending|prometheus_remote_storage_samples_retried_total|prometheus_remote_storage_samples_total|prometheus_remote_storage_sent_batch_duration_seconds_bucket|prometheus_remote_storage_sent_batch_duration_seconds_count|prometheus_remote_storage_sent_batch_duration_seconds_sum|prometheus_remote_storage_shard_capacity|prometheus_remote_storage_shards|prometheus_remote_storage_shards_desired|prometheus_remote_storage_shards_max|prometheus_remote_storage_shards_min|prometheus_remote_storage_succeeded_samples_total|prometheus_remote_write_wal_samples_appended_total|prometheus_remote_write_wal_storage_active_series|prometheus_sd_discovered_targets|prometheus_target_interval_length_seconds_count|prometheus_target_interval_length_seconds_sum|prometheus_target_scrapes_exceeded_sample_limit_total|prometheus_target_scrapes_sample_duplicate_timestamp_total|prometheus_target_scrapes_sample_out_of_bounds_total|prometheus_target_scrapes_sample_out_of_order_total|prometheus_target_sync_length_seconds_sum|prometheus_wal_watcher_current_segment|otelcol_receiver_accepted_spans_total|otelcol_receiver_refused_spans_total|rpc_server_duration_milliseconds_bucket|scrape_duration_seconds|traces_exporter_send_failed_spans|traces_exporter_send_failed_spans_total|traces_exporter_sent_spans|traces_exporter_sent_spans_total|traces_loadbalancer_backend_outcome|traces_loadbalancer_num_backends|traces_receiver_accepted_spans|traces_receiver_accepted_spans_total|traces_receiver_refused_spans|traces_receiver_refused_spans_total"
        scrape_interval = "60s"
        scrape_timeout = "10s"
        scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
        scrape_classic_histograms = false
        scrape_native_histograms = false
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }
    }
    alloy_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    
    livedebugging {
      enabled = true
    }
    
    
    // Destination: grafana-cloud-metrics (prometheus)
    otelcol.exporter.prometheus "grafana_cloud_metrics" {
      add_metric_suffixes = true
      resource_to_telemetry_conversion = false
      forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]
    }
    
    prometheus.remote_write "grafana_cloud_metrics" {
      endpoint {
        url = "https://prometheus-prod-13-prod-us-east-0.grafana.net/api/prom/push"
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["tenantId"]),
        }
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["metricsUser"])
          password = remote.kubernetes.secret.grafana_cloud_metrics.data["grafanaCloudAccessPolicyToken"]
        }
        tls_config {
          insecure_skip_verify = false
          ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["ca"])
          cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["cert"])
          key_pem = remote.kubernetes.secret.grafana_cloud_metrics.data["key"]
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "k8s-monitoring-gc-feature-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "k8s-monitoring-gc-feature-test"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_metrics" {
      name      = "grafana-cloud-credentials"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-singleton
  namespace: default
data:
  config.alloy: |
    // Feature: Cluster Events
    declare "cluster_events" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.source.kubernetes_events "cluster_events" {
        job_name   = "integrations/kubernetes/eventhandler"
        log_format = "logfmt"
        forward_to = [loki.process.cluster_events.receiver]
      }
    
      loki.process "cluster_events" {
    
        // add a static source label to the logs so they can be differentiated / restricted if necessary
        stage.static_labels {
          values = {
            "source" = "kubernetes-events",
          }
        }
    
        // extract some of the fields from the log line, these could be used as labels, structured metadata, etc.
        stage.logfmt {
          mapping = {
            "component" = "sourcecomponent", // map the sourcecomponent field to component
            "kind" = "",
            "level" = "type", // most events don't have a level but they do have a "type" i.e. Normal, Warning, Error, etc.
            "name" = "",
            "node" = "sourcehost", // map the sourcehost field to node
            "reason" = "",
          }
        }
        // set these values as labels, they may or may not be used as index labels in Loki as they can be dropped
        // prior to being written to Loki, but this makes them available
        stage.labels {
          values = {
            "component" = "",
            "kind" = "",
            "level" = "",
            "name" = "",
            "node" = "",
            "reason" = "",
          }
        }
    
        // if kind=Node, set the node label by copying the name field
        stage.match {
          selector = "{kind=\"Node\"}"
    
          stage.labels {
            values = {
              "node" = "name",
            }
          }
        }
    
        // set the level extracted key value as a normalized log level
        stage.match {
          selector = "{level=\"Normal\"}"
    
          stage.static_labels {
            values = {
              level = "Info",
            }
          }
        }
        stage.structured_metadata {
          values = {
            "name" = "name",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["job","level","namespace","node","source","reason"]
        }
        stage.labels {
          values = {
            "service_name" = "job",
          }
        }
        forward_to = argument.logs_destinations.value
      }
    }
    cluster_events "feature" {
      logs_destinations = [
        loki.write.grafana_cloud_logs.receiver,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.grafana_cloud_metrics.receiver,
      ]
    }
    
    livedebugging {
      enabled = true
    }
    
    
    // Destination: grafana-cloud-metrics (prometheus)
    otelcol.exporter.prometheus "grafana_cloud_metrics" {
      add_metric_suffixes = true
      resource_to_telemetry_conversion = false
      forward_to = [prometheus.remote_write.grafana_cloud_metrics.receiver]
    }
    
    prometheus.remote_write "grafana_cloud_metrics" {
      endpoint {
        url = "https://prometheus-prod-13-prod-us-east-0.grafana.net/api/prom/push"
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["tenantId"]),
        }
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["metricsUser"])
          password = remote.kubernetes.secret.grafana_cloud_metrics.data["grafanaCloudAccessPolicyToken"]
        }
        tls_config {
          insecure_skip_verify = false
          ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["ca"])
          cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_metrics.data["cert"])
          key_pem = remote.kubernetes.secret.grafana_cloud_metrics.data["key"]
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "k8s-monitoring-gc-feature-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "k8s-monitoring-gc-feature-test"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_metrics" {
      name      = "grafana-cloud-credentials"
      namespace = "default"
    }
    
    // Destination: grafana-cloud-logs (loki)
    otelcol.exporter.loki "grafana_cloud_logs" {
      forward_to = [loki.write.grafana_cloud_logs.receiver]
    }
    
    loki.write "grafana_cloud_logs" {
      endpoint {
        url = "https://logs-prod-006.grafana.net/loki/api/v1/push"
        retry_on_http_429 = true
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["tenantId"])
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["logsUser"])
          password = remote.kubernetes.secret.grafana_cloud_logs.data["grafanaCloudAccessPolicyToken"]
        }
        tls_config {
          insecure_skip_verify = false
          ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["ca"])
          cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["cert"])
          key_pem = remote.kubernetes.secret.grafana_cloud_logs.data["key"]
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "k8s-monitoring-gc-feature-test",
        "k8s_cluster_name" = "k8s-monitoring-gc-feature-test",
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_logs" {
      name      = "grafana-cloud-credentials"
      namespace = "default"
    }
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.6.1", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="applicationObservability", protocols="otlpgrpc", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="autoInstrumentation", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{deployments="kube-state-metrics,node-exporter,windows-exporter,kepler", feature="clusterMetrics", sources="kubelet,kubeletResource,cadvisor,kube-state-metrics,node-exporter,windows-exporter,kepler", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="clusterEvents", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="integrations", sources="alloy", version="1.0.0"} 1
    # EOF
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
    
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
    
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
    
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
    
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
    
        // explicitly set service_namespace.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.namespace]
        // - pod.namespace
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_namespace",
            "namespace",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_namespace"
        }
    
        // explicitly set service_instance_id.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.instance.id]
        // - concat([k8s.namespace.name, k8s.pod.name, k8s.container.name], '.')
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_instance_id"]
          target_label = "service_instance_id"
        }
        rule {
          source_labels = ["service_instance_id", "namespace", "pod", "container"]
          separator = "."
          regex = "^\\.([^.]+\\.[^.]+\\.[^.]+)$"
          target_label = "service_instance_id"
        }
    
        // set resource attributes
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex = "(.+)"
          target_label = "app_kubernetes_io_name"
        }
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
    
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
    
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
    
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
    
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
    
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
    
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
    
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
    
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        stage.structured_metadata {
          values = {
            "k8s_pod_name" = "k8s_pod_name",
            "pod" = "pod",
            "service_instance_id" = "service_instance_id",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["__tenant_id__","app_kubernetes_io_name","container","instance","job","level","namespace","service_name","service_namespace","deployment_environment","deployment_environment_name","k8s_namespace_name","k8s_deployment_name","k8s_statefulset_name","k8s_daemonset_name","k8s_cronjob_name","k8s_job_name","k8s_node_name"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.grafana_cloud_logs.receiver,
      ]
    }
    
    livedebugging {
      enabled = true
    }
    
    
    // Destination: grafana-cloud-logs (loki)
    otelcol.exporter.loki "grafana_cloud_logs" {
      forward_to = [loki.write.grafana_cloud_logs.receiver]
    }
    
    loki.write "grafana_cloud_logs" {
      endpoint {
        url = "https://logs-prod-006.grafana.net/loki/api/v1/push"
        retry_on_http_429 = true
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["tenantId"])
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["logsUser"])
          password = remote.kubernetes.secret.grafana_cloud_logs.data["grafanaCloudAccessPolicyToken"]
        }
        tls_config {
          insecure_skip_verify = false
          ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["ca"])
          cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_logs.data["cert"])
          key_pem = remote.kubernetes.secret.grafana_cloud_logs.data["key"]
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "k8s-monitoring-gc-feature-test",
        "k8s_cluster_name" = "k8s-monitoring-gc-feature-test",
      }
    }
    
    remote.kubernetes.secret "grafana_cloud_logs" {
      name      = "grafana-cloud-credentials"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-receiver
  namespace: default
data:
  config.alloy: |
    // Feature: Application Observability
    declare "application_observability" {
      argument "metrics_destinations" {
        comment = "Must be a list of metrics destinations where collected metrics should be forwarded to"
      }
    
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      argument "traces_destinations" {
        comment = "Must be a list of trace destinations where collected trace should be forwarded to"
      }
    
      // OTLP Receiver
      otelcol.receiver.otlp "receiver" {
        grpc {
          endpoint = "0.0.0.0:4317"
          include_metadata = false
          max_recv_msg_size = "4MiB"
          read_buffer_size = "512KiB"
          write_buffer_size = "32KiB"
        }
        debug_metrics {
          disable_high_cardinality_metrics = true
        }
        output {
          metrics = [otelcol.processor.resourcedetection.default.input]
          logs = [otelcol.processor.resourcedetection.default.input]
          traces = [otelcol.processor.resourcedetection.default.input]
        }
      }
    
      // Resource Detection Processor
      otelcol.processor.resourcedetection "default" {
        detectors = ["env","system"]
        override = true
    
        system {
          hostname_sources = ["os"]
        }
    
        output {
          metrics = [otelcol.processor.k8sattributes.default.input]
          logs = [otelcol.processor.k8sattributes.default.input]
          traces = [otelcol.processor.k8sattributes.default.input]
        }
      }
    
      // K8s Attributes Processor
      otelcol.processor.k8sattributes "default" {
        passthrough = false
        extract {
          metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.ip"
          }
        }
        pod_association {
          source {
            from = "resource_attribute"
            name = "k8s.pod.uid"
          }
        }
        pod_association {
          source {
            from = "connection"
          }
        }
    
        output {
          metrics = [otelcol.processor.transform.default.input]
          logs = [otelcol.processor.transform.default.input]
          traces = [otelcol.processor.transform.default.input, otelcol.connector.host_info.default.input]
        }
      }
    
      // Host Info Connector
      otelcol.connector.host_info "default" {
        host_identifiers = [ "k8s.node.name" ]
    
        output {
          metrics = [otelcol.processor.batch.default.input]
        }
      }
    
      // Transform Processor
      otelcol.processor.transform "default" {
        error_mode = "ignore"
        log_statements {
          context = "resource"
          statements = [
            "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
            "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
            "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          ]
        }
    
        output {
          metrics = [otelcol.processor.batch.default.input]
          logs = [otelcol.processor.batch.default.input]
          traces = [otelcol.processor.batch.default.input]
        }
      }
    
      // Batch Processor
      otelcol.processor.batch "default" {
        send_batch_size = 8192
        send_batch_max_size = 0
        timeout = "2s"
    
        output {
          metrics = argument.metrics_destinations.value
          logs = argument.logs_destinations.value
          traces = argument.traces_destinations.value
        }
      }
    }
    application_observability "feature" {
      metrics_destinations = [
        otelcol.processor.attributes.grafana_cloud_otlp_endpoint.input,
      ]
      logs_destinations = [
        otelcol.processor.attributes.grafana_cloud_otlp_endpoint.input,
      ]
      traces_destinations = [
        otelcol.processor.attributes.grafana_cloud_otlp_endpoint.input,
      ]
    }
    
    livedebugging {
      enabled = true
    }
    
    
    // Destination: grafana-cloud-otlp-endpoint (otlp)
    otelcol.receiver.prometheus "grafana_cloud_otlp_endpoint" {
      output {
        metrics = [otelcol.processor.attributes.grafana_cloud_otlp_endpoint.input]
      }
    }
    otelcol.receiver.loki "grafana_cloud_otlp_endpoint" {
      output {
        logs = [otelcol.processor.attributes.grafana_cloud_otlp_endpoint.input]
      }
    }
    
    otelcol.processor.attributes "grafana_cloud_otlp_endpoint" {
      output {
        metrics = [otelcol.processor.transform.grafana_cloud_otlp_endpoint.input]
        logs = [otelcol.processor.transform.grafana_cloud_otlp_endpoint.input]
        traces = [otelcol.processor.transform.grafana_cloud_otlp_endpoint.input]
      }
    }
    
    otelcol.processor.transform "grafana_cloud_otlp_endpoint" {
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "k8s-monitoring-gc-feature-test")`,
          `set(attributes["k8s.cluster.name"], "k8s-monitoring-gc-feature-test")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      metric_statements {
        context = "datapoint"
        statements = [
          `set(attributes["cluster"], "k8s-monitoring-gc-feature-test")`,
          `set(attributes["k8s.cluster.name"], "k8s-monitoring-gc-feature-test")`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "k8s-monitoring-gc-feature-test")`,
          `set(attributes["k8s.cluster.name"], "k8s-monitoring-gc-feature-test")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      log_statements {
        context = "log"
        statements = [
          `delete_key(attributes, "loki.attribute.labels")`,
          `delete_key(attributes, "loki.resource.labels")`,
          `set(resource.attributes["k8s.container.name"], attributes["container"] ) where resource.attributes["k8s.container.name"] == nil and attributes["container"] != nil`,
          `delete_key(attributes, "container") where attributes["container"] == resource.attributes["k8s.container.name"]`,
          `set(resource.attributes["k8s.cronjob.name"], attributes["cronjob"] ) where resource.attributes["k8s.cronjob.name"] == nil and attributes["cronjob"] != nil`,
          `delete_key(attributes, "cronjob") where attributes["cronjob"] == resource.attributes["k8s.cronjob.name"]`,
          `set(resource.attributes["k8s.daemonset.name"], attributes["daemonset"] ) where resource.attributes["k8s.daemonset.name"] == nil and attributes["daemonset"] != nil`,
          `delete_key(attributes, "daemonset") where attributes["daemonset"] == resource.attributes["k8s.daemonset.name"]`,
          `set(resource.attributes["k8s.deployment.name"], attributes["deployment"] ) where resource.attributes["k8s.deployment.name"] == nil and attributes["deployment"] != nil`,
          `delete_key(attributes, "deployment") where attributes["deployment"] == resource.attributes["k8s.deployment.name"]`,
          `set(resource.attributes["deployment.environment"], attributes["deployment_environment"] ) where resource.attributes["deployment.environment"] == nil and attributes["deployment_environment"] != nil`,
          `delete_key(attributes, "deployment_environment") where attributes["deployment_environment"] == resource.attributes["deployment.environment"]`,
          `set(resource.attributes["deployment.environment.name"], attributes["deployment_environment_name"] ) where resource.attributes["deployment.environment.name"] == nil and attributes["deployment_environment_name"] != nil`,
          `delete_key(attributes, "deployment_environment_name") where attributes["deployment_environment_name"] == resource.attributes["deployment.environment.name"]`,
          `set(resource.attributes["k8s.job.name"], attributes["job_name"] ) where resource.attributes["k8s.job.name"] == nil and attributes["job_name"] != nil`,
          `delete_key(attributes, "job_name") where attributes["job_name"] == resource.attributes["k8s.job.name"]`,
          `set(resource.attributes["k8s.namespace.name"], attributes["namespace"] ) where resource.attributes["k8s.namespace.name"] == nil and attributes["namespace"] != nil`,
          `delete_key(attributes, "namespace") where attributes["namespace"] == resource.attributes["k8s.namespace.name"]`,
          `set(resource.attributes["k8s.pod.name"], attributes["pod"] ) where resource.attributes["k8s.pod.name"] == nil and attributes["pod"] != nil`,
          `delete_key(attributes, "pod") where attributes["pod"] == resource.attributes["k8s.pod.name"]`,
          `set(resource.attributes["k8s.replicaset.name"], attributes["replicaset"] ) where resource.attributes["k8s.replicaset.name"] == nil and attributes["replicaset"] != nil`,
          `delete_key(attributes, "replicaset") where attributes["replicaset"] == resource.attributes["k8s.replicaset.name"]`,
          `set(resource.attributes["service.name"], attributes["service_name"] ) where resource.attributes["service.name"] == nil and attributes["service_name"] != nil`,
          `delete_key(attributes, "service_name") where attributes["service_name"] == resource.attributes["service.name"]`,
          `set(resource.attributes["service.namespace"], attributes["service_namespace"] ) where resource.attributes["service.namespace"] == nil and attributes["service_namespace"] != nil`,
          `delete_key(attributes, "service_namespace") where attributes["service_namespace"] == resource.attributes["service.namespace"]`,
          `set(resource.attributes["k8s.statefulset.name"], attributes["statefulset"] ) where resource.attributes["k8s.statefulset.name"] == nil and attributes["statefulset"] != nil`,
          `delete_key(attributes, "statefulset") where attributes["statefulset"] == resource.attributes["k8s.statefulset.name"]`,
        ]
      }
    
      trace_statements {
        context = "resource"
        statements = [
          `set(attributes["cluster"], "k8s-monitoring-gc-feature-test")`,
          `set(attributes["k8s.cluster.name"], "k8s-monitoring-gc-feature-test")`,
          `delete_key(attributes, "process.pid")`,
          `delete_key(attributes, "process.parent_pid")`,
          `delete_key(attributes, "process.executable.path")`,
          `delete_key(attributes, "process.command_line")`,
          `delete_key(attributes, "process.command_args")`,
          `delete_key(attributes, "process.owner")`,
          `delete_key(attributes, "process.runtime.version")`,
          `delete_key(attributes, "process.runtime.description")`,
          `delete_key(attributes, "host.ip")`,
          `delete_key(attributes, "host.mac")`,
          `delete_key(attributes, "k8s.pod.start_time")`,
          `delete_key(attributes, "k8s.pod.uid")`,
          `delete_key(attributes, "container.image.id")`,
          `delete_key(attributes, "container.image.repo_digests")`,
          `delete_key(attributes, "os.description")`,
          `delete_key(attributes, "os.build_id")`,
        ]
      }
    
      output {
        metrics = [otelcol.processor.batch.grafana_cloud_otlp_endpoint.input]
        logs = [otelcol.processor.batch.grafana_cloud_otlp_endpoint.input]
        traces = [otelcol.exporter.loadbalancing.grafana_cloud_otlp_endpoint_sampler.input]
      }
    }
    
    otelcol.exporter.loadbalancing "grafana_cloud_otlp_endpoint_sampler" {
      resolver {
        kubernetes {
          service = "k8smon-grafana-cloud-otlp-endpoint-sampler"
        }
      }
      protocol {
        otlp {
          client {
            tls {
              insecure = true
            }
          }
        }
      }
    }
    
    otelcol.processor.batch "grafana_cloud_otlp_endpoint" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
        logs = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
        traces = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
      }
    }
    otelcol.exporter.otlphttp "grafana_cloud_otlp_endpoint" {
      client {
        endpoint = "https://otlp-gateway-prod-us-east-0.grafana.net./otlp"
        auth = otelcol.auth.basic.grafana_cloud_otlp_endpoint.handler
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["tenantId"]),
        }
        tls {
          insecure = false
          insecure_skip_verify = false
          ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["ca"])
          cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["cert"])
          key_pem = remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["key"]
        }
      }
    
      retry_on_failure {
        enabled = true
        initial_interval = "5s"
        max_interval = "30s"
        max_elapsed_time = "5m"
      }
    
      sending_queue {
        enabled = true
      }
    }
    
    otelcol.auth.basic "grafana_cloud_otlp_endpoint" {
      username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["otlpUser"])
      password = remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["grafanaCloudAccessPolicyToken"]
    }
    
    remote.kubernetes.secret "grafana_cloud_otlp_endpoint" {
      name      = "grafana-cloud-credentials"
      namespace = "default"
    }
---
# Source: k8s-monitoring/templates/beyla-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.9.9
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: config
data:
  beyla-config.yml: |-
    attributes:
      kubernetes:
        cluster_name: k8s-monitoring-gc-feature-test
        enable: true
    discovery:
      exclude_services:
      - exe_path: .*alloy.*|.*otelcol.*|.*beyla.*
      services:
      - k8s_namespace: .
    filter:
      network:
        k8s_dst_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
        k8s_src_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
    internal_metrics:
      prometheus:
        path: /internal/metrics
        port: 9090
    otel_traces_export:
      endpoint: http://k8smon-alloy-receiver.default.svc.cluster.local:4317
    prometheus_export:
      features:
      - application
      - network
      - application_service_graph
      - application_span
      - application_host
      path: /metrics
      port: 9090
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-operator
rules:
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "serviceaccounts", "services"]
    verbs: ["*"]
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "statefulsets"]
    verbs: ["*"]
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["*"]
  # Rules which allow the management of Ingresses and NetworkPolicies.
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["*"]
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["*"]
  # Rules which allow the management of ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings.
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    verbs: ["*"]
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-beyla
  labels:
    helm.sh/chart: beyla-1.9.9
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
rules:
  - apiGroups: [ "apps" ]
    resources: [ "replicasets" ]
    verbs: [ "list", "watch" ]
  - apiGroups: [ "" ]
    resources: [ "pods", "services", "nodes" ]
    verbs: [ "list", "watch", "get" ]
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kepler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-kepler-clusterrole
rules:
  - apiGroups: [""]
    resources:
      - nodes/metrics # access /metrics/resource
      - nodes/proxy
      - nodes/stats
      - pods
    verbs:
      - get
      - watch
      - list
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/clusterrole.yaml
# Cluster role giving opencost to get, list, watch required resources
# No write permissions are required
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - deployments
      - nodes
      - nodes/proxy
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - deployments
      - daemonsets
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator-alloy-manager
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-operator
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-beyla
  labels:
    helm.sh/chart: beyla-1.9.9
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: k8smon-beyla
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-beyla
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kepler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-kepler-clusterrole-binding
roleRef:
  kind: ClusterRole
  name: k8smon-kepler-clusterrole
  apiGroup: "rbac.authorization.k8s.io"
subjects:
  - kind: ServiceAccount
    name: k8smon-kepler
    namespace: default
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-opencost
subjects:
  - kind: ServiceAccount
    name: k8smon-opencost
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-alloy-operator-leader-election-role
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-alloy-operator-leader-election-rolebinding
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8smon-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-operator
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kepler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-kepler
  namespace: default
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9102
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: http
    protocol: TCP
    port: 8080
    targetPort: http
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
  annotations:
    prometheus.io/scrape: "false"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
  type: "ClusterIP"
  ports:
    - name: http
      port: 9003
      targetPort: 9003
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/windows-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-windows-exporter
  namespace: default
  labels:
    helm.sh/chart: windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: windows-exporter
    app.kubernetes.io/name: windows-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "0.31.3"
    release: k8smon
spec:
  type: ClusterIP
  ports:
    - port: 9182
      targetPort: metrics
      protocol: TCP
      appProtocol: http
      name: metrics
  selector:
    app.kubernetes.io/name: windows-exporter
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/autoInstrumentation/charts/beyla/templates/daemon-set.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-beyla
  namespace: default
  labels:
    helm.sh/chart: beyla-1.9.9
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: workload
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: beyla
      app.kubernetes.io/instance: k8smon
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        k8s.grafana.com/job: default/beyla
        k8s.grafana.com/logs.job: integrations/beyla
      labels:
        helm.sh/chart: beyla-1.9.9
        app.kubernetes.io/name: beyla
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.7.5"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: beyla
        app.kubernetes.io/component: workload
    spec:
      serviceAccountName: k8smon-beyla
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: beyla
          image: docker.io/grafana/beyla:2.7.5
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          ports:
          - name: metrics
            containerPort: 9090
            protocol: TCP
          env:
            - name: BEYLA_CONFIG_PATH
              value: "/etc/beyla/config/beyla-config.yml"
          volumeMounts:
            - mountPath: /etc/beyla/config
              name: beyla-config
            - mountPath: /sys/fs/cgroup
              name: cgroup
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: beyla-config
          configMap:
            name: k8smon-beyla
        - name: cgroup
          hostPath:
            path: /sys/fs/cgroup
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kepler/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-kepler
  namespace: default
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kepler
      app.kubernetes.io/component: exporter
  template:
    metadata:
      annotations:
        k8s.grafana.com/logs.job: integrations/kepler
      labels:
        app.kubernetes.io/name: kepler
        app.kubernetes.io/component: exporter
    spec:
      hostNetwork: true
      serviceAccountName: k8smon-kepler
      containers:
      - name: kepler-exporter
        image: "quay.io/sustainable_computing_io/kepler:release-0.8.0"
        imagePullPolicy: Always
        securityContext:
            privileged: true
        args:
          - -v=$(KEPLER_LOG_LEVEL)
        env:
          - name: NODE_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: METRIC_PATH
            value: "/metrics"
          - name: BIND_ADDRESS
            value: "0.0.0.0:9102"
          - name: "CGROUP_METRICS"
            value: "*"
          - name: "CPU_ARCH_OVERRIDE"
            value: ""
          - name: "ENABLE_EBPF_CGROUPID"
            value: "true"
          - name: "ENABLE_GPU"
            value: "true"
          - name: "ENABLE_PROCESS_METRICS"
            value: "false"
          - name: "ENABLE_QAT"
            value: "false"
          - name: "EXPOSE_CGROUP_METRICS"
            value: "false"
          - name: "EXPOSE_ESTIMATED_IDLE_POWER_METRICS"
            value: "true"
          - name: "EXPOSE_HW_COUNTER_METRICS"
            value: "true"
          - name: "EXPOSE_IRQ_COUNTER_METRICS"
            value: "true"
          - name: "KEPLER_LOG_LEVEL"
            value: "1"
        ports:
        - containerPort: 9102
          hostPort: 9102
          name: http
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
        startupProbe:
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 1
        volumeMounts:
          - name: lib-modules
            mountPath: /lib/modules
          - name: tracing
            mountPath: /sys
          - name: proc
            mountPath: /proc
          - name: config-dir
            mountPath: /etc/kepler
      volumes:
        - name: lib-modules
          hostPath:
            path: /lib/modules
            type: DirectoryOrCreate
        - name: tracing
          hostPath:
            path: /sys
            type: Directory
        - name: proc
          hostPath:
            path: /proc
            type: Directory
        - name: config-dir
          emptyDir:
            sizeLimit: 100Ki
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                - fargate
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-node-exporter
  namespace: default
  labels:
    helm.sh/chart: node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: node-exporter
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/instance: k8smon
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        k8s.grafana.com/logs.job: integrations/node_exporter
      labels:
        helm.sh/chart: node-exporter-4.49.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: node-exporter
        app.kubernetes.io/name: node-exporter
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.10.2"
        release: k8smon
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: k8smon-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
            - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|ramfs|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tmpfs|tracefs)$
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                - fargate
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/windows-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-windows-exporter
  namespace: default
  labels:
    helm.sh/chart: windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: windows-exporter
    app.kubernetes.io/name: windows-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "0.31.3"
    release: k8smon
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: windows-exporter
      app.kubernetes.io/instance: k8smon
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        k8s.grafana.com/logs.job: integrations/windows_exporter
      labels:
        helm.sh/chart: windows-exporter-0.12.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: windows-exporter
        app.kubernetes.io/name: windows-exporter
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "0.31.3"
        release: k8smon
    spec:
      automountServiceAccountToken: false
      securityContext:
        windowsOptions:
          hostProcess: true
          runAsUserName: NT AUTHORITY\system
      initContainers:
        - name: configure-firewall
          image: ghcr.io/prometheus-community/windows-exporter:0.31.3
          command: [ "powershell" ]
          args: [ "New-NetFirewallRule", "-DisplayName", "'windows-exporter'", "-Direction", "inbound", "-Profile", "Any", "-Action", "Allow", "-LocalPort", "9182", "-Protocol", "TCP" ]
      serviceAccountName: k8smon-windows-exporter
      containers:
        - name: windows-exporter
          image: ghcr.io/prometheus-community/windows-exporter:0.31.3
          imagePullPolicy: IfNotPresent
          args:
            - --config.file=%CONTAINER_SANDBOX_MOUNT_POINT%/config.yml
            - --collector.textfile.directories=%CONTAINER_SANDBOX_MOUNT_POINT%
            - --web.listen-address=:9182
          env:
          ports:
            - name: metrics
              containerPort: 9182
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /health
              port: 9182
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /health
              port: 9182
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /config.yml
              subPath: config.yml
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: windows
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-windows-exporter
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-operator
  namespace: default
  labels:
    helm.sh/chart: alloy-operator-0.3.12
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.12
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.4.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8smon-alloy-operator
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
            - --leader-elect
            - --leader-election-id=k8smon-alloy-operator

          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: k8smon
  replicas: 1
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-6.4.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.17.0"
        release: k8smon
      annotations:
      
        k8s.grafana.com/logs.job: integrations/kubernetes/kube-state-metrics
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: k8smon-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --metric-labels-allowlist=nodes=[agentpool,alpha.eksctl.io/cluster-name,alpha.eksctl.io/nodegroup-name,beta.kubernetes.io/instance-type,cloud.google.com/gke-nodepool,cluster-name,ec2.amazonaws.com/Name,ec2.amazonaws.com/aws-autoscaling-groupName,ec2.amazonaws.com/aws-autoscaling-group-name,ec2.amazonaws.com/name,eks.amazonaws.com/nodegroup,k8s.io/cloud-provider-aws,karpenter.sh/nodepool,kubernetes.azure.com/cluster,kubernetes.io/arch,kubernetes.io/hostname,kubernetes.io/os,node.kubernetes.io/instance-type,topology.kubernetes.io/region,topology.kubernetes.io/zone]
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
        ports:
        - containerPort: 8080
          name: http
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/charts/clusterMetrics/charts/opencost/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
  annotations:
    checksum/configs: b6f6a394f20549ac439b978912c55a5ecc0fc93b2c71568d9c2b7fb6085dadae
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
      app.kubernetes.io/instance: k8smon
  strategy: 
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opencost
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-opencost
      automountServiceAccountToken: true
      tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
      nodeSelector:
        kubernetes.io/os: linux
      initContainers:
      containers:
        - name: k8smon-opencost
          image: ghcr.io/opencost/opencost:1.117.6@sha256:6f1a0e6fe21559a77051e7b7f9e4ac6bc80277131492ae084e8365ada805af91
          imagePullPolicy: IfNotPresent
          args:
          ports:
            - containerPort: 9003
              name: http
          resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 10m
              memory: 55Mi
          startupProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 20
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 3
          env:
            - name: LOG_LEVEL
              value: info
            - name: CUSTOM_COST_ENABLED
              value: "false"
            - name: INSTALL_NAMESPACE
              value: default
            - name: PROMETHEUS_QUERY_RESOLUTION_SECONDS
              value: "300"
            - name: API_PORT
              value: "9003"
            - name: PROMETHEUS_SERVER_ENDPOINT
              value: "https://prometheus-prod-13-prod-us-east-0.grafana.net/api/prom"
            - name: INSECURE_SKIP_VERIFY
              value: "false"
            - name: CLOUD_PROVIDER_API_KEY
              valueFrom:
                secretKeyRef:
                  name: k8smon-opencost
                  key: CLOUD_PROVIDER_API_KEY
            - name: CLUSTER_ID
              value: "k8s-monitoring-gc-feature-test"
            - name: DB_BASIC_AUTH_USERNAME
              valueFrom:
                secretKeyRef:
                  name: grafana-cloud-credentials
                  key: metricsUser
            - name: DB_BASIC_AUTH_PW
              valueFrom:
                secretKeyRef:
                  name: grafana-cloud-credentials
                  key: grafanaCloudAccessPolicyToken
            - name: RESOLUTION_1D_RETENTION
              value: "15"
            - name: RESOLUTION_1H_RETENTION
              value: "49"
            - name: CLOUD_COST_ENABLED
              value: "false"
            - name: CLOUD_COST_MONTH_TO_DATE_INTERVAL
              value: "6"
            - name: CLOUD_COST_REFRESH_RATE_HOURS
              value: "6"
            - name: CLOUD_COST_QUERY_WINDOW_DAYS
              value: "7"
            - name: CLOUD_COST_RUN_WINDOW_DAYS
              value: "3"
            - name: EMIT_KSM_V1_METRICS
              value: "false"
            - name: EMIT_KSM_V1_METRICS_ONLY
              value: "true"
            # Add any additional provided variables
            - name: CURRENT_CLUSTER_ID_FILTER_ENABLED
              value: "true"
            - name: PROM_CLUSTER_ID_LABEL
              value: "cluster"
          volumeMounts:
            - mountPath: /var/configs
              name: configs
              readOnly: false
      volumes:
        - emptyDir: {}
          name: configs
---
# Source: k8s-monitoring/templates/alloy-sampler.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-grafana-cloud-otlp-endpoint-sampler
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: |-
        otelcol.receiver.otlp "sampler_receiver" {
          grpc {
            max_recv_msg_size = "4MB"
          }
  
          output {
            traces = [otelcol.processor.tail_sampling.tail_sampler.input]
          }
        }
        otelcol.processor.tail_sampling "tail_sampler" {
          // https://grafana.com/docs/alloy/latest/reference/components/otelcol.processor.tail_sampling/
  
          decision_wait = "15s"
          decision_cache = {
            sampled_cache_size     = 0,
            non_sampled_cache_size = 0,
          }
  
          policy {
            name = "sample-15pct-traces"
            type = "probabilistic"
            probabilistic {
              sampling_percentage = 15
            }
          }
  
          output {
            traces = [otelcol.processor.batch.grafana_cloud_otlp_endpoint.input]
          }
        }
  
        livedebugging {
          enabled = true
        }
  
  
        // Destination: grafana-cloud-otlp-endpoint (otlp)
        otelcol.processor.batch "grafana_cloud_otlp_endpoint" {
          timeout = "2s"
          send_batch_size = 8192
          send_batch_max_size = 0
  
          output {
            metrics = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
            logs = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
            traces = [otelcol.exporter.otlphttp.grafana_cloud_otlp_endpoint.input]
          }
        }
        otelcol.exporter.otlphttp "grafana_cloud_otlp_endpoint" {
          client {
            endpoint = "https://otlp-gateway-prod-us-east-0.grafana.net./otlp"
            auth = otelcol.auth.basic.grafana_cloud_otlp_endpoint.handler
            headers = {
              "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["tenantId"]),
            }
            tls {
              insecure = false
              insecure_skip_verify = false
              ca_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["ca"])
              cert_pem = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["cert"])
              key_pem = remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["key"]
            }
          }
  
          retry_on_failure {
            enabled = true
            initial_interval = "5s"
            max_interval = "30s"
            max_elapsed_time = "5m"
          }
  
          sending_queue {
            enabled = true
          }
        }
  
        otelcol.auth.basic "grafana_cloud_otlp_endpoint" {
          username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["otlpUser"])
          password = remote.kubernetes.secret.grafana_cloud_otlp_endpoint.data["grafanaCloudAccessPolicyToken"]
        }
  
        remote.kubernetes.secret "grafana_cloud_otlp_endpoint" {
          name      = "grafana-cloud-credentials"
          namespace = "default"
        }
      create: true
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
      source: k8s-monitoring-gc-feature-test
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 2
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: statefulset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  fullnameOverride: k8smon-grafana-cloud-otlp-endpoint-sampler
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: null
  namespaceOverride: null
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-metrics
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: true
      name: alloy-metrics
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
      source: k8s-monitoring-gc-feature-test
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: statefulset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-metrics
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-singleton
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
      source: k8s-monitoring-gc-feature-test
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: deployment
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-singleton
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-logs
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: true
      extra: []
      varlog: true
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
      source: k8s-monitoring-gc-feature-test
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations:
    - effect: NoSchedule
      operator: Exists
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-logs
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8smon-alloy-receiver
  namespace: default
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - CHOWN
        - DAC_OVERRIDE
        - FOWNER
        - FSETID
        - KILL
        - SETGID
        - SETUID
        - SETPCAP
        - NET_BIND_SERVICE
        - NET_RAW
        - SYS_CHROOT
        - MKNOD
        - AUDIT_WRITE
        - SETFCAP
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: false
    hostPID: false
    initContainers: []
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
      source: k8s-monitoring-gc-feature-test
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-receiver
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["collectors.grafana.com"]
    resources: ["alloys"]
    verbs: ["get", "list", "watch", "delete"]
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-add-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-add-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    namespace: default
roleRef:
  kind: Role
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-add-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-add-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.6.1
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "15"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-add-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-add-finalizer
      containers:
        - name: add-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              kubectl patch \
                --namespace=default \
                --patch='{"metadata":{"finalizers":["k8s.grafana.com/finalizer"]}}' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
  namespace: default
  labels:
    app.kubernetes.io/name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
    app.kubernetes.io/instance: k8smon
    helm.sh/chart: k8s-monitoring-3.6.1
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      labels:
        app.kubernetes.io/name: k8smon-k8s-monitoring
        app.kubernetes.io/instance: k8smon
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8smon-k8s-monitoring-remove-alloy-and-finalizer
      containers:
        - name: remove-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              echo "Deleting Alloy instance: alloy/k8smon-alloy-metrics..."
              kubectl delete alloy/k8smon-alloy-metrics --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-metrics --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-metrics or it may not exist."

              echo "Deleting Alloy instance: alloy/k8smon-alloy-singleton..."
              kubectl delete alloy/k8smon-alloy-singleton --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-singleton --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-singleton or it may not exist."

              echo "Deleting Alloy instance: alloy/k8smon-alloy-logs..."
              kubectl delete alloy/k8smon-alloy-logs --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-logs --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-logs or it may not exist."

              echo "Deleting Alloy instance: alloy/k8smon-alloy-receiver..."
              kubectl delete alloy/k8smon-alloy-receiver --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-alloy-receiver --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-alloy-receiver or it may not exist."

              echo "Deleting Alloy instance: alloy/k8smon-grafana-cloud-otlp-endpoint-sampler..."
              kubectl delete alloy/k8smon-grafana-cloud-otlp-endpoint-sampler --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8smon-grafana-cloud-otlp-endpoint-sampler --timeout=60s || echo "Timed out waiting for deletion of alloy/k8smon-grafana-cloud-otlp-endpoint-sampler or it may not exist."

              kubectl patch \
                --namespace=default \
                --type json \
                --patch='[{"op": "remove", "path": "/metadata/finalizers"}]' \
                deployment/k8smon-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
