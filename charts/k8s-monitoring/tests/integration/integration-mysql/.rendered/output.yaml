---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "localloki-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  tenantId: "MQ=="
  username: "bG9raQ=="
  password: "bG9raXBhc3N3b3Jk"
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Destination: localPrometheus (prometheus)
    otelcol.exporter.prometheus "localprometheus" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.localprometheus.receiver]
    }
    
    prometheus.remote_write "localprometheus" {
      endpoint {
        url = "http://prometheus-server.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "mysql-integration-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "mysql-integration-test"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
    
    declare "mysql_integration" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      
      
      remote.kubernetes.secret "test_database" {
        name      = "test-database-mysql"
        namespace = "mysql"
      }
      
      prometheus.exporter.mysql "test_database" {
        data_source_name = string.format("%s:%s@(%s:%d)/",
          "root",
          remote.kubernetes.secret.test_database.data["mysql-root-password"],
          "test-database-mysql.mysql.svc",
          3306,
        )
        enable_collectors = ["heartbeat","mysql.user"]
      }
      prometheus.scrape "test_database" {
        targets    = prometheus.exporter.mysql.test_database.targets
        job_name   = "integration/mysql"
        forward_to = [prometheus.relabel.test_database.receiver]
      }
      
      prometheus.relabel "test_database" {
        max_cache_size = 100000
        rule {
          target_label = "instance"
          replacement = "test-database"
        }
        forward_to = argument.metrics_destinations.value
      }
    }
    mysql_integration "integration" {
      metrics_destinations = [
        prometheus.remote_write.localprometheus.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.localprometheus.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.5", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="integrations", sources="mysql", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Destination: localLoki (loki)
    otelcol.exporter.loki "localloki" {
      forward_to = [loki.write.localloki.receiver]
    }
    
    loki.write "localloki" {
      endpoint {
        url = "http://loki.loki.svc:3100/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.localloki.data["tenantId"])
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.localloki.data["username"])
          password = remote.kubernetes.secret.localloki.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "mysql-integration-test",
        "k8s_cluster_name" = "mysql-integration-test",
      }
    }
    
    remote.kubernetes.secret "localloki" {
      name      = "localloki-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
      
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
      
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
      
        // set the job label from the k8s.grafana.com/logs.job annotation if it exists
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
      
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
      
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          separator = ";"
          regex = "(?:test-database)"
          target_label = "integration"
          replacement = "mysql"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
          separator = ";"
          regex = "(?:test-database)"
          target_label = "instance"
          replacement = "test-database"
        }
      }
      
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
      
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
      
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
      
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
      
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
      
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
      
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
      
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
      
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
      
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        // Integration: MySQL
        stage.match {
          selector = "{integration=\"mysql\"}"
        
          stage.regex {
            expression = `(?P<timestamp>.+) (?P<thread>[\d]+) \[(?P<label>.+?)\]( \[(?P<err_code>.+?)\] \[(?P<subsystem>.+?)\])? (?P<msg>.+)`
          }
        
          stage.labels {
            values = {
              level = "label",
              err_code = "err_code",
              subsystem = "subsystem",
            }
          }
        
          stage.drop {
            expression = "^ *$"
            drop_counter_reason = "drop empty lines"
          }
        
          stage.static_labels {
            values = {
              job = "integrations/mysql",
            }
          }
        
          stage.label_drop {
            values = ["integration"]
          }
        }
      
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["app_kubernetes_io_name","container","instance","job","level","namespace","pod","service_name","integration"]
        }
      
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.localloki.receiver,
      ]
    }
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-metrics
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      clustering:
        enabled: true
        name: alloy-metrics
      configMap:
        create: false
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-metrics
    controller:
      replicas: 1
      type: statefulset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8smon-alloy-logs
  namespace: default
spec:
  interval: 1m
  chart:
    spec:
      chart: alloy
      sourceRef:
        kind: HelmRepository
        name: k8smon
        namespace: default
      interval: 1m
  values: 
    alloy:
      configMap:
        create: false
      mounts:
        dockercontainers: true
        varlog: true
      nodeSelector:
        kubernetes.io/os: linux
      podAnnotations:
        k8s.grafana.com/logs.job: integrations/alloy
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - CHOWN
          - DAC_OVERRIDE
          - FOWNER
          - FSETID
          - KILL
          - SETGID
          - SETUID
          - SETPCAP
          - NET_BIND_SERVICE
          - NET_RAW
          - SYS_CHROOT
          - MKNOD
          - AUDIT_WRITE
          - SETFCAP
          drop:
          - ALL
        seccompProfile:
          type: RuntimeDefault
    collectorName: alloy-logs
    controller:
      type: daemonset
    crds:
      create: false
    enabled: true
    extraConfig: ""
    liveDebugging:
      enabled: false
    logging:
      format: logfmt
      level: info
    remoteConfig:
      auth:
        password: ""
        passwordFrom: ""
        passwordKey: password
        type: none
        username: ""
        usernameFrom: ""
        usernameKey: username
      enabled: false
      extraAttributes: {}
      pollFrequency: 5m
      secret:
        create: true
        embed: false
        name: ""
        namespace: ""
      url: ""
---
# Source: k8s-monitoring/templates/temp_helm_repo.yaml
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: k8smon
  namespace: default
spec:
  interval: 1m
  url: https://grafana.github.io/helm-charts
