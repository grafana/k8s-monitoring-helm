---
# Source: k8s-monitoring/charts/alloy-logs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-0.10.1
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-metrics
  namespace: default
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "prometheus-basicauth-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  username: "YmFzaWN1c2Vy"
  password: "YmFzaWNwYXNzd29yZA=="
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "prometheus-bearer-token-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  bearerToken: "bXktYmVhcmVyLXRva2Vu"
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "prometheus-otlp-basicauth-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  username: "YmFzaWN1c2Vy"
  password: "YmFzaWNwYXNzd29yZA=="
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "prometheus-otlp-bearer-token-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  bearerToken: "bXktYmVhcmVyLXRva2Vu"
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "loki-noauth-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  tenantId: "MQ=="
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "loki-basicauth-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  tenantId: "MQ=="
  username: "YmFzaWN1c2Vy"
  password: "YmFzaWNwYXNzd29yZA=="
---
# Source: k8s-monitoring/templates/destination_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "loki-bearer-token-k8smon-k8s-monitoring"
  namespace: "default"
type: Opaque
data:
  tenantId: "MQ=="
  bearerToken: "bXktYmVhcmVyLXRva2Vu"
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-metrics
  namespace: default
data:
  config.alloy: |-
    // Destination: prometheus-noauth (prometheus)
    otelcol.exporter.prometheus "prometheus_noauth" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus_noauth.receiver]
    }
    
    prometheus.remote_write "prometheus_noauth" {
      endpoint {
        url = "http://prometheus-server.prometheus.svc:9090/api/v1/write"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
      external_labels = {
        destination = "prometheus-noauth",
      }
    }
    // Destination: prometheus-basicauth (prometheus)
    otelcol.exporter.prometheus "prometheus_basicauth" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus_basicauth.receiver]
    }
    
    prometheus.remote_write "prometheus_basicauth" {
      endpoint {
        url = "http://nginx-auth-gateway.default.svc/metrics/basic/api/v1/write"
        headers = {
        }
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.prometheus_basicauth.data["username"])
          password = remote.kubernetes.secret.prometheus_basicauth.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
      external_labels = {
        destination = "prometheus-basicauth",
      }
    }
    
    remote.kubernetes.secret "prometheus_basicauth" {
      name      = "prometheus-basicauth-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: prometheus-bearer-token (prometheus)
    otelcol.exporter.prometheus "prometheus_bearer_token" {
      add_metric_suffixes = true
      forward_to = [prometheus.remote_write.prometheus_bearer_token.receiver]
    }
    
    prometheus.remote_write "prometheus_bearer_token" {
      endpoint {
        url = "http://nginx-auth-gateway.default.svc/metrics/bearer/api/v1/write"
        headers = {
        }
        bearer_token = remote.kubernetes.secret.prometheus_bearer_token.data["bearerToken"]
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s.cluster.name"]
          regex = ""
          replacement = "auth-integration-test"
          target_label = "cluster"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
      external_labels = {
        destination = "prometheus-bearer-token",
      }
    }
    
    remote.kubernetes.secret "prometheus_bearer_token" {
      name      = "prometheus-bearer-token-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: prometheus-otlp-noauth (otlp)
    otelcol.receiver.prometheus "prometheus_otlp_noauth" {
      output {
        metrics = [otelcol.processor.attributes.prometheus_otlp_noauth.input]
      }
    }
    
    otelcol.processor.attributes "prometheus_otlp_noauth" {
      action {
        key = "cluster"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "k8s.cluster.name"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "destination"
        action = "upsert"
        value = "prometheus-otlp-noauth"
      }
      output {
        metrics = [otelcol.processor.transform.prometheus_otlp_noauth.input]
        logs = [otelcol.processor.transform.prometheus_otlp_noauth.input]
        traces = [otelcol.processor.transform.prometheus_otlp_noauth.input]
      }
    }
    
    otelcol.processor.transform "prometheus_otlp_noauth" {
      error_mode = "ignore"
    
      output {
        metrics = [otelcol.processor.batch.prometheus_otlp_noauth.input]
      }
    }
    
    otelcol.processor.batch "prometheus_otlp_noauth" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlphttp.prometheus_otlp_noauth.input]
      }
    }
    otelcol.exporter.otlphttp "prometheus_otlp_noauth" {
      client {
        endpoint = "http://prometheus-server.prometheus.svc:9090/api/v1/otlp"
        tls {
          insecure = false
          insecure_skip_verify = false
        }
      }
    }
    // Destination: prometheus-otlp-basicauth (otlp)
    otelcol.receiver.prometheus "prometheus_otlp_basicauth" {
      output {
        metrics = [otelcol.processor.attributes.prometheus_otlp_basicauth.input]
      }
    }
    otelcol.auth.basic "prometheus_otlp_basicauth" {
      username = nonsensitive(remote.kubernetes.secret.prometheus_otlp_basicauth.data["username"])
      password = remote.kubernetes.secret.prometheus_otlp_basicauth.data["password"]
    }
    
    otelcol.processor.attributes "prometheus_otlp_basicauth" {
      action {
        key = "cluster"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "k8s.cluster.name"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "destination"
        action = "upsert"
        value = "prometheus-otlp-basicauth"
      }
      output {
        metrics = [otelcol.processor.transform.prometheus_otlp_basicauth.input]
        logs = [otelcol.processor.transform.prometheus_otlp_basicauth.input]
        traces = [otelcol.processor.transform.prometheus_otlp_basicauth.input]
      }
    }
    
    otelcol.processor.transform "prometheus_otlp_basicauth" {
      error_mode = "ignore"
    
      output {
        metrics = [otelcol.processor.batch.prometheus_otlp_basicauth.input]
      }
    }
    
    otelcol.processor.batch "prometheus_otlp_basicauth" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlphttp.prometheus_otlp_basicauth.input]
      }
    }
    otelcol.exporter.otlphttp "prometheus_otlp_basicauth" {
      client {
        endpoint = "http://nginx-auth-gateway.default.svc/metrics/basic/api/v1/otlp"
        auth = otelcol.auth.basic.prometheus_otlp_basicauth.handler
        tls {
          insecure = false
          insecure_skip_verify = false
        }
      }
    }
    
    remote.kubernetes.secret "prometheus_otlp_basicauth" {
      name      = "prometheus-otlp-basicauth-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: prometheus-otlp-bearer-token (otlp)
    otelcol.receiver.prometheus "prometheus_otlp_bearer_token" {
      output {
        metrics = [otelcol.processor.attributes.prometheus_otlp_bearer_token.input]
      }
    }
    otelcol.auth.bearer "prometheus_otlp_bearer_token" {
      token = remote.kubernetes.secret.prometheus_otlp_bearer_token.data["bearerToken"]
    }
    
    otelcol.processor.attributes "prometheus_otlp_bearer_token" {
      action {
        key = "cluster"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "k8s.cluster.name"
        action = "upsert"
        value = "auth-integration-test"
      }
      action {
        key = "destination"
        action = "upsert"
        value = "prometheus-otlp-bearer-token"
      }
      output {
        metrics = [otelcol.processor.transform.prometheus_otlp_bearer_token.input]
        logs = [otelcol.processor.transform.prometheus_otlp_bearer_token.input]
        traces = [otelcol.processor.transform.prometheus_otlp_bearer_token.input]
      }
    }
    
    otelcol.processor.transform "prometheus_otlp_bearer_token" {
      error_mode = "ignore"
    
      output {
        metrics = [otelcol.processor.batch.prometheus_otlp_bearer_token.input]
      }
    }
    
    otelcol.processor.batch "prometheus_otlp_bearer_token" {
      timeout = "2s"
      send_batch_size = 8192
      send_batch_max_size = 0
    
      output {
        metrics = [otelcol.exporter.otlphttp.prometheus_otlp_bearer_token.input]
      }
    }
    otelcol.exporter.otlphttp "prometheus_otlp_bearer_token" {
      client {
        endpoint = "http://nginx-auth-gateway.default.svc/metrics/bearer/api/v1/otlp"
        auth = otelcol.auth.bearer.prometheus_otlp_bearer_token.handler
        tls {
          insecure = false
          insecure_skip_verify = false
        }
      }
    }
    
    remote.kubernetes.secret "prometheus_otlp_bearer_token" {
      name      = "prometheus-otlp-bearer-token-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Feature: Cluster Metrics
    declare "cluster_metrics" {
      argument "metrics_destinations" {
        comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
      }
      
      remote.kubernetes.configmap "kubernetes" {
        name = "k8smon-alloy-module-kubernetes"
        namespace = "default"
      }
      
      import.string "kubernetes" {
        content = remote.kubernetes.configmap.kubernetes.data["core_metrics.alloy"]
      }  
      
      kubernetes.kubelet "scrape" {
        clustering = true
        keep_metrics = "up|container_cpu_usage_seconds_total|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubernetes_build_info|namespace_workload_pod|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes|kubernetes_build_info"
        scrape_interval = "60s"
        max_cache_size = 100000
        forward_to = argument.metrics_destinations.value
      }                        
    }
    cluster_metrics "feature" {
      metrics_destinations = [
        prometheus.remote_write.prometheus_noauth.receiver,
        prometheus.remote_write.prometheus_basicauth.receiver,
        prometheus.remote_write.prometheus_bearer_token.receiver,
        otelcol.receiver.prometheus.prometheus_otlp_noauth.receiver,
        otelcol.receiver.prometheus.prometheus_otlp_basicauth.receiver,
        otelcol.receiver.prometheus.prometheus_otlp_bearer_token.receiver,
      ]
    }
    
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "1h"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.prometheus_noauth.receiver,
        prometheus.remote_write.prometheus_basicauth.receiver,
        prometheus.remote_write.prometheus_bearer_token.receiver,
      ]
    }
    
    
    
    
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="2.0.0-rc.6", namespace="default"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{deployments="%!s(<nil>)", feature="clusterMetrics", sources="kubelet", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Destination: loki-noauth (loki)
    otelcol.exporter.loki "loki_noauth" {
      forward_to = [loki.write.loki_noauth.receiver]
    }
    
    loki.write "loki_noauth" {
      endpoint {
        url = "http://loki.loki.svc:3100/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.loki_noauth.data["tenantId"])
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "auth-integration-test",
        "k8s_cluster_name" = "auth-integration-test",
        destination = "loki-noauth",
      }
    }
    
    remote.kubernetes.secret "loki_noauth" {
      name      = "loki-noauth-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: loki-basicauth (loki)
    otelcol.exporter.loki "loki_basicauth" {
      forward_to = [loki.write.loki_basicauth.receiver]
    }
    
    loki.write "loki_basicauth" {
      endpoint {
        url = "http://nginx-auth-gateway.default.svc/logs/basic/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.loki_basicauth.data["tenantId"])
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.loki_basicauth.data["username"])
          password = remote.kubernetes.secret.loki_basicauth.data["password"]
        }
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "auth-integration-test",
        "k8s_cluster_name" = "auth-integration-test",
        destination = "loki-basicauth",
      }
    }
    
    remote.kubernetes.secret "loki_basicauth" {
      name      = "loki-basicauth-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Destination: loki-bearer-token (loki)
    otelcol.exporter.loki "loki_bearer_token" {
      forward_to = [loki.write.loki_bearer_token.receiver]
    }
    
    loki.write "loki_bearer_token" {
      endpoint {
        url = "http://nginx-auth-gateway.default.svc/logs/bearer/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.loki_bearer_token.data["tenantId"])
        bearer_token = remote.kubernetes.secret.loki_bearer_token.data["bearerToken"]
        tls_config {
          insecure_skip_verify = false
        }
      }
      external_labels = {
        cluster = "auth-integration-test",
        "k8s_cluster_name" = "auth-integration-test",
        destination = "loki-bearer-token",
      }
    }
    
    remote.kubernetes.secret "loki_bearer_token" {
      name      = "loki-bearer-token-k8smon-k8s-monitoring"
      namespace = "default"
    }
    
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
      
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex         = "(.+)"
          target_label  = "app_kubernetes_io_name"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex         = "(.+)"
          target_label  = "job"
        }
      
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
      }
      
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + env("HOSTNAME")
        }
        namespaces {
          names = ["default"]
        }
      }
      
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
      
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
      
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
      
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
      
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {}
      
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
      
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
      
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
      
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.loki_noauth.receiver,
        loki.write.loki_basicauth.receiver,
        loki.write.loki_bearer_token.receiver,
      ]
    }
---
# Source: k8s-monitoring/templates/alloy-modules-configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-module-kubernetes
data:
  core_metrics.alloy: |
    /*
    Module: job-cadvisor
    Description: Scrapes cadvisor
    
    Note: Every argument except for "forward_to" is optional, and does have a defined default value.  However, the values for these
          arguments are not defined using the default = " ... " argument syntax, but rather using the coalesce(argument.value, " ... ").
          This is because if the argument passed in from another consuming module is set to null, the default = " ... " syntax will
          does not override the value passed in, where coalesce() will return the first non-null value.
    */
    declare "cadvisor" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all cadvisor metric (default: integrations/kubernetes/cadvisor)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.cadvisor.output
      }
    
      // cadvisor service discovery for all of the nodes
      discovery.kubernetes "cadvisor" {
        role = "node"
    
        selectors {
          role = "node"
          field = join(coalesce(argument.field_selectors.value, []), ",")
          label = join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // cadvisor relabelings (pre-scrape)
      discovery.relabel "cadvisor" {
        targets = discovery.kubernetes.cadvisor.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes cadvisor metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/cadvisor"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // cadvisor scrape job
      prometheus.scrape "cadvisor" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/cadvisor")
        forward_to = [prometheus.relabel.cadvisor.receiver]
        targets = discovery.relabel.cadvisor.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // cadvisor metric relabelings (post-scrape)
      prometheus.relabel "cadvisor" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(up|container_(cpu_(cfs_(periods|throttled_periods)_total|usage_seconds_total)|fs_(reads|writes)(_bytes)?_total|memory_(cache|rss|swap|working_set_bytes)|network_(receive|transmit)_(bytes|packets(_dropped)?_total))|machine_memory_bytes)")
          action = "keep"
        }
    
        // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","container"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
          action = "drop"
        }
    
        // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
        rule {
          source_labels = ["__name__","image"]
          separator = "@"
          regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
          action = "drop"
        }
    
        // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
        rule {
          source_labels = ["__name__", "boot_id"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "boot_id"
          replacement = "NA"
        }
        rule {
          source_labels = ["__name__", "system_uuid"]
          separator = "@"
          regex = "machine_memory_bytes@.*"
          target_label = "system_uuid"
          replacement = "NA"
        }
    
        // Filter out non-physical devices/interfaces
        rule {
          source_labels = ["__name__", "device"]
          separator = "@"
          regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_fs_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_fs_.*"
          target_label = "__keepme"
          replacement = ""
        }
        rule {
          source_labels = ["__name__", "interface"]
          separator = "@"
          regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
          target_label = "__keepme"
          replacement = "1"
        }
        rule {
          source_labels = ["__name__", "__keepme"]
          separator = "@"
          regex = "container_network_.*@"
          action = "drop"
        }
        rule {
          source_labels = ["__name__"]
          regex = "container_network_.*"
          target_label = "__keepme"
          replacement = ""
        }
      }
    }
    
    declare "resources" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all resources metric (default: integrations/kubernetes/kube-resources)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.resources.output
      }
    
      // resources service discovery for all of the nodes
      discovery.kubernetes "resources" {
        role = "node"
    
        selectors {
          role = "node"
          field = join(coalesce(argument.field_selectors.value, []), ",")
          label = join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // resources relabelings (pre-scrape)
      discovery.relabel "resources" {
        targets = discovery.kubernetes.resources.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes resources metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/resource"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // resources scrape job
      prometheus.scrape "resources" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-resources")
        forward_to = [prometheus.relabel.resources.receiver]
        targets = discovery.relabel.resources.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // resources metric relabelings (post-scrape)
      prometheus.relabel "resources" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "kubelet" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kubelet metric (default: integrations/kubernetes/kube-kubelet)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.kubelet.output
      }
    
      // kubelet service discovery for all of the nodes
      discovery.kubernetes "kubelet" {
        role = "node"
    
        selectors {
          role = "node"
          field = join(coalesce(argument.field_selectors.value, []), ",")
          label = join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // kubelet relabelings (pre-scrape)
      discovery.relabel "kubelet" {
        targets = discovery.kubernetes.kubelet.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes kubelet metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // kubelet scrape job
      prometheus.scrape "kubelet" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kubelet")
        forward_to = [prometheus.relabel.kubelet.receiver]
        targets = discovery.relabel.kubelet.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // kubelet metric relabelings (post-scrape)
      prometheus.relabel "kubelet" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "apiserver" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: default)"
        optional = true
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "port_name" {
        comment = "The value of the label for the selector (default: https)"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kube-apiserver metrics (default: integrations/kubernetes/kube-apiserver)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      // drop metrics and les from kube-prometheus
      // https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "drop_les" {
        comment = "Regular expression of metric les label values to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.apiserver.output
      }
    
      // kube-apiserver service discovery
      discovery.kubernetes "apiserver" {
        role = "service"
    
        selectors {
          role = "service"
          field = join(coalesce(argument.field_selectors.value, ["metadata.name=kubernetes"]), ",")
          label = join(coalesce(argument.label_selectors.value, []), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, ["default"])
        }
      }
    
      // apiserver relabelings (pre-scrape)
      discovery.relabel "apiserver" {
        targets = discovery.kubernetes.apiserver.targets
    
        // only keep targets with a matching port name
        rule {
          source_labels = ["__meta_kubernetes_service_port_name"]
          regex = coalesce(argument.port_name.value, "https")
          action = "keep"
        }
    
        // set the namespace
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
    
        // set the service_name
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_service_name"]
          target_label = "service"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_service_label_app_kubernetes_io_name",
            "__meta_kubernetes_service_label_k8s_app",
            "__meta_kubernetes_service_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
      // kube-apiserver scrape job
      prometheus.scrape "apiserver" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-apiserver")
        forward_to = [prometheus.relabel.apiserver.receiver]
        targets = discovery.relabel.apiserver.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // apiserver metric relabelings (post-scrape)
      prometheus.relabel "apiserver" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(((go|process)_.+)|kubelet_(pod_(worker|start)_latency_microseconds|cgroup_manager_latency_microseconds|pleg_relist_(latency|interval)_microseconds|runtime_operations(_latency_microseconds|_errors)?|eviction_stats_age_microseconds|device_plugin_(registration_count|alloc_latency_microseconds)|network_plugin_operations_latency_microseconds)|scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_(predicate|priority|preemption)_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)|apiserver_(request_(count|latencies(_summary)?)|dropped_requests|storage_(data_key_generation|transformation_(failures_total|latencies_microseconds))|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)|kubelet_docker_(operations(_latency_microseconds|_errors|_timeout)?)|reflector_(items_per_(list|watch)|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)|etcd_(helper_(cache_(hit|miss)_count|cache_entry_count|object_counts)|request_(cache_(get|add)_latencies_summary|latencies_summary)|debugging.*|disk.*|server.*)|transformation_(latencies_microseconds|failures_total)|(admission_quota_controller|APIServiceOpenAPIAggregationControllerQueue1|APIServiceRegistrationController|autoregister|AvailableConditionController|crd_(autoregistration_controller|Establishing|finalizer|naming_condition_controller|openapi_controller)|DiscoveryController|non_structural_schema_condition_controller|kubeproxy_sync_proxy_rules|rest_client_request_latency|storage_operation_(errors_total|status_count))(_.*)|apiserver_admission_(controller_admission|step_admission)_latencies_seconds_.*)")
          action = "drop"
        }
    
        // drop metrics whose name and le label match the drop_les regex
        rule {
          source_labels = [
            "__name__",
            "le",
          ]
          regex = coalesce(argument.drop_les.value, "apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "probes" {
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"metadata.name=kubernetes\"])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      argument "job_label" {
        comment = "The job label to add for all probes metric (default: integrations/kubernetes/kube-probes)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.probes.output
      }
    
      // probes service discovery for all of the nodes
      discovery.kubernetes "probes" {
        role = "node"
    
        selectors {
          role = "node"
          field = join(coalesce(argument.field_selectors.value, []), ",")
          label = join(coalesce(argument.label_selectors.value, []), ",")
        }
      }
    
      // probes relabelings (pre-scrape)
      discovery.relabel "probes" {
        targets = discovery.kubernetes.probes.targets
    
        // set the address to use the kubernetes service dns name
        rule {
          target_label = "__address__"
          replacement  = "kubernetes.default.svc.cluster.local:443"
        }
    
        // set the metrics path to use the proxy path to the nodes probes metrics endpoint
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          regex = "(.+)"
          replacement = "/api/v1/nodes/${1}/proxy/metrics/probes"
          target_label = "__metrics_path__"
        }
    
        // set the node label
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "node"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_node_label_app_kubernetes_io_name",
            "__meta_kubernetes_node_label_k8s_app",
            "__meta_kubernetes_node_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // probes scrape job
      prometheus.scrape "probes" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-probes")
        forward_to = [prometheus.relabel.probes.receiver]
        targets = discovery.relabel.probes.output
        scheme = "https"
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
    
        tls_config {
          ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          insecure_skip_verify = false
          server_name = "kubernetes"
        }
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // probes metric relabelings (post-scrape)
      prometheus.relabel "probes" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
    
    declare "kube_dns" {
      argument "forward_to" {
        comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
      }
      // arguments for kubernetes discovery
      argument "namespaces" {
        comment = "The namespaces to look for targets in (default: [\"kube-system\"])"
        optional = true
      }
      argument "field_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [])"
        optional = true
      }
      argument "label_selectors" {
        // Docs: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        comment = "The label selectors to use to find matching targets (default: [\"k8s-app=kube-dns\"])"
        optional = true
      }
      argument "port_name" {
        comment = "The of the port to scrape metrics from (default: metrics)"
        optional = true
      }
      argument "job_label" {
        comment = "The job label to add for all kube_dns metric (default: integrations/kubernetes/kube-dns)"
        optional = true
      }
      argument "keep_metrics" {
        comment = "A regular expression of metrics to keep (default: see below)"
        optional = true
      }
      argument "drop_metrics" {
        comment = "A regular expression of metrics to drop (default: see below)"
        optional = true
      }
      argument "scrape_interval" {
        comment = "How often to scrape metrics from the targets (default: 60s)"
        optional = true
      }
      argument "scrape_timeout" {
        comment = "How long before a scrape times out (default: 10s)"
        optional = true
      }
      argument "max_cache_size" {
        comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  This should be at least 2x-5x your largest scrape target or samples appended rate."
        optional = true
      }
      argument "clustering" {
        // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
        comment = "Whether or not clustering should be enabled (default: false)"
        optional = true
      }
    
      export "output" {
        value = discovery.relabel.kube_dns.output
      }
    
      // kube_dns service discovery for all of the nodes
      discovery.kubernetes "kube_dns" {
        role = "endpoints"
    
        selectors {
          role = "endpoints"
          field = join(coalesce(argument.field_selectors.value, []), ",")
          label = join(coalesce(argument.label_selectors.value, ["k8s-app=kube-dns"]), ",")
        }
    
        namespaces {
          names = coalesce(argument.namespaces.value, ["kube-system"])
        }
      }
    
      // kube_dns relabelings (pre-scrape)
      discovery.relabel "kube_dns" {
        targets = discovery.kubernetes.kube_dns.targets
    
        // keep only the specified metrics port name, and pods that are Running and ready
        rule {
          source_labels = [
            "__meta_kubernetes_pod_container_port_name",
            "__meta_kubernetes_pod_phase",
            "__meta_kubernetes_pod_ready",
          ]
          separator = "@"
          regex = coalesce(argument.port_name.value, "metrics") + "@Running@true"
          action = "keep"
        }
    
        // drop any init containers
        rule {
          source_labels = ["__meta_kubernetes_pod_container_init"]
          regex = "true"
          action = "drop"
        }
    
        // set the namespace label
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
    
        // set the pod label
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
    
        // set the container label
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
    
        // set a workload label
        rule {
          source_labels = [
            "__meta_kubernetes_pod_controller_kind",
            "__meta_kubernetes_pod_controller_name",
          ]
          separator = "/"
          target_label  = "workload"
        }
        // remove the hash from the ReplicaSet
        rule {
          source_labels = ["workload"]
          regex = "(ReplicaSet/.+)-.+"
          target_label  = "workload"
        }
    
        // set the app name if specified as metadata labels "app:" or "app.kubernetes.io/name:" or "k8s-app:"
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_label_k8s_app",
            "__meta_kubernetes_pod_label_app",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "app"
        }
    
        // set the service label
        rule {
          source_labels = ["__meta_kubernetes_service_name"]
          target_label  = "service"
        }
    
        // set a source label
        rule {
          action = "replace"
          replacement = "kubernetes"
          target_label = "source"
        }
      }
    
       // kube_dns scrape job
      prometheus.scrape "kube_dns" {
        job_name = coalesce(argument.job_label.value, "integrations/kubernetes/kube-dns")
        forward_to = [prometheus.relabel.kube_dns.receiver]
        targets = discovery.relabel.kube_dns.output
        scrape_interval = coalesce(argument.scrape_interval.value, "60s")
        scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")
    
        clustering {
          enabled = coalesce(argument.clustering.value, false)
        }
      }
    
      // kube_dns metric relabelings (post-scrape)
      prometheus.relabel "kube_dns" {
        forward_to = argument.forward_to.value
        max_cache_size = coalesce(argument.max_cache_size.value, 100000)
    
        // drop metrics that match the drop_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.drop_metrics.value, "(^(go|process|promhttp)_.+$)")
          action = "drop"
        }
    
        // keep only metrics that match the keep_metrics regex
        rule {
          source_labels = ["__name__"]
          regex = coalesce(argument.keep_metrics.value, "(.+)")
          action = "keep"
        }
      }
    }
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.10.1
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.10.1
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-logs
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-logs
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-metrics
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-metrics
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-logs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.10.1
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-metrics-cluster
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  clusterIP: 'None'
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-logs/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.10.1
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-logs
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
      labels:
        app.kubernetes.io/name: alloy-logs
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-logs
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.5.1
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - SETPCAP
              - NET_BIND_SERVICE
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: dockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-logs
        - name: varlog
          hostPath:
            path: /var/log
        - name: dockercontainers
          hostPath:
            path: /var/lib/docker/containers
---
# Source: k8s-monitoring/charts/alloy-metrics/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: k8smon-alloy-metrics
  labels:
    helm.sh/chart: alloy-metrics-0.10.1
    app.kubernetes.io/name: alloy-metrics
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.5.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  podManagementPolicy: Parallel
  minReadySeconds: 10
  serviceName: k8smon-alloy-metrics
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-metrics
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-metrics
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-metrics
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.5.1
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --cluster.enabled=true
            - --cluster.join-addresses=k8smon-alloy-metrics-cluster
            - --cluster.name=alloy-metrics
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - CHOWN
              - DAC_OVERRIDE
              - FOWNER
              - FSETID
              - KILL
              - SETGID
              - SETUID
              - SETPCAP
              - NET_BIND_SERVICE
              - NET_RAW
              - SYS_CHROOT
              - MKNOD
              - AUDIT_WRITE
              - SETFCAP
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-metrics
