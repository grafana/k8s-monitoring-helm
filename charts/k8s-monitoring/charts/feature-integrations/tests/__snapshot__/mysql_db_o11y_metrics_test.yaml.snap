should create the MySQL config:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        remote.kubernetes.secret "my_database" {
          name      = "my-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "my_database" {
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["password"]),
            "my-db.mysql.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }

        database_observability.mysql "my_database" {
          targets = prometheus.exporter.mysql.my_database.targets
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["password"]),
            "my-db.mysql.svc",
            3306,
          )
          allow_update_performance_schema_settings = false
          query_details {
            collect_interval = "1m"
          }
          query_samples {
            collect_interval = "1m"
            disable_query_redaction = false
            auto_enable_setup_consumers = false
            setup_consumers_check_interval = "1h"
          }
          schema_details {
            collect_interval = "1m"
            cache_enabled = false
            cache_size = 256
            cache_ttl = "10m"
          }
          setup_consumers {
            collect_interval = "1m"
          }
          enable_collectors = ["query_details","query_samples","schema_details","setup_consumers"]

          forward_to = argument.logs_destinations.value
        }
        prometheus.scrape "my_database" {
          targets = database_observability.mysql.my_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.my_database.receiver]
        }

        prometheus.relabel "my_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "my-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
works with multiple MySQL Instances:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        prometheus.exporter.mysql "test_db" {
          data_source_name = string.format("%s:%d/", "database.test.svc", 3306)
          enable_collectors = ["heartbeat","mysql.user"]
        }

        database_observability.mysql "test_db" {
          targets = prometheus.exporter.mysql.test_db.targets
          data_source_name = string.format("%s:%d/", "database.test.svc", 3306)
          allow_update_performance_schema_settings = false
          query_details {
            collect_interval = "1m"
          }
          query_samples {
            collect_interval = "1m"
            disable_query_redaction = false
            auto_enable_setup_consumers = false
            setup_consumers_check_interval = "1h"
          }
          schema_details {
            collect_interval = "1m"
            cache_enabled = false
            cache_size = 256
            cache_ttl = "10m"
          }
          setup_consumers {
            collect_interval = "1m"
          }
          enable_collectors = ["query_details","query_samples","schema_details","setup_consumers"]

          forward_to = argument.logs_destinations.value
        }
        prometheus.scrape "test_db" {
          targets = database_observability.mysql.test_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_db.receiver]
        }

        prometheus.relabel "test_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }

        prometheus.exporter.mysql "staging_db" {
          data_source_name = "root:password@database.staging.svc:3306/"
          enable_collectors = ["heartbeat","mysql.user"]
        }

        database_observability.mysql "staging_db" {
          targets = prometheus.exporter.mysql.staging_db.targets
          data_source_name = "root:password@database.staging.svc:3306/"
          allow_update_performance_schema_settings = false
          query_details {
            collect_interval = "1m"
          }
          query_samples {
            collect_interval = "1m"
            disable_query_redaction = false
            auto_enable_setup_consumers = false
            setup_consumers_check_interval = "1h"
          }
          schema_details {
            collect_interval = "1m"
            cache_enabled = false
            cache_size = 256
            cache_ttl = "10m"
          }
          setup_consumers {
            collect_interval = "1m"
          }
          enable_collectors = ["query_details","query_samples","schema_details","setup_consumers"]

          forward_to = argument.logs_destinations.value
        }
        prometheus.scrape "staging_db" {
          targets = database_observability.mysql.staging_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.staging_db.receiver]
        }

        prometheus.relabel "staging_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "staging-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }

        remote.kubernetes.secret "prod_db" {
          name      = "prod-db-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "prod_db" {
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["password"]),
            "database.prod.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }

        database_observability.mysql "prod_db" {
          targets = prometheus.exporter.mysql.prod_db.targets
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["password"]),
            "database.prod.svc",
            3306,
          )
          allow_update_performance_schema_settings = false
          query_details {
            collect_interval = "1m"
          }
          query_samples {
            collect_interval = "1m"
            disable_query_redaction = false
            auto_enable_setup_consumers = false
            setup_consumers_check_interval = "1h"
          }
          schema_details {
            collect_interval = "1m"
            cache_enabled = false
            cache_size = 256
            cache_ttl = "10m"
          }
          setup_consumers {
            collect_interval = "1m"
          }
          enable_collectors = ["query_details","query_samples","schema_details","setup_consumers"]

          forward_to = argument.logs_destinations.value
        }
        prometheus.scrape "prod_db" {
          targets = database_observability.mysql.prod_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.prod_db.receiver]
        }

        prometheus.relabel "prod_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "prod-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
