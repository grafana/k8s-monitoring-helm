allows you to set the network protocol:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "test_database" {
          data_source_name = string.format("%s:%s@tcp(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.test_database.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.test_database.data["password"]),
            "test-database-mysql.mysql.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.mysql.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
allows you to set the tls:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "test_database" {
          data_source_name = string.format("%s:%s@(%s:%d)/?allowFallbackToPlaintext=true&tls=preferred",
            convert.nonsensitive(remote.kubernetes.secret.test_database.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.test_database.data["password"]),
            "test-database-mysql.mysql.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.mysql.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
should create the MySQL config:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        remote.kubernetes.secret "my_database" {
          name      = "my-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "my_database" {
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.my_database.data["password"]),
            "my-db.mysql.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "my_database" {
          targets = prometheus.exporter.mysql.my_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.my_database.receiver]
        }

        prometheus.relabel "my_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "my-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
works when referencing the MySQL Secret:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-mysql"
          namespace = "mysql"
        }

        prometheus.exporter.mysql "test_database" {
          data_source_name = string.format("%s:%s@(%s:%d)/",
            sys.env(MYSQL_ROOT_USER),
            convert.nonsensitive(remote.kubernetes.secret.test_database.data["mysql-root-password"]),
            "test-database-mysql.mysql.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.mysql.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
works with multiple MySQL Instances:
  1: |
    |-
      declare "mysql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }
        argument "logs_destinations" {
          comment = "Must be a list of log destinations where collected logs should be forwarded to"
        }

        prometheus.exporter.mysql "test_db" {
          data_source_name = string.format("%s:%d/", "database.test.svc", 3306)
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "test_db" {
          targets = prometheus.exporter.mysql.test_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_db.receiver]
        }

        prometheus.relabel "test_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }

        prometheus.exporter.mysql "staging_db" {
          data_source_name = "root:password@database.staging.svc:3306/"
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "staging_db" {
          targets = prometheus.exporter.mysql.staging_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.staging_db.receiver]
        }

        prometheus.relabel "staging_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "staging-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }

        remote.kubernetes.secret "prod_db" {
          name      = "prod-db-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.mysql "prod_db" {
          data_source_name = string.format("%s:%s@(%s:%d)/",
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["username"]),
            convert.nonsensitive(remote.kubernetes.secret.prod_db.data["password"]),
            "database.prod.svc",
            3306,
          )
          enable_collectors = ["heartbeat","mysql.user"]
        }
        prometheus.scrape "prod_db" {
          targets = prometheus.exporter.mysql.prod_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.prod_db.receiver]
        }

        prometheus.relabel "prod_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "prod-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/mysql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
