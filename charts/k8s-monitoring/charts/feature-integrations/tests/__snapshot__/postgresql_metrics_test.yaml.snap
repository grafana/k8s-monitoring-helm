allows you to set the network protocol:
  1: |
    |-
      declare "postgresql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.postgres "test_database" {
          data_source_names = [string.format("tcp://%s:%s@%s:%d/",
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.test_database.data["username"])),
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.test_database.data["password"])),
            "test-database-postgresql.postgresql.svc",
            5432,
          )]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.postgres.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
allows you to set the tls:
  1: |
    |-
      declare "postgresql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.postgres "test_database" {
          data_source_names = [string.format("postgresql://%s:%s@%s:%d/",
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.test_database.data["username"])),
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.test_database.data["password"])),
            "test-database-postgresql.postgresql.svc",
            5432,
          )]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.postgres.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
should create the PostgreSQL config:
  1: |
    |-
      declare "postgresql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }

        remote.kubernetes.secret "my_database" {
          name      = "my-database-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.postgres "my_database" {
          data_source_names = [string.format("postgresql://%s:%s@%s:%d/",
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.my_database.data["username"])),
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.my_database.data["password"])),
            "my-db.postgresql.svc",
            5432,
          )]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "my_database" {
          targets = prometheus.exporter.postgres.my_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.my_database.receiver]
        }

        prometheus.relabel "my_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "my-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
works when referencing the PostgreSQL Secret:
  1: |
    |-
      declare "postgresql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }

        remote.kubernetes.secret "test_database" {
          name      = "test-database-postgresql"
          namespace = "postgresql"
        }

        prometheus.exporter.postgres "test_database" {
          data_source_names = [string.format("postgresql://%s:%s@%s:%d/",
            encoding.url_encode(sys.env(MYSQL_ROOT_USER)),
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.test_database.data["postgresql-root-password"])),
            "test-database-postgresql.postgresql.svc",
            5432,
          )]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "test_database" {
          targets = prometheus.exporter.postgres.test_database.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_database.receiver]
        }

        prometheus.relabel "test_database" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-database"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
works with multiple PostgreSQL Instances:
  1: |
    |-
      declare "postgresql_integration" {
        argument "metrics_destinations" {
          comment = "Must be a list of metric destinations where collected metrics should be forwarded to"
        }

        prometheus.exporter.postgres "test_db" {
          data_source_names = [string.format("postgresql://%s:%d/", "database.test.svc", 5432)]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "test_db" {
          targets = prometheus.exporter.postgres.test_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.test_db.receiver]
        }

        prometheus.relabel "test_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "test-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }

        prometheus.exporter.postgres "staging_db" {
          data_source_names = ["root:password@database.staging.svc:3306/"]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "staging_db" {
          targets = prometheus.exporter.postgres.staging_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.staging_db.receiver]
        }

        prometheus.relabel "staging_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "staging-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }

        remote.kubernetes.secret "prod_db" {
          name      = "prod-db-release-name-feature-integrations"
          namespace = "NAMESPACE"
        }

        prometheus.exporter.postgres "prod_db" {
          data_source_names = [string.format("postgresql://%s:%s@%s:%d/",
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.prod_db.data["username"])),
            encoding.url_encode(convert.nonsensitive(remote.kubernetes.secret.prod_db.data["password"])),
            "database.prod.svc",
            5432,
          )]
          enabled_collectors = ["database","locks","replication","replication_slot","stat_bgwriter","stat_database","stat_progress_vacuum","stat_user_tables","statio_user_tables","wal"]
          disable_default_metrics = false
          disable_settings_metrics = false
        }
        prometheus.scrape "prod_db" {
          targets = prometheus.exporter.postgres.prod_db.targets
          clustering {
            enabled = true
          }

          scrape_interval = "60s"
          scrape_timeout = "10s"
          scrape_protocols = ["OpenMetricsText1.0.0","OpenMetricsText0.0.1","PrometheusText0.0.4"]
          scrape_classic_histograms = false
          scrape_native_histograms = false
          forward_to = [prometheus.relabel.prod_db.receiver]
        }

        prometheus.relabel "prod_db" {
          max_cache_size = 100000
          rule {
            target_label = "instance"
            replacement = "prod-db"
          }
          rule {
            target_label = "job"
            replacement = "integration/postgresql"
          }
          forward_to = argument.metrics_destinations.value
        }
      }
