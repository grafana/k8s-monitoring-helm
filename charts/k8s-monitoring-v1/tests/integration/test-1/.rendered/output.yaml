---
# Source: k8s-monitoring/charts/alloy-events/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: k8smon-alloy-events
  namespace: default
  labels:
    helm.sh/chart: alloy-events-1.4.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy-logs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-1.4.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy-profiles/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: k8smon-alloy-profiles
  namespace: default
  labels:
    helm.sh/chart: alloy-profiles-1.4.0
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: k8smon-alloy
  namespace: default
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/opencost/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
automountServiceAccountToken: false
---
# Source: k8s-monitoring/templates/log-service-credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: loki-k8s-monitoring
  namespace: default
type: Opaque
data:

  host: "aHR0cDovL2xva2kubG9raS5zdmM6MzEwMA=="
  username: "bG9raQ=="
  password: "bG9raXBhc3N3b3Jk"
  tenantId: "MQ=="
---
# Source: k8s-monitoring/templates/profiles-service-credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: pyroscope-k8s-monitoring
  namespace: default
type: Opaque
data:

  host: "aHR0cDovL3B5cm9zY29wZS5weXJvc2NvcGUuc3ZjOjQwNDA="
  tenantId: "MQ=="
---
# Source: k8s-monitoring/templates/trace-service-credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: tempo-k8s-monitoring
  namespace: default
type: Opaque
data:

  host: "dGVtcG8udGVtcG8uc3ZjOjQzMTc="
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy
  namespace: default
data:
  config.alloy: |-
    discovery.kubernetes "nodes" {
      role = "node"
    }
    
    discovery.kubernetes "services" {
      role = "service"
    }
    
    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }
    
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    // OTLP Receivers
    otelcol.receiver.otlp "receiver" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      http {
        endpoint = "0.0.0.0:4318"
      }
      debug_metrics {
        disable_high_cardinality_metrics = true
      }
      output {
        metrics = [otelcol.processor.resourcedetection.default.input]
        logs = [otelcol.processor.resourcedetection.default.input]
        traces = [otelcol.processor.resourcedetection.default.input]
      }
    }
    
    
    
    
    // Processors
    otelcol.processor.transform "add_metric_datapoint_attributes" {
      error_mode = "ignore"
      metric_statements {
        context = "datapoint"
        statements = [
          "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
          "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
        ]
      }
      output {
        metrics = [otelcol.processor.k8sattributes.default.input]
      }
    }
    otelcol.processor.resourcedetection "default" {
      system {
        hostname_sources = ["os"]
      }
    
      detectors = ["env","system"]
    
      output {
        metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
        logs    = [otelcol.processor.k8sattributes.default.input]
        traces  = [otelcol.processor.k8sattributes.default.input]
      }
    }
    
    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
      }
      pod_association {
        source {
          from = "connection"
        }
      }
    
      output {
        metrics = [otelcol.processor.attributes.default.input]
        logs    = [otelcol.processor.attributes.default.input]
        traces  = [otelcol.processor.attributes.default.input]
      }
    }
    
    otelcol.processor.attributes "default" {
    
      output {
        metrics = [otelcol.processor.transform.default.input]
        logs    = [otelcol.processor.transform.default.input]
        traces  = [
          otelcol.processor.transform.default.input,
          otelcol.connector.host_info.default.input,
        ]
      }
    }
    otelcol.connector.host_info "default" {
      host_identifiers = [ "k8s.node.name" ]
      output {
        metrics = [otelcol.processor.batch.host_info_batch.input]
      }
    }
    
    otelcol.processor.batch "host_info_batch" {
      output {
        metrics = [otelcol.exporter.prometheus.host_info_metrics.input]
      }
    }
    
    otelcol.exporter.prometheus "host_info_metrics" {
      add_metric_suffixes = false
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }
    
    otelcol.processor.transform "default" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
          "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
          "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      trace_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      output {
        metrics = [otelcol.processor.filter.default.input]
        logs = [otelcol.processor.filter.default.input]
        traces = [otelcol.processor.filter.default.input]
      }
    }
    
    otelcol.processor.filter "default" {
      error_mode = "ignore"
      traces {
        span = [
          "attributes[\"http.route\"] == \"/live\"",
          "attributes[\"http.route\"] == \"/healthy\"",
          "attributes[\"http.route\"] == \"/ready\"",
        ]
      }
    
      output {
        metrics = [otelcol.processor.batch.batch_processor.input]
        logs = [otelcol.processor.batch.batch_processor.input]
        traces = [otelcol.processor.batch.batch_processor.input]
      }
    }
    
    otelcol.processor.batch "batch_processor" {
      send_batch_size = 16384
      send_batch_max_size = 0
      timeout = "2s"
      output {
        metrics = [otelcol.exporter.prometheus.metrics_converter.input]
        logs = [otelcol.exporter.loki.logs_converter.input]
        traces = [otelcol.exporter.otlp.traces_service.input]
      }
    }
    otelcol.exporter.prometheus "metrics_converter" {
      add_metric_suffixes = true
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    otelcol.exporter.loki "logs_converter" {
      forward_to = [loki.process.pod_logs.receiver]
    }
    // Annotation Autodiscovery
    discovery.relabel "annotation_autodiscovery_pods" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }
    
      // Choose the pod port
      // The discovery generates a target for each declared container port of the pod.
      // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
        action = "replace"
        target_label = "__scrape_interval__"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_services" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }
    
      // Choose the service port
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
        action = "replace"
        target_label = "__scrape_interval__"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_http" {
      targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "drop"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_https" {
      targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "keep"
      }
    }
    
    prometheus.scrape "annotation_autodiscovery_http" {
      targets = discovery.relabel.annotation_autodiscovery_http.output
      scrape_interval = "60s"
      honor_labels = true
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }
    
    prometheus.scrape "annotation_autodiscovery_https" {
      targets = discovery.relabel.annotation_autodiscovery_https.output
      scrape_interval = "60s"
      honor_labels = true
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }
    
    prometheus.relabel "annotation_autodiscovery" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Grafana Alloy
    discovery.relabel "alloy" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "alloy.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        regex = "http-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
    }
    
    prometheus.scrape "alloy" {
      job_name = "integrations/alloy"
      targets = discovery.relabel.alloy.output
      scrape_interval = "60s"
      forward_to = [prometheus.relabel.alloy.receiver]
      clustering {
        enabled = true
      }
    }
    
    prometheus.relabel "alloy" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|alloy_build_info"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubernetes Monitoring Telemetry
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_build_info"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubelet
    discovery.relabel "kubelet" {
      targets = discovery.kubernetes.nodes.targets
    }
    
    prometheus.scrape "kubelet" {
      job_name   = "integrations/kubernetes/kubelet"
      targets  = discovery.relabel.kubelet.output
      scheme   = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubelet.receiver]
    }
    
    prometheus.relabel "kubelet" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubelet Resource
    discovery.relabel "kubelet_resource" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        replacement   = "/metrics/resource"
        target_label  = "__metrics_path__"
      }
    }
    
    prometheus.scrape "kubelet_resource" {
      job_name   = "integrations/kubernetes/resources"
      targets  = discovery.relabel.kubelet_resource.output
      scheme   = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubelet_resource.receiver]
    }
    
    prometheus.relabel "kubelet_resource" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // cAdvisor
    discovery.relabel "cadvisor" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        replacement   = "/metrics/cadvisor"
        target_label  = "__metrics_path__"
      }
    }
    
    prometheus.scrape "cadvisor" {
      job_name   = "integrations/kubernetes/cadvisor"
      targets    = discovery.relabel.cadvisor.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.cadvisor.receiver]
    }
    
    prometheus.relabel "cadvisor" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
        action = "keep"
      }
      // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","container"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
        action = "drop"
      }
      // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","image"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
        action = "drop"
      }
      // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
      rule {
        source_labels = ["__name__", "boot_id"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "boot_id"
        replacement = "NA"
      }
      rule {
        source_labels = ["__name__", "system_uuid"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "system_uuid"
        replacement = "NA"
      }
      // Filter out non-physical devices/interfaces
      rule {
        source_labels = ["__name__", "device"]
        separator = "@"
        regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_fs_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_fs_.*"
        target_label = "__keepme"
        replacement = ""
      }
      rule {
        source_labels = ["__name__", "interface"]
        separator = "@"
        regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_network_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_network_.*"
        target_label = "__keepme"
        replacement = ""
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // API Server
    discovery.relabel "apiserver" {
      targets = discovery.kubernetes.endpoints.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "default"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_name"]
        regex = "kubernetes"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_endpoint_port_name"]
        regex = "https"
        action = "keep"
      }
    }
    
    prometheus.scrape "apiserver" {
      job_name   = "integrations/kubernetes/apiserver"
      targets    = discovery.relabel.apiserver.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.apiserver.receiver]
    }
    
    prometheus.relabel "apiserver" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Controller Manager
    discovery.relabel "kube_controller_manager" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_component"]
        regex = "kube-controller-manager"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10257"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_controller_manager" {
      job_name   = "kube-controller-manager"
      targets    = discovery.relabel.kube_controller_manager.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_controller_manager.receiver]
    }
    
    prometheus.relabel "kube_controller_manager" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Scheduler
    discovery.relabel "kube_scheduler" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_component"]
        regex = "kube-scheduler"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10259"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_scheduler" {
      job_name   = "kube-scheduler"
      targets    = discovery.relabel.kube_scheduler.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_scheduler.receiver]
    }
    
    prometheus.relabel "kube_scheduler" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Proxy
    discovery.relabel "kube_proxy" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_k8s_app"]
        regex = "kube-proxy"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10249"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_proxy" {
      job_name   = "integrations/kubernetes/kube-proxy"
      targets    = discovery.relabel.kube_proxy.output
      scheme     = "http"
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_proxy.receiver]
    }
    
    prometheus.relabel "kube_proxy" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube State Metrics
    discovery.relabel "kube_state_metrics" {
      targets = discovery.kubernetes.endpoints.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_release"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "kube-state-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_endpoint_port_name"]
        regex = "http"
        action = "keep"
      }
    }
    
    prometheus.scrape "kube_state_metrics" {
      job_name   = "integrations/kubernetes/kube-state-metrics"
      targets    = discovery.relabel.kube_state_metrics.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_state_metrics.receiver]
    }
    
    prometheus.relabel "kube_state_metrics" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_pod_init_.*|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
        action = "keep"
      }
      rule {
        source_labels = ["__name__"]
        regex = "kube_pod_info"
        action = "drop"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Node Exporter
    discovery.relabel "node_exporter" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_release"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "prometheus-node-exporter.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "replace"
        target_label = "instance"
      }
    }
    
    prometheus.scrape "node_exporter" {
      job_name   = "integrations/node_exporter"
      targets  = discovery.relabel.node_exporter.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.node_exporter.receiver]
    }
    
    prometheus.relabel "node_exporter" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
        action = "keep"
      }
      // Drop metrics for certain file systems
      rule {
        source_labels = ["__name__", "fstype"]
        separator = "@"
        regex = "node_filesystem.*@(ramfs|tmpfs)"
        action = "drop"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // OpenCost
    discovery.relabel "opencost" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "opencost"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
    }
    
    prometheus.scrape "opencost" {
      targets    = discovery.relabel.opencost.output
      job_name   = "integrations/kubernetes/opencost"
      honor_labels = true
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.opencost.receiver]
    }
    
    prometheus.relabel "opencost" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator PodMonitor objects
    prometheus.operator.podmonitors "pod_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator Probe objects
    prometheus.operator.probes "probes" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator ServiceMonitor objects
    prometheus.operator.servicemonitors "service_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Metrics Service
    remote.kubernetes.secret "metrics_service" {
      name = "prometheus"
      namespace = "default"
    }
    
    prometheus.relabel "metrics_service" {
      max_cache_size = 100000
      rule {
        source_labels = ["cluster"]
        regex = ""
        replacement = "ci-test-cluster"
        target_label = "cluster"
      }
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }
    
    prometheus.remote_write "metrics_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["host"]) + "/api/v1/write"
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["tenantId"]),
        }
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["username"])
          password = remote.kubernetes.secret.metrics_service.data["password"]
        }
    
        tls_config {
          insecure_skip_verify = true
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    
      external_labels = {
        color = "red",
      }
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Move high-cardinality labels to structured metadata
      stage.structured_metadata {
        values = {
          name = "app_kubernetes_io_name",
          pod = "",
        }
      }
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
      // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
      // container runtime label as it is no longer needed.
      stage.label_drop {
        values = [
          "filename",
          "tmp_container_runtime",
          "app_kubernetes_io_name",
          "pod",
        ]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    // Tempo
    remote.kubernetes.secret "traces_service" {
      name = "tempo-k8s-monitoring"
      namespace = "default"
    }
    
    
    otelcol.exporter.otlp "traces_service" {
      client {
        endpoint = convert.nonsensitive(remote.kubernetes.secret.traces_service.data["host"])
    
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.traces_service.data["tenantId"]),
        }
        tls {
          insecure             = true
          insecure_skip_verify = true
        }
      }
    }
    
    logging {
      level  = "info"
      format = "logfmt"
    }
    
    livedebugging {
      enabled = true
    }
    tracing {
      sampling_fraction = 0.1
      write_to = [otelcol.processor.k8sattributes.default.input]
    }
  k8s-monitoring-build-info-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart as well as a summary of enabled features
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="1.6.48", namespace="default", metrics="enabled,alloy,autoDiscover,kube-state-metrics,node-exporter,kubelet,kubeletResource,cadvisor,apiserver,cost,extraConfig", logs="enabled,events,pod_logs", traces="enabled", deployments="kube-state-metrics,prometheus-node-exporter,opencost"} 1
---
# Source: k8s-monitoring/templates/alloy-events-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-events
  namespace: default
data:
  config.alloy: |-
    // Cluster Events
    loki.source.kubernetes_events "cluster_events" {
      job_name   = "integrations/kubernetes/eventhandler"
      log_format = "logfmt"
      forward_to = [
        loki.process.cluster_events.receiver,
      ]
    }
    
    loki.process "cluster_events" {
      forward_to = [
        loki.process.logs_service.receiver,
        loki.echo.cluster_events.receiver,
      ]
    }
    loki.echo "cluster_events" {}
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/alloy-logs-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Pod Logs
    discovery.kubernetes "pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "$1"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex         = "(.+)"
        target_label  = "app_kubernetes_io_name"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
        regex         = "(.+)"
        target_label  = "job"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "/var/log/pods/*$1/*.log"
        target_label = "__path__"
      }
    
      // set the container runtime as a label
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_pod_container_id"]
        regex = "^(\\S+):\\/\\/.+$"
        replacement = "$1"
        target_label = "tmp_container_runtime"
      }
    }
    
    discovery.relabel "filtered_pod_logs" {
      targets = discovery.relabel.pod_logs.output
      rule {  // Drop anything with a "falsy" annotation value
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_autogather"]
        regex = "(false|no|skip)"
        action = "drop"
      }
    }
    
    local.file_match "pod_logs" {
      path_targets = discovery.relabel.filtered_pod_logs.output
    }
    
    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Move high-cardinality labels to structured metadata
      stage.structured_metadata {
        values = {
          name = "app_kubernetes_io_name",
          pod = "",
        }
      }
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
      // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
      // container runtime label as it is no longer needed.
      stage.label_drop {
        values = [
          "filename",
          "tmp_container_runtime",
          "app_kubernetes_io_name",
          "pod",
        ]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/alloy-profiles-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-profiles
  namespace: default
data:
  config.alloy: |-
    // Profiles: eBPF
    discovery.kubernetes "ebpf_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "ebpf_pods" {
      targets = discovery.kubernetes.ebpf_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex = "Succeeded|Failed|Completed"
        action = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_ebpf_enabled"]
        regex         = "false"
        action        = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label = "node"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label = "container"
      }
      // provide arbitrary service_name label, otherwise it will be set to {__meta_kubernetes_namespace}/{__meta_kubernetes_pod_container_name}
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "@"
        regex = "(.*)@(.*)"
        replacement = "ebpf/${1}/${2}"
        target_label = "service_name"
      }
    }
    
    pyroscope.ebpf "ebpf_pods" {
      targets = discovery.relabel.ebpf_pods.output
      demangle = "none"
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Profiles: Java
    discovery.kubernetes "java_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "potential_java_pods" {
      targets = discovery.kubernetes.java_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex         = "Succeeded|Failed|Completed"
        action        = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_java_enabled"]
        regex         = "false"
        action        = "drop"
      }
    }
    
    discovery.process "java_pods" {
      join = discovery.relabel.potential_java_pods.output
    }
    
    discovery.relabel "java_pods" {
      targets = discovery.process.java_pods.targets
      rule {
        source_labels = ["__meta_process_exe"]
        action = "keep"
        regex = ".*/java$"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label = "node"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label = "container"
      }
    }
    
    pyroscope.java "java_pods" {
      targets = discovery.relabel.java_pods.output
      profiling_config {
        interval = "60s"
        alloc = "512k"
        cpu = true
        sample_rate = 100
        lock = "10ms"
      }
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Profiles: pprof
    discovery.kubernetes "pprof_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "pprof_pods" {
      targets = discovery.kubernetes.pprof_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex         = "Pending|Succeeded|Failed|Completed"
        action        = "drop"
      }
    
      rule {
        regex  = "__meta_kubernetes_pod_label_(.+)"
        action = "labelmap"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
    }
    
    discovery.relabel "pprof_pods_memory" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_memory" {
      targets = discovery.relabel.pprof_pods_memory.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = true
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_block" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_block" {
      targets = discovery.relabel.pprof_pods_block.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = true
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_goroutine" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_goroutine" {
      targets = discovery.relabel.pprof_pods_goroutine.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = true
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_mutex" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_mutex" {
      targets = discovery.relabel.pprof_pods_mutex.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = true
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_cpu" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_cpu" {
      targets = discovery.relabel.pprof_pods_cpu.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = true
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_fgprof" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_fgprof" {
      targets = discovery.relabel.pprof_pods_fgprof.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = true
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Pyroscope
    remote.kubernetes.secret "profiles_service" {
      name = "pyroscope-k8s-monitoring"
      namespace = "default"
    }
    pyroscope.write "profiles_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.profiles_service.data["host"])
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.profiles_service.data["tenantId"]),
        }
    
      }
      external_labels = {
        cluster = "ci-test-cluster",
      }
    }
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/charts/alloy-events/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-1.4.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  - apiGroups:
    - ""
    - discovery.k8s.io
    - networking.k8s.io
    resources:
    - endpoints
    - endpointslices
    - ingresses
    - pods
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/log
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.grafana.com
    resources:
    - podlogs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - prometheusrules
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - podmonitors
    - servicemonitors
    - probes
    - scrapeconfigs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - nodes/proxy
    - nodes/metrics
    verbs:
    - get
    - list
    - watch
  - nonResourceURLs:
    - /metrics
    verbs:
    - get
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-1.4.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  - apiGroups:
    - ""
    - discovery.k8s.io
    - networking.k8s.io
    resources:
    - endpoints
    - endpointslices
    - ingresses
    - pods
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/log
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.grafana.com
    resources:
    - podlogs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - prometheusrules
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - podmonitors
    - servicemonitors
    - probes
    - scrapeconfigs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - nodes/proxy
    - nodes/metrics
    verbs:
    - get
    - list
    - watch
  - nonResourceURLs:
    - /metrics
    verbs:
    - get
---
# Source: k8s-monitoring/charts/alloy-profiles/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-profiles
  labels:
    helm.sh/chart: alloy-profiles-1.4.0
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  - apiGroups:
    - ""
    - discovery.k8s.io
    - networking.k8s.io
    resources:
    - endpoints
    - endpointslices
    - ingresses
    - pods
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/log
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.grafana.com
    resources:
    - podlogs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - prometheusrules
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - podmonitors
    - servicemonitors
    - probes
    - scrapeconfigs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - nodes/proxy
    - nodes/metrics
    verbs:
    - get
    - list
    - watch
  - nonResourceURLs:
    - /metrics
    verbs:
    - get
---
# Source: k8s-monitoring/charts/alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  - apiGroups:
    - ""
    - discovery.k8s.io
    - networking.k8s.io
    resources:
    - endpoints
    - endpointslices
    - ingresses
    - pods
    - services
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - pods
    - pods/log
    - namespaces
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.grafana.com
    resources:
    - podlogs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - prometheusrules
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - monitoring.coreos.com
    resources:
    - podmonitors
    - servicemonitors
    - probes
    - scrapeconfigs
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - configmaps
    - secrets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    - extensions
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - nodes
    - nodes/proxy
    - nodes/metrics
    verbs:
    - get
    - list
    - watch
  - nonResourceURLs:
    - /metrics
    verbs:
    - get
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: k8s-monitoring/charts/opencost/templates/clusterrole.yaml
# Cluster role giving opencost to get, list, watch required resources
# No write permissions are required
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - deployments
      - nodes
      - nodes/proxy
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - deployments
      - daemonsets
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/alloy-events/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-1.4.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-events
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-events
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-1.4.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-logs
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-logs
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-profiles/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-profiles
  labels:
    helm.sh/chart: alloy-profiles-1.4.0
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-profiles
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-profiles
    namespace: default
---
# Source: k8s-monitoring/charts/alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy
    namespace: default
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  name: k8smon-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/opencost/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-opencost
subjects:
  - kind: ServiceAccount
    name: k8smon-opencost
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-events/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-events
  namespace: default
  labels:
    helm.sh/chart: alloy-events-1.4.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-logs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-1.4.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-profiles/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-profiles
  namespace: default
  labels:
    helm.sh/chart: alloy-profiles-1.4.0
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-cluster
  namespace: default
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  clusterIP: 'None'
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: UDP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/charts/alloy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy
  namespace: default
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: UDP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
  annotations:
spec:
  type: "ClusterIP"
  ports:
  - name: http
    protocol: TCP
    port: 8080
    targetPort: http
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/opencost/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
  type: "ClusterIP"
  ports:
    - name: http
      port: 9003
      targetPort: 9003
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/templates/grafana-agent-receiver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: UDP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/charts/alloy-logs/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-1.4.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-logs
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-logs
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-logs
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.11.3
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
            - name: varlog
              mountPath: /var/log
              readOnly: true
        - name: config-reloader
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          args:
            - --watched-dir=/etc/alloy
            - --reload-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-logs
        - name: varlog
          hostPath:
            path: /var/log
---
# Source: k8s-monitoring/charts/alloy-profiles/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-alloy-profiles
  namespace: default
  labels:
    helm.sh/chart: alloy-profiles-1.4.0
    app.kubernetes.io/name: alloy-profiles
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-profiles
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-profiles
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-profiles
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.11.3
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=public-preview
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          securityContext:
            privileged: true
            runAsGroup: 0
            runAsUser: 0
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          args:
            - --watched-dir=/etc/alloy
            - --reload-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
      hostPID: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-profiles
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.10.2"
    release: k8smon
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: k8smon
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        k8s.grafana.com/logs.job: integrations/node_exporter
      labels:
        helm.sh/chart: prometheus-node-exporter-4.49.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.10.2"
        release: k8smon
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: k8smon-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                - fargate
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: k8s-monitoring/charts/alloy-events/templates/controllers/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-events
  namespace: default
  labels:
    helm.sh/chart: alloy-events-1.4.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-events
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-events
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-events
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.11.3
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          args:
            - --watched-dir=/etc/alloy
            - --reload-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-events
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.17.0"
    release: k8smon
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: k8smon
  replicas: 1
  strategy:
    type: Recreate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-6.4.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.17.0"
        release: k8smon
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: k8smon-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --metric-labels-allowlist=nodes=[agentpool,alpha.eksctl.io/cluster-name,alpha.eksctl.io/nodegroup-name,beta.kubernetes.io/instance-type,cloud.google.com/gke-nodepool,cluster_name,ec2_amazonaws_com_Name,ec2_amazonaws_com_aws_autoscaling_groupName,ec2_amazonaws_com_aws_autoscaling_group_name,ec2_amazonaws_com_name,eks_amazonaws_com_nodegroup,k8s_io_cloud_provider_aws,karpenter.sh/nodepool,kubernetes.azure.com/cluster,kubernetes.io/arch,kubernetes.io/hostname,kubernetes.io/os,node.kubernetes.io/instance-type,topology.kubernetes.io/region,topology.kubernetes.io/zone]
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
        ports:
        - containerPort: 8080
          name: http
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
---
# Source: k8s-monitoring/charts/opencost/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-opencost
  namespace: default
  labels:
    helm.sh/chart: opencost-2.3.2
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.117.6"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
  annotations:
    checksum/configs: 930260623d85bc2f803e4342c37094e5e1f9ba8b76314664d4abc01dca140c56
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
      app.kubernetes.io/instance: k8smon
  strategy: 
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opencost
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-opencost
      automountServiceAccountToken: true
      tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
      nodeSelector:
        kubernetes.io/os: linux
      initContainers:
      containers:
        - name: k8smon-opencost
          image: ghcr.io/opencost/opencost:1.117.6@sha256:6f1a0e6fe21559a77051e7b7f9e4ac6bc80277131492ae084e8365ada805af91
          imagePullPolicy: IfNotPresent
          args:
          ports:
            - containerPort: 9003
              name: http
          resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 10m
              memory: 55Mi
          startupProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 20
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 3
          env:
            - name: LOG_LEVEL
              value: info
            - name: CUSTOM_COST_ENABLED
              value: "false"
            - name: INSTALL_NAMESPACE
              value: default
            - name: PROMETHEUS_QUERY_RESOLUTION_SECONDS
              value: "300"
            - name: API_PORT
              value: "9003"
            - name: PROMETHEUS_SERVER_ENDPOINT
              value: "http://prometheus-server.prometheus.svc.cluster.local:9090"
            - name: INSECURE_SKIP_VERIFY
              value: "true"
            - name: CLUSTER_ID
              value: "default-cluster"
            - name: DB_BASIC_AUTH_USERNAME
              valueFrom:
                secretKeyRef:
                  name: prometheus
                  key: username
            - name: DB_BASIC_AUTH_PW
              valueFrom:
                secretKeyRef:
                  name: prometheus
                  key: password
            - name: RESOLUTION_1D_RETENTION
              value: "15"
            - name: RESOLUTION_1H_RETENTION
              value: "49"
            - name: CLOUD_COST_ENABLED
              value: "false"
            - name: CLOUD_COST_MONTH_TO_DATE_INTERVAL
              value: "6"
            - name: CLOUD_COST_REFRESH_RATE_HOURS
              value: "6"
            - name: CLOUD_COST_QUERY_WINDOW_DAYS
              value: "7"
            - name: CLOUD_COST_RUN_WINDOW_DAYS
              value: "3"
            - name: EMIT_KSM_V1_METRICS
              value: "false"
            - name: EMIT_KSM_V1_METRICS_ONLY
              value: "true"
            # Add any additional provided variables
            - name: CLOUD_PROVIDER_API_KEY
              value: "AIzaSyD29bGxmHAVEOBYtgd8sYM2gM2ekfxQX4U"
            - name: CURRENT_CLUSTER_ID_FILTER_ENABLED
              value: "true"
            - name: PROM_CLUSTER_ID_LABEL
              value: "cluster"
---
# Source: k8s-monitoring/charts/alloy/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: k8smon-alloy
  namespace: default
  labels:
    helm.sh/chart: alloy-1.4.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v1.11.3"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 2
  podManagementPolicy: Parallel
  serviceName: k8smon-alloy
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.11.3
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --cluster.enabled=true
            - --cluster.join-addresses=k8smon-alloy-cluster
            - --cluster.name=alloy
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
            - containerPort: 4317
              name: otlp-grpc
              protocol: TCP
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 9999
              name: prometheus
              protocol: TCP
            - containerPort: 14250
              name: jaeger-grpc
              protocol: TCP
            - containerPort: 6832
              name: jaeger-binary
              protocol: UDP
            - containerPort: 6831
              name: jaeger-compact
              protocol: UDP
            - containerPort: 14268
              name: jaeger-http
              protocol: TCP
            - containerPort: 9411
              name: zipkin
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.81.0
          args:
            - --watched-dir=/etc/alloy
            - --reload-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 10m
              memory: 50Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy
---
# Source: k8s-monitoring/templates/hooks/validate-configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "validate-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.21.3
    helm.sh/chart: "k8s-monitoring-1.6.48"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
data:
  config.alloy: |-
    discovery.kubernetes "nodes" {
      role = "node"
    }
    
    discovery.kubernetes "services" {
      role = "service"
    }
    
    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }
    
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    // OTLP Receivers
    otelcol.receiver.otlp "receiver" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      http {
        endpoint = "0.0.0.0:4318"
      }
      debug_metrics {
        disable_high_cardinality_metrics = true
      }
      output {
        metrics = [otelcol.processor.resourcedetection.default.input]
        logs = [otelcol.processor.resourcedetection.default.input]
        traces = [otelcol.processor.resourcedetection.default.input]
      }
    }
    
    
    
    
    // Processors
    otelcol.processor.transform "add_metric_datapoint_attributes" {
      error_mode = "ignore"
      metric_statements {
        context = "datapoint"
        statements = [
          "set(attributes[\"deployment.environment\"], resource.attributes[\"deployment.environment\"])",
          "set(attributes[\"service.version\"], resource.attributes[\"service.version\"])",
        ]
      }
      output {
        metrics = [otelcol.processor.k8sattributes.default.input]
      }
    }
    otelcol.processor.resourcedetection "default" {
      system {
        hostname_sources = ["os"]
      }
    
      detectors = ["env","system"]
    
      output {
        metrics = [otelcol.processor.transform.add_metric_datapoint_attributes.input]
        logs    = [otelcol.processor.k8sattributes.default.input]
        traces  = [otelcol.processor.k8sattributes.default.input]
      }
    }
    
    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
      }
      pod_association {
        source {
          from = "connection"
        }
      }
    
      output {
        metrics = [otelcol.processor.attributes.default.input]
        logs    = [otelcol.processor.attributes.default.input]
        traces  = [otelcol.processor.attributes.default.input]
      }
    }
    
    otelcol.processor.attributes "default" {
    
      output {
        metrics = [otelcol.processor.transform.default.input]
        logs    = [otelcol.processor.transform.default.input]
        traces  = [
          otelcol.processor.transform.default.input,
          otelcol.connector.host_info.default.input,
        ]
      }
    }
    otelcol.connector.host_info "default" {
      host_identifiers = [ "k8s.node.name" ]
      output {
        metrics = [otelcol.processor.batch.host_info_batch.input]
      }
    }
    
    otelcol.processor.batch "host_info_batch" {
      output {
        metrics = [otelcol.exporter.prometheus.host_info_metrics.input]
      }
    }
    
    otelcol.exporter.prometheus "host_info_metrics" {
      add_metric_suffixes = false
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }
    
    otelcol.processor.transform "default" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      metric_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      log_statements {
        context = "resource"
        statements = [
          "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
          "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
          "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      trace_statements {
        context = "resource"
        statements = [
          "set(attributes[\"k8s.cluster.name\"], \"ci-test-cluster\") where attributes[\"k8s.cluster.name\"] == nil",
          "set(attributes[\"service.instance.id\"], attributes[\"k8s.pod.uid\"]) where attributes[\"service.instance.id\"] == nil",
        ]
      }
      output {
        metrics = [otelcol.processor.filter.default.input]
        logs = [otelcol.processor.filter.default.input]
        traces = [otelcol.processor.filter.default.input]
      }
    }
    
    otelcol.processor.filter "default" {
      error_mode = "ignore"
      traces {
        span = [
          "attributes[\"http.route\"] == \"/live\"",
          "attributes[\"http.route\"] == \"/healthy\"",
          "attributes[\"http.route\"] == \"/ready\"",
        ]
      }
    
      output {
        metrics = [otelcol.processor.batch.batch_processor.input]
        logs = [otelcol.processor.batch.batch_processor.input]
        traces = [otelcol.processor.batch.batch_processor.input]
      }
    }
    
    otelcol.processor.batch "batch_processor" {
      send_batch_size = 16384
      send_batch_max_size = 0
      timeout = "2s"
      output {
        metrics = [otelcol.exporter.prometheus.metrics_converter.input]
        logs = [otelcol.exporter.loki.logs_converter.input]
        traces = [otelcol.exporter.otlp.traces_service.input]
      }
    }
    otelcol.exporter.prometheus "metrics_converter" {
      add_metric_suffixes = true
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    otelcol.exporter.loki "logs_converter" {
      forward_to = [loki.process.pod_logs.receiver]
    }
    // Annotation Autodiscovery
    discovery.relabel "annotation_autodiscovery_pods" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }
    
      // Choose the pod port
      // The discovery generates a target for each declared container port of the pod.
      // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_portNumber", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_metrics_scrapeInterval"]
        action = "replace"
        target_label = "__scrape_interval__"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_services" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_scrape"]
        regex = "true"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_job"]
        action = "replace"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_instance"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_path"]
        action = "replace"
        target_label = "__metrics_path__"
      }
    
      // Choose the service port
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portName"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_portNumber"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scheme"]
        action = "replace"
        target_label = "__scheme__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_k8s_grafana_com_metrics_scrapeInterval"]
        action = "replace"
        target_label = "__scrape_interval__"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_http" {
      targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "drop"
      }
    }
    
    discovery.relabel "annotation_autodiscovery_https" {
      targets = array.concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output)
      rule {
        source_labels = ["__scheme__"]
        regex = "https"
        action = "keep"
      }
    }
    
    prometheus.scrape "annotation_autodiscovery_http" {
      targets = discovery.relabel.annotation_autodiscovery_http.output
      scrape_interval = "60s"
      honor_labels = true
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }
    
    prometheus.scrape "annotation_autodiscovery_https" {
      targets = discovery.relabel.annotation_autodiscovery_https.output
      scrape_interval = "60s"
      honor_labels = true
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.annotation_autodiscovery.receiver]
    }
    
    prometheus.relabel "annotation_autodiscovery" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Grafana Alloy
    discovery.relabel "alloy" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "alloy.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        regex = "http-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
    }
    
    prometheus.scrape "alloy" {
      job_name = "integrations/alloy"
      targets = discovery.relabel.alloy.output
      scrape_interval = "60s"
      forward_to = [prometheus.relabel.alloy.receiver]
      clustering {
        enabled = true
      }
    }
    
    prometheus.relabel "alloy" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|alloy_build_info"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubernetes Monitoring Telemetry
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8smon"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_build_info"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubelet
    discovery.relabel "kubelet" {
      targets = discovery.kubernetes.nodes.targets
    }
    
    prometheus.scrape "kubelet" {
      job_name   = "integrations/kubernetes/kubelet"
      targets  = discovery.relabel.kubelet.output
      scheme   = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubelet.receiver]
    }
    
    prometheus.relabel "kubelet" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|go_goroutines|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_free|kubelet_volume_stats_inodes_used|kubelet_volume_stats_used_bytes|kubernetes_build_info|namespace_workload_pod|process_cpu_seconds_total|process_resident_memory_bytes|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kubelet Resource
    discovery.relabel "kubelet_resource" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        replacement   = "/metrics/resource"
        target_label  = "__metrics_path__"
      }
    }
    
    prometheus.scrape "kubelet_resource" {
      job_name   = "integrations/kubernetes/resources"
      targets  = discovery.relabel.kubelet_resource.output
      scheme   = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubelet_resource.receiver]
    }
    
    prometheus.relabel "kubelet_resource" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu_usage_seconds_total|node_memory_working_set_bytes"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // cAdvisor
    discovery.relabel "cadvisor" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        replacement   = "/metrics/cadvisor"
        target_label  = "__metrics_path__"
      }
    }
    
    prometheus.scrape "cadvisor" {
      job_name   = "integrations/kubernetes/cadvisor"
      targets    = discovery.relabel.cadvisor.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.cadvisor.receiver]
    }
    
    prometheus.relabel "cadvisor" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_usage_bytes|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
        action = "keep"
      }
      // Drop empty container labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","container"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*)@"
        action = "drop"
      }
      // Drop empty image labels, addressing https://github.com/google/cadvisor/issues/2688
      rule {
        source_labels = ["__name__","image"]
        separator = "@"
        regex = "(container_cpu_.*|container_fs_.*|container_memory_.*|container_network_.*)@"
        action = "drop"
      }
      // Normalizing unimportant labels (not deleting to continue satisfying <label>!="" checks)
      rule {
        source_labels = ["__name__", "boot_id"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "boot_id"
        replacement = "NA"
      }
      rule {
        source_labels = ["__name__", "system_uuid"]
        separator = "@"
        regex = "machine_memory_bytes@.*"
        target_label = "system_uuid"
        replacement = "NA"
      }
      // Filter out non-physical devices/interfaces
      rule {
        source_labels = ["__name__", "device"]
        separator = "@"
        regex = "container_fs_.*@(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dasd.+)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_fs_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_fs_.*"
        target_label = "__keepme"
        replacement = ""
      }
      rule {
        source_labels = ["__name__", "interface"]
        separator = "@"
        regex = "container_network_.*@(en[ospx][0-9].*|wlan[0-9].*|eth[0-9].*)"
        target_label = "__keepme"
        replacement = "1"
      }
      rule {
        source_labels = ["__name__", "__keepme"]
        separator = "@"
        regex = "container_network_.*@"
        action = "drop"
      }
      rule {
        source_labels = ["__name__"]
        regex = "container_network_.*"
        target_label = "__keepme"
        replacement = ""
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // API Server
    discovery.relabel "apiserver" {
      targets = discovery.kubernetes.endpoints.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "default"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_name"]
        regex = "kubernetes"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_endpoint_port_name"]
        regex = "https"
        action = "keep"
      }
    }
    
    prometheus.scrape "apiserver" {
      job_name   = "integrations/kubernetes/apiserver"
      targets    = discovery.relabel.apiserver.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.apiserver.receiver]
    }
    
    prometheus.relabel "apiserver" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Controller Manager
    discovery.relabel "kube_controller_manager" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_component"]
        regex = "kube-controller-manager"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10257"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_controller_manager" {
      job_name   = "kube-controller-manager"
      targets    = discovery.relabel.kube_controller_manager.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_controller_manager.receiver]
    }
    
    prometheus.relabel "kube_controller_manager" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Scheduler
    discovery.relabel "kube_scheduler" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_component"]
        regex = "kube-scheduler"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10259"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_scheduler" {
      job_name   = "kube-scheduler"
      targets    = discovery.relabel.kube_scheduler.output
      scheme     = "https"
      scrape_interval = "60s"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_scheduler.receiver]
    }
    
    prometheus.relabel "kube_scheduler" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube Proxy
    discovery.relabel "kube_proxy" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "kube-system"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_k8s_app"]
        regex = "kube-proxy"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_ip"]
        replacement = "$1:10249"
        target_label = "__address__"
      }
    }
    
    prometheus.scrape "kube_proxy" {
      job_name   = "integrations/kubernetes/kube-proxy"
      targets    = discovery.relabel.kube_proxy.output
      scheme     = "http"
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_proxy.receiver]
    }
    
    prometheus.relabel "kube_proxy" {
      max_cache_size = 100000
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Kube State Metrics
    discovery.relabel "kube_state_metrics" {
      targets = discovery.kubernetes.endpoints.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_release"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "kube-state-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_endpoint_port_name"]
        regex = "http"
        action = "keep"
      }
    }
    
    prometheus.scrape "kube_state_metrics" {
      job_name   = "integrations/kubernetes/kube-state-metrics"
      targets    = discovery.relabel.kube_state_metrics.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kube_state_metrics.receiver]
    }
    
    prometheus.relabel "kube_state_metrics" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|kube_configmap_info|kube_configmap_metadata_resource_version|kube_cronjob.*|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_condition|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_node.*|kube_persistentvolume_status_phase|kube_persistentvolumeclaim_access_mode|kube_persistentvolumeclaim_info|kube_persistentvolumeclaim_labels|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_persistentvolumeclaim_status_phase|kube_pod_completion_time|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_last_terminated_reason|kube_pod_container_status_last_terminated_timestamp|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_restart_policy|kube_pod_spec_volumes_persistentvolumeclaims_info|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_reason|kube_pod_init_.*|kube_replicaset.*|kube_resourcequota|kube_secret_metadata_resource_version|kube_statefulset.*"
        action = "keep"
      }
      rule {
        source_labels = ["__name__"]
        regex = "kube_pod_info"
        action = "drop"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Node Exporter
    discovery.relabel "node_exporter" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_release"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "prometheus-node-exporter.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "replace"
        target_label = "instance"
      }
    }
    
    prometheus.scrape "node_exporter" {
      job_name   = "integrations/node_exporter"
      targets  = discovery.relabel.node_exporter.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.node_exporter.receiver]
    }
    
    prometheus.relabel "node_exporter" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|node_network_receive_bytes_total|node_network_receive_drop_total|node_network_transmit_bytes_total|node_network_transmit_drop_total|process_cpu_seconds_total|process_resident_memory_bytes"
        action = "keep"
      }
      // Drop metrics for certain file systems
      rule {
        source_labels = ["__name__", "fstype"]
        separator = "@"
        regex = "node_filesystem.*@(ramfs|tmpfs)"
        action = "drop"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // OpenCost
    discovery.relabel "opencost" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "opencost"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
    }
    
    prometheus.scrape "opencost" {
      targets    = discovery.relabel.opencost.output
      job_name   = "integrations/kubernetes/opencost"
      honor_labels = true
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.opencost.receiver]
    }
    
    prometheus.relabel "opencost" {
      max_cache_size = 100000
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
        action = "keep"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator PodMonitor objects
    prometheus.operator.podmonitors "pod_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator Probe objects
    prometheus.operator.probes "probes" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Prometheus Operator ServiceMonitor objects
    prometheus.operator.servicemonitors "service_monitors" {
      clustering {
        enabled = true
      }
      scrape {
        default_scrape_interval = "60s"
      }
      forward_to = [prometheus.relabel.metrics_service.receiver]
    }
    
    // Metrics Service
    remote.kubernetes.secret "metrics_service" {
      name = "prometheus"
      namespace = "default"
    }
    
    prometheus.relabel "metrics_service" {
      max_cache_size = 100000
      rule {
        source_labels = ["cluster"]
        regex = ""
        replacement = "ci-test-cluster"
        target_label = "cluster"
      }
      forward_to = [prometheus.remote_write.metrics_service.receiver]
    }
    
    prometheus.remote_write "metrics_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["host"]) + "/api/v1/write"
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["tenantId"]),
        }
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.metrics_service.data["username"])
          password = remote.kubernetes.secret.metrics_service.data["password"]
        }
    
        tls_config {
          insecure_skip_verify = true
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    
      external_labels = {
        color = "red",
      }
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Move high-cardinality labels to structured metadata
      stage.structured_metadata {
        values = {
          name = "app_kubernetes_io_name",
          pod = "",
        }
      }
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
      // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
      // container runtime label as it is no longer needed.
      stage.label_drop {
        values = [
          "filename",
          "tmp_container_runtime",
          "app_kubernetes_io_name",
          "pod",
        ]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    // Tempo
    remote.kubernetes.secret "traces_service" {
      name = "tempo-k8s-monitoring"
      namespace = "default"
    }
    
    
    otelcol.exporter.otlp "traces_service" {
      client {
        endpoint = convert.nonsensitive(remote.kubernetes.secret.traces_service.data["host"])
    
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.traces_service.data["tenantId"]),
        }
        tls {
          insecure             = true
          insecure_skip_verify = true
        }
      }
    }
    
    logging {
      level  = "info"
      format = "logfmt"
    }
    
    livedebugging {
      enabled = true
    }
    tracing {
      sampling_fraction = 0.1
      write_to = [otelcol.processor.k8sattributes.default.input]
    }
  events.alloy: |-
    // Cluster Events
    loki.source.kubernetes_events "cluster_events" {
      job_name   = "integrations/kubernetes/eventhandler"
      log_format = "logfmt"
      forward_to = [
        loki.process.cluster_events.receiver,
      ]
    }
    
    loki.process "cluster_events" {
      forward_to = [
        loki.process.logs_service.receiver,
        loki.echo.cluster_events.receiver,
      ]
    }
    loki.echo "cluster_events" {}
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
  logs.alloy: |-
    // Pod Logs
    discovery.kubernetes "pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "$1"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex         = "(.+)"
        target_label  = "app_kubernetes_io_name"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
        regex         = "(.+)"
        target_label  = "job"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "/var/log/pods/*$1/*.log"
        target_label = "__path__"
      }
    
      // set the container runtime as a label
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_pod_container_id"]
        regex = "^(\\S+):\\/\\/.+$"
        replacement = "$1"
        target_label = "tmp_container_runtime"
      }
    }
    
    discovery.relabel "filtered_pod_logs" {
      targets = discovery.relabel.pod_logs.output
      rule {  // Drop anything with a "falsy" annotation value
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_autogather"]
        regex = "(false|no|skip)"
        action = "drop"
      }
    }
    
    local.file_match "pod_logs" {
      path_targets = discovery.relabel.filtered_pod_logs.output
    }
    
    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=~\"containerd|cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Move high-cardinality labels to structured metadata
      stage.structured_metadata {
        values = {
          name = "app_kubernetes_io_name",
          pod = "",
        }
      }
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
      // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
      // container runtime label as it is no longer needed.
      stage.label_drop {
        values = [
          "filename",
          "tmp_container_runtime",
          "app_kubernetes_io_name",
          "pod",
        ]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "ci-test-cluster",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = convert.nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
  profiles.alloy: |-
    // Profiles: eBPF
    discovery.kubernetes "ebpf_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "ebpf_pods" {
      targets = discovery.kubernetes.ebpf_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex = "Succeeded|Failed|Completed"
        action = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_ebpf_enabled"]
        regex         = "false"
        action        = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label = "node"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label = "container"
      }
      // provide arbitrary service_name label, otherwise it will be set to {__meta_kubernetes_namespace}/{__meta_kubernetes_pod_container_name}
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "@"
        regex = "(.*)@(.*)"
        replacement = "ebpf/${1}/${2}"
        target_label = "service_name"
      }
    }
    
    pyroscope.ebpf "ebpf_pods" {
      targets = discovery.relabel.ebpf_pods.output
      demangle = "none"
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Profiles: Java
    discovery.kubernetes "java_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "potential_java_pods" {
      targets = discovery.kubernetes.java_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex         = "Succeeded|Failed|Completed"
        action        = "drop"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_java_enabled"]
        regex         = "false"
        action        = "drop"
      }
    }
    
    discovery.process "java_pods" {
      join = discovery.relabel.potential_java_pods.output
    }
    
    discovery.relabel "java_pods" {
      targets = discovery.process.java_pods.targets
      rule {
        source_labels = ["__meta_process_exe"]
        action = "keep"
        regex = ".*/java$"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        target_label = "node"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label = "container"
      }
    }
    
    pyroscope.java "java_pods" {
      targets = discovery.relabel.java_pods.output
      profiling_config {
        interval = "60s"
        alloc = "512k"
        cpu = true
        sample_rate = 100
        lock = "10ms"
      }
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Profiles: pprof
    discovery.kubernetes "pprof_pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + sys.env("HOSTNAME")
      }
    }
    
    discovery.relabel "pprof_pods" {
      targets = discovery.kubernetes.pprof_pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex         = "Pending|Succeeded|Failed|Completed"
        action        = "drop"
      }
    
      rule {
        regex  = "__meta_kubernetes_pod_label_(.+)"
        action = "labelmap"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
    }
    
    discovery.relabel "pprof_pods_memory" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_memory" {
      targets = discovery.relabel.pprof_pods_memory.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = true
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_block" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_block_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_block" {
      targets = discovery.relabel.pprof_pods_block.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = true
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_goroutine" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_goroutine_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_goroutine" {
      targets = discovery.relabel.pprof_pods_goroutine.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = true
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_mutex" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_mutex_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_mutex" {
      targets = discovery.relabel.pprof_pods_mutex.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = true
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_cpu" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_cpu_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_cpu" {
      targets = discovery.relabel.pprof_pods_cpu.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = true
        }
        profile.fgprof {
          enabled = false
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    discovery.relabel "pprof_pods_fgprof" {
      targets = discovery.relabel.pprof_pods.output
    
      // Keep only pods with the scrape annotation set
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scrape"]
        regex         = "true"
        action        = "keep"
      }
    
      // Rules to choose the right container
      rule {
        source_labels = ["container"]
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_container"]
        regex = "(.+)"
        target_label = "__tmp_container"
      }
      rule {
        source_labels = ["container"]
        action = "keepequal"
        target_label = "__tmp_container"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_container"
      }
    
      // Rules to choose the right port by name
      // The discovery generates a target for each declared container port of the pod.
      // If the portName annotation has value, keep only the target where the port name matches the one of the annotation.
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port_name"]
        regex = "(.+)"
        target_label = "__tmp_port"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_port_name"]
        action = "keepequal"
        target_label = "__tmp_port"
      }
      rule {
        action = "labeldrop"
        regex = "__tmp_port"
      }
    
      // If the portNumber annotation has a value, override the target address to use it, regardless whether it is
      // one of the declared ports on that Pod.
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
        replacement = "[$2]:$1" // IPv6
        target_label = "__address__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_port", "__meta_kubernetes_pod_ip"]
        regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
        replacement = "$2:$1"
        target_label = "__address__"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_scheme"]
        regex         = "(https?)"
        target_label  = "__scheme__"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_fgprof_path"]
        regex         = "(.+)"
        target_label  = "__profile_path__"
      }
    }
    
    pyroscope.scrape "pyroscope_scrape_fgprof" {
      targets = discovery.relabel.pprof_pods_fgprof.output
    
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      profiling_config {
        profile.memory {
          enabled = false
        }
        profile.block {
          enabled = false
        }
        profile.goroutine {
          enabled = false
        }
        profile.mutex {
          enabled = false
        }
        profile.process_cpu {
          enabled = false
        }
        profile.fgprof {
          enabled = true
        }
        profile.godeltaprof_memory {
          enabled = false
        }
        profile.godeltaprof_mutex {
          enabled = false
        }
        profile.godeltaprof_block {
          enabled = false
        }
      }
    
      scrape_interval = "15s"
      scrape_timeout = "18s"
    
      forward_to = [pyroscope.write.profiles_service.receiver]
    }
    // Pyroscope
    remote.kubernetes.secret "profiles_service" {
      name = "pyroscope-k8s-monitoring"
      namespace = "default"
    }
    pyroscope.write "profiles_service" {
      endpoint {
        url = convert.nonsensitive(remote.kubernetes.secret.profiles_service.data["host"])
        headers = {
          "X-Scope-OrgID" = convert.nonsensitive(remote.kubernetes.secret.profiles_service.data["tenantId"]),
        }
    
      }
      external_labels = {
        cluster = "ci-test-cluster",
      }
    }
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.21.3
    helm.sh/chart: "k8s-monitoring-1.6.48"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-1"
data:
  testQueries.json: |-
    {
      "queries": [
        {
          "query": "up{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "alloy_build_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "kubernetes_build_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "node_cpu_usage_seconds_total{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "machine_memory_bytes{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "kube_node_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "node_exporter_build_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "apiserver_request_total{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "workqueue_adds_total{cluster=\"ci-test-cluster\", job=\"kube-controller-manager\"}",
          "type": "promql"
        },
        {
          "query": "kubeproxy_sync_proxy_rules_service_changes_total{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "scheduler_unschedulable_pods{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "opencost_build_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        },
        {
          "query": "grafana_kubernetes_monitoring_build_info{cluster=\"ci-test-cluster\"}",
          "type": "promql"
        }
      ]
    }
---
# Source: k8s-monitoring/templates/hooks/validate-configuration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "validate-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.21.3
    helm.sh/chart: "k8s-monitoring-1.6.48"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  restartPolicy: Never
  nodeSelector:
        kubernetes.io/os: linux
  tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          operator: Equal
          value: arm64
  containers:
    - name: alloy
      image: "docker.io/grafana/alloy:v1.11.3"
      command:
      - bash
      - -c
      - |
        echo Validating Grafana Alloy config file
        if ! alloy fmt /etc/alloy/config.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run --stability.level generally-available  "/etc/alloy/config.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy config file is valid"
        echo Validating Grafana Alloy for Events config file
        if ! alloy fmt /etc/alloy/events.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run --stability.level generally-available  "/etc/alloy/events.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy for Events config file is valid"
        echo Validating Grafana Alloy for Logs config file
        if ! alloy fmt /etc/alloy/logs.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run --stability.level generally-available  "/etc/alloy/logs.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy for Logs config file is valid"
        echo Validating Grafana Alloy for Profiles config file
        if ! alloy fmt /etc/alloy/profiles.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run --stability.level public-preview  "/etc/alloy/profiles.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy for Profiles config file is valid"
      env:
        - name: KUBERNETES_SERVICE_HOST  # Intentionally disable its connection to Kubernetes to make it fail in a known way
          value: ""
        - name: KUBERNETES_SERVICE_PORT  # Intentionally disable its connection to Kubernetes to make it fail in a known way
          value: ""
      volumeMounts:
        - name: config
          mountPath: /etc/alloy
  volumes:
    - name: config
      configMap:
        name: "validate-k8smon-k8s-monitoring"
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "analyze-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.21.3
    helm.sh/chart: "k8s-monitoring-1.6.48"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "0"
spec:
  restartPolicy: OnFailure
  nodeSelector:
        kubernetes.io/os: linux
  containers:
    - name: config-analysis
      image: ghcr.io/grafana/k8s-monitoring-test:1.6.48
      command: [/etc/bin/config-analysis.sh]
      env:
        - name: ALLOY_HOST
          value: k8smon-alloy.default.svc:12345
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "test-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.21.3
    helm.sh/chart: "k8s-monitoring-1.6.48"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "0"
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 9
  template:
    metadata:
      name: "test-k8smon-k8s-monitoring"
      namespace: default
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "k8smon"
        helm.sh/chart: "k8s-monitoring-1.6.48"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: query-test
          image: ghcr.io/grafana/k8s-monitoring-test:1.6.48
          command: ["bash", "-c", "/etc/bin/query-test.sh /etc/test/testQueries.json"]
          volumeMounts:
            - name: test-files
              mountPath: /etc/test
          env:
            - name: SINCE
              value: 
            - name: PROMETHEUS_HOST
              valueFrom:
                secretKeyRef:
                  name: prometheus
                  key: host
                  optional: true
            - name: PROMETHEUS_URL
              value: $(PROMETHEUS_HOST)/api/v1/query
            - name: PROMETHEUS_USER
              valueFrom:
                secretKeyRef:
                  name: prometheus
                  key: username
                  optional: true
            - name: PROMETHEUS_PASS
              valueFrom:
                secretKeyRef:
                  name: prometheus
                  key: password
                  optional: true
            - name: LOKI_HOST
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: host
            - name: LOKI_URL
              value: $(LOKI_HOST)/loki/api/v1/query
            - name: LOKI_USER
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: username
                  optional: true
            - name: LOKI_PASS
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: password
                  optional: true
            - name: LOKI_TENANTID
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: tenantId
                  optional: true
            - name: TEMPO_HOST
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: host
                  optional: true
            - name: TEMPO_URL
              value: $(TEMPO_HOST)/api/search
            - name: TEMPO_USER
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: username
                  optional: true
            - name: TEMPO_PASS
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: password
                  optional: true
            - name: PROFILECLI_URL
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: host
                  optional: true
            - name: PROFILECLI_USERNAME
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: username
                  optional: true
            - name: PROFILECLI_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: password
                  optional: true
            - name: PROFILECLI_TENANT_ID
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: tenantId
                  optional: true

      volumes:
        - name: test-files
          configMap:
            name: "test-k8smon-k8s-monitoring"
