---
# Source: k8s-monitoring/charts/grafana-agent-logs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-grafana-agent-logs
  labels:
    helm.sh/chart: grafana-agent-logs-0.19.0
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: k8s-monitoring/charts/grafana-agent/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.9.2"
  name: k8smon-kube-state-metrics
  namespace: default
imagePullSecrets:
---
# Source: k8s-monitoring/charts/opencost/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-1.18.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.105.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.6.0"
---
# Source: k8s-monitoring/templates/credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana-agent-credentials
  namespace: default
type: Opaque
data:
  prometheus_host: "aHR0cHM6Ly9wcm9tZXRoZXVzLWRldi0wMS1kZXYtdXMtY2VudHJhbC0wLmdyYWZhbmEtZGV2Lm5ldA=="
  prometheus_username: "MTIxMTk="
  prometheus_password: "Z2xjX2Rldl9leUp2SWpvaU16TTRJaXdpYmlJNkltOTBaV3d0ZEdWemRDSXNJbXNpT2lJelozUkdOVGMwYlRFemVsWnRVa2hUYWpOVlpqUTBNSEFpTENKdElqcDdJbklpT2lKa1pYWXRkWE10WTJWdWRISmhiQ0o5ZlE9PQ=="
  loki_host: "aHR0cHM6Ly9sb2dzLWRldi0wMDUuZ3JhZmFuYS1kZXYubmV0"
  loki_username: "MTQ2MjA1"
  loki_password: "Z2xjX2Rldl9leUp2SWpvaU16TTRJaXdpYmlJNkltOTBaV3d0ZEdWemRDSXNJbXNpT2lJelozUkdOVGMwYlRFemVsWnRVa2hUYWpOVlpqUTBNSEFpTENKdElqcDdJbklpT2lKa1pYWXRkWE10WTJWdWRISmhiQ0o5ZlE9PQ=="
---
# Source: k8s-monitoring/templates/grafana-agent-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-grafana-agent
  namespace: default
data:
  config.river: |-    
    discovery.kubernetes "nodes" {
      role = "node"
    }
        
    discovery.kubernetes "pods" {
      role = "pod"
    }
        
    discovery.kubernetes "services" {
      role = "service"
    }
        
    // Grafana Agent
    discovery.relabel "agent" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "grafana-agent"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "agent" {
      job_name   = "integrations/agent"
      targets  = discovery.relabel.agent.output
      forward_to = [prometheus.relabel.agent.receiver]
    }
    
    prometheus.relabel "agent" {
      rule {
        source_labels = ["__name__"]
        regex = "up|agent_build_info"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // Kubelet
    discovery.relabel "kubelet" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        target_label = "__address__"
        replacement  = "kubernetes.default.svc.cluster.local:443"
      }
      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        regex         = "(.+)"
        replacement   = "/api/v1/nodes/${1}/proxy/metrics"
        target_label  = "__metrics_path__"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "kubelet" {
      job_name   = "integrations/kubernetes/kubelet"
      targets  = discovery.relabel.kubelet.output
      scheme   = "https"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      forward_to = [prometheus.relabel.kubelet.receiver]
    }
    
    prometheus.relabel "kubelet" {
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_usage_seconds_total|kubelet_certificate_manager_client_expiration_renew_errors|kubelet_certificate_manager_client_ttl_seconds|kubelet_certificate_manager_server_ttl_seconds|kubelet_cgroup_manager_duration_seconds_bucket|kubelet_cgroup_manager_duration_seconds_count|kubelet_node_config_error|kubelet_node_name|kubelet_pleg_relist_duration_seconds_bucket|kubelet_pleg_relist_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kubelet_pod_start_duration_seconds_bucket|kubelet_pod_start_duration_seconds_count|kubelet_pod_worker_duration_seconds_bucket|kubelet_pod_worker_duration_seconds_count|kubelet_running_container_count|kubelet_running_containers|kubelet_running_pod_count|kubelet_running_pods|kubelet_runtime_operations_errors_total|kubelet_runtime_operations_total|kubelet_server_expiration_renew_errors|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes|kubelet_volume_stats_inodes_used|kubernetes_build_info|namespace_workload_pod|rest_client_requests_total|storage_operation_duration_seconds_count|storage_operation_errors_total|volume_manager_total_volumes"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // cAdvisor
    discovery.relabel "cadvisor" {
      targets = discovery.kubernetes.nodes.targets
      rule {
        target_label = "__address__"
        replacement  = "kubernetes.default.svc.cluster.local:443"
      }
      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        regex         = "(.+)"
        replacement   = "/api/v1/nodes/${1}/proxy/metrics/cadvisor"
        target_label  = "__metrics_path__"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "cadvisor" {
      job_name   = "integrations/kubernetes/cadvisor"
      targets    = discovery.relabel.cadvisor.output
      scheme     = "https"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      forward_to = [prometheus.relabel.cadvisor.receiver]
    }
    
    prometheus.relabel "cadvisor" {
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_cfs_periods_total|container_cpu_cfs_throttled_periods_total|container_cpu_usage_seconds_total|container_fs_reads_bytes_total|container_fs_reads_total|container_fs_writes_bytes_total|container_fs_writes_total|container_memory_cache|container_memory_rss|container_memory_swap|container_memory_working_set_bytes|container_network_receive_bytes_total|container_network_receive_packets_dropped_total|container_network_receive_packets_total|container_network_transmit_bytes_total|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|machine_memory_bytes"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // Kube State Metrics
    discovery.relabel "kube_state_metrics" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "kube-state-metrics"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "kube_state_metrics" {
      job_name   = "integrations/kubernetes/kube-state-metrics"
      targets    = discovery.relabel.kube_state_metrics.output
      forward_to = [prometheus.relabel.kube_state_metrics.receiver]
    }
    
    prometheus.relabel "kube_state_metrics" {
      rule {
        source_labels = ["__name__"]
        regex = "up|kube_daemonset.*|kube_deployment_metadata_generation|kube_deployment_spec_replicas|kube_deployment_status_observed_generation|kube_deployment_status_replicas_available|kube_deployment_status_replicas_updated|kube_horizontalpodautoscaler_spec_max_replicas|kube_horizontalpodautoscaler_spec_min_replicas|kube_horizontalpodautoscaler_status_current_replicas|kube_horizontalpodautoscaler_status_desired_replicas|kube_job.*|kube_namespace_status_phase|kube_namespace_status_phase|kube_node.*|kube_persistentvolumeclaim_resource_requests_storage_bytes|kube_pod_container_info|kube_pod_container_resource_limits|kube_pod_container_resource_requests|kube_pod_container_status_restarts_total|kube_pod_container_status_waiting_reason|kube_pod_container_status_waiting_reason|kube_pod_info|kube_pod_owner|kube_pod_start_time|kube_pod_status_phase|kube_pod_status_phase|kube_pod_status_reason|kube_replicaset.*|kube_resourcequota|kube_statefulset.*"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // Node Exporter
    discovery.relabel "node_exporter" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_instance"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex = "prometheus-node-exporter.*"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "replace"
        target_label = "instance"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "node_exporter" {
      job_name   = "integrations/node_exporter"
      targets  = discovery.relabel.node_exporter.output
      forward_to = [prometheus.relabel.node_exporter.receiver]
    }
    
    prometheus.relabel "node_exporter" {
      rule {
        source_labels = ["__name__"]
        regex = "up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|process_cpu_seconds_total|process_resident_memory_bytes"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // OpenCost
    discovery.relabel "opencost" {
      targets = discovery.kubernetes.services.targets
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_instance"]
        regex = "k8smon"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
        regex = "opencost"
        action = "keep"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_name"]
        regex = "http"
        action = "keep"
      }
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
    }
    
    prometheus.scrape "opencost" {
      job_name   = "integrations/kubernetes/opencost"
      targets    = discovery.relabel.opencost.output
      forward_to = [prometheus.relabel.opencost.receiver]
    }
    
    prometheus.relabel "opencost" {
      rule {
        source_labels = ["__name__"]
        regex = "up|container_cpu_allocation|container_gpu_allocation|container_memory_allocation_bytes|deployment_match_labels|kubecost_cluster_info|kubecost_cluster_management_cost|kubecost_cluster_memory_working_set_bytes|kubecost_http_requests_total|kubecost_http_response_size_bytes|kubecost_http_response_time_seconds|kubecost_load_balancer_cost|kubecost_network_internet_egress_cost|kubecost_network_region_egress_cost|kubecost_network_zone_egress_cost|kubecost_node_is_spot|node_cpu_hourly_cost|node_gpu_count|node_gpu_hourly_cost|node_ram_hourly_cost|node_total_hourly_cost|opencost_build_info|pod_pvc_allocation|pv_hourly_cost|service_selector_labels|statefulSet_match_labels"
        action = "keep"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // PodMonitor
    prometheus.operator.podmonitors "pod_monitors" {
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // ServiceMonitor
    prometheus.operator.servicemonitors "service_monitors" {
      rule {
        source_labels = ["__name__"]
        replacement   = "otel-collector-test"
        target_label  = "cluster"
      }
      forward_to = [prometheus.remote_write.grafana_cloud_prometheus.receiver]
    }
        
    // Grafana Cloud Prometheus
    local.file "prometheus_host" {
      filename  = "/etc/grafana-agent-credentials/prometheus_host"
    }
    
    local.file "prometheus_username" {
      filename  = "/etc/grafana-agent-credentials/prometheus_username"
    }
    
    local.file "prometheus_password" {
      filename  = "/etc/grafana-agent-credentials/prometheus_password"
      is_secret = true
    }
    
    prometheus.remote_write "grafana_cloud_prometheus" {
      endpoint {
        url = nonsensitive(local.file.prometheus_host.content) + "/api/prom/push"
    
        basic_auth {
          username = local.file.prometheus_username.content
          password = local.file.prometheus_password.content
        }
      }
    }
        
    // Cluster Events
    loki.source.kubernetes_events "cluster_events" {
      job_name   = "integrations/kubernetes/eventhandler"
      forward_to = [loki.process.cluster_events.receiver]
    }
    
    loki.process "cluster_events" {
      stage.static_labels {
        values = {
          cluster = "otel-collector-test",
        }
      }
    
      forward_to = [loki.write.grafana_cloud_loki.receiver]
    }
        
    // Grafana Cloud Loki
    local.file "loki_host" {
      filename  = "/etc/grafana-agent-credentials/loki_host"
    }
    
    local.file "loki_username" {
      filename  = "/etc/grafana-agent-credentials/loki_username"
    }
    
    local.file "loki_password" {
      filename  = "/etc/grafana-agent-credentials/loki_password"
      is_secret = true
    }
    
    loki.write "grafana_cloud_loki" {
      endpoint {
        url = nonsensitive(local.file.loki_host.content) + "/loki/api/v1/push"
    
        basic_auth {
          username = local.file.loki_username.content
          password = local.file.loki_password.content
        }
      }
    }
---
# Source: k8s-monitoring/templates/grafana-agent-logs-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-grafana-agent-logs
  namespace: default
data:
  config.river: |-    
    discovery.kubernetes "pods" {
      role = "pod"
    }
        
    // Pod Logs
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
    
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
        separator = "/"
        action = "replace"
        replacement = "$1"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_node_name"]
        action = "keep"
        regex = env("HOSTNAME")
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "/var/log/pods/*$1/*.log"
        target_label = "__path__"
      }
    }
    
    local.file_match "pod_logs" {
      path_targets = discovery.relabel.pod_logs.output
    }
    
    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }
    
    loki.process "pod_logs" {
      stage.static_labels {
        values = {
          cluster = "otel-collector-test",
        }
      }
    
      stage.docker {}
      forward_to = [loki.write.grafana_cloud_loki.receiver]
    }
        
    // Grafana Cloud Loki
    local.file "loki_host" {
      filename  = "/etc/grafana-agent-credentials/loki_host"
    }
    
    local.file "loki_username" {
      filename  = "/etc/grafana-agent-credentials/loki_username"
    }
    
    local.file "loki_password" {
      filename  = "/etc/grafana-agent-credentials/loki_password"
      is_secret = true
    }
    
    loki.write "grafana_cloud_loki" {
      endpoint {
        url = nonsensitive(local.file.loki_host.content) + "/loki/api/v1/push"
    
        basic_auth {
          username = local.file.loki_username.content
          password = local.file.loki_password.content
        }
      }
    }
---
# Source: k8s-monitoring/charts/grafana-agent-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-grafana-agent-logs
  labels:
    helm.sh/chart: grafana-agent-logs-0.19.0
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/grafana-agent/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.9.2"
  name: k8smon-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: k8s-monitoring/charts/opencost/templates/clusterrole.yaml
# Cluster role giving opencost to get, list, watch required resources
# No write permissions are required
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-1.18.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.105.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - deployments
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - statefulsets
      - deployments
      - daemonsets
      - replicasets
    verbs:
      - list
      - watch
  - apiGroups:
      - batch
    resources:
      - cronjobs
      - jobs
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - storage.k8s.io
    resources:
      - storageclasses
    verbs:
      - get
      - list
      - watch
---
# Source: k8s-monitoring/charts/grafana-agent-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-grafana-agent-logs
  labels:
    helm.sh/chart: grafana-agent-logs-0.19.0
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-grafana-agent-logs
subjects:
  - kind: ServiceAccount
    name: k8smon-grafana-agent-logs
    namespace: default
---
# Source: k8s-monitoring/charts/grafana-agent/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-grafana-agent
subjects:
  - kind: ServiceAccount
    name: k8smon-grafana-agent
    namespace: default
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.9.2"
  name: k8smon-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: k8smon-kube-state-metrics
  namespace: default
---
# Source: k8s-monitoring/charts/opencost/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-1.18.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.105.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-opencost
subjects:
  - kind: ServiceAccount
    name: k8smon-opencost
    namespace: default
---
# Source: k8s-monitoring/charts/grafana-agent-logs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-grafana-agent-logs
  labels:
    helm.sh/chart: grafana-agent-logs-0.19.0
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 80
      targetPort: 80
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/grafana-agent/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-grafana-agent-cluster
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: 'None'
  selector:
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 80
      targetPort: 80
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/grafana-agent/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
  ports:
    - name: http-metrics
      port: 80
      targetPort: 80
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.9.2"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/opencost/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-1.18.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.105.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
  type: ClusterIP
  ports:
    - name: http
      port: 9003
      targetPort: 9003
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.6.0"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
---
# Source: k8s-monitoring/charts/grafana-agent-logs/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-grafana-agent-logs
  labels:
    helm.sh/chart: grafana-agent-logs-0.19.0
    app.kubernetes.io/name: grafana-agent-logs
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana-agent-logs
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana-agent-logs
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-grafana-agent-logs
      containers:
        - name: grafana-agent
          image: docker.io/grafana/agent:v0.35.2
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/agent/config.river
            - --storage.path=/tmp/agent
            - --server.http.listen-addr=0.0.0.0:80
          env:
            - name: AGENT_MODE
              value: flow
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 80
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 80
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/agent
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: dockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            -
              mountPath: /etc/grafana-agent-credentials
              name: grafana-agent-credentials
        - name: config-reloader
          image: docker.io/jimmidyson/configmap-reload:v0.8.0
          args:
            - --volume-dir=/etc/agent
            - --webhook-url=http://localhost:80/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/agent
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-grafana-agent-logs
        - name: varlog
          hostPath:
            path: /var/log
        - name: dockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: grafana-agent-credentials
          secret:
            secretName: grafana-agent-credentials
---
# Source: k8s-monitoring/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.21.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.6.0"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: k8smon
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.21.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "1.6.0"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: k8smon-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.6.0
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: k8s-monitoring/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-5.10.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "2.9.2"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: k8smon
  replicas: 1
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-5.10.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: k8smon
        app.kubernetes.io/version: "2.9.2"
    spec:
      hostNetwork: false
      serviceAccountName: k8smon-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.9.2
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/charts/opencost/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-opencost
  labels:
    helm.sh/chart: opencost-1.18.0
    app.kubernetes.io/name: opencost
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "1.105.0"
    app.kubernetes.io/part-of: opencost
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opencost
      app.kubernetes.io/instance: k8smon
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opencost
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-opencost
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: k8smon-opencost
          image: "quay.io/kubecost1/kubecost-cost-model:prod-1.105.0"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9003
              name: http
          resources:
            limits:
              cpu: 999m
              memory: 1Gi
            requests:
              cpu: 10m
              memory: 55Mi
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 9003
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 3
          env:
            - name: PROMETHEUS_SERVER_ENDPOINT
              value: "https://prom.example.com/api/prom"
            - name: CLUSTER_ID
              value: "default-cluster"
            # If username, password or bearer_token are defined, pull from secrets
            - name: DB_BASIC_AUTH_USERNAME
              valueFrom:
                secretKeyRef:
                  name: grafana-agent-credentials
                  key: prometheus_username
            - name: DB_BASIC_AUTH_PW
              valueFrom:
                secretKeyRef:
                  name: grafana-agent-credentials
                  key: prometheus_password
            # Add any additional provided variables
            - name: CLOUD_PROVIDER_API_KEY
              value: "AIzaSyD29bGxmHAVEOBYtgd8sYM2gM2ekfxQX4U"
            - name: EMIT_KSM_V1_METRICS
              value: "false"
            - name: EMIT_KSM_V1_METRICS_ONLY
              value: "true"
            - name: PROM_CLUSTER_ID_LABEL
              value: "cluster"
---
# Source: k8s-monitoring/charts/grafana-agent/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: grafana-agent-0.19.0
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/instance: k8smon
    app.kubernetes.io/version: "v0.35.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  minReadySeconds: 10
  serviceName: k8smon-grafana-agent
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana-agent
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana-agent
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-grafana-agent
      containers:
        - name: grafana-agent
          image: docker.io/grafana/agent:v0.35.2
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/agent/config.river
            - --storage.path=/tmp/agent
            - --server.http.listen-addr=0.0.0.0:80
            - --cluster.enabled=true
            - --cluster.join-addresses=k8smon-grafana-agent-cluster
          env:
            - name: AGENT_MODE
              value: flow
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 80
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 80
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/agent
            -
              mountPath: /etc/grafana-agent-credentials
              name: grafana-agent-credentials
        - name: config-reloader
          image: docker.io/jimmidyson/configmap-reload:v0.8.0
          args:
            - --volume-dir=/etc/agent
            - --webhook-url=http://localhost:80/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/agent
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-grafana-agent
        - name: grafana-agent-credentials
          secret:
            secretName: grafana-agent-credentials
