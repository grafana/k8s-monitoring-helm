---
# Source: k8s-monitoring/charts/alloy-events/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-events
  namespace: default
  labels:
    helm.sh/chart: alloy-events-0.5.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy-logs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy-logs
  namespace: default
  labels:
    helm.sh/chart: alloy-logs-0.5.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/charts/alloy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8smon-alloy
  namespace: default
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: k8s-monitoring/templates/log-service-credentials.yaml
apiVersion: v1
kind: Secret
metadata:
  name: loki-k8s-monitoring
  namespace: default
type: Opaque
data:

  host: "aHR0cHM6Ly9sb2tpLmV4YW1wbGUuY29t"
  username: "MTIzNDU="
  password: "SXQncyBhIHNlY3JldCB0byBldmVyeW9uZQ=="
  tenantId: "MjAwMA=="
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy
  namespace: default
data:
  config.alloy: |-
    discovery.kubernetes "nodes" {
      role = "node"
    }
    
    discovery.kubernetes "services" {
      role = "service"
    }
    
    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }
    
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    // OTLP Receivers
    otelcol.receiver.otlp "receiver" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      http {
        endpoint = "0.0.0.0:4318"
      }
      debug_metrics {
        disable_high_cardinality_metrics = true
      }
      output {
        logs = [otelcol.processor.resourcedetection.default.input]
      }
    }
    
    
    
    
    // Processors
    
    otelcol.processor.resourcedetection "default" {
      detectors = ["env", "system"]
    
      system {
        hostname_sources = ["os"]
      }
    
      output {
        logs    = [otelcol.processor.k8sattributes.default.input]
      }
    }
    
    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
      }
      pod_association {
        source {
          from = "connection"
        }
      }
    
      output {
        logs    = [otelcol.processor.transform.default.input]
      }
    }
    
    otelcol.processor.transform "default" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      log_statements {
        context = "resource"
        statements = [
          "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
          "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
          "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          "set(attributes[\"k8s.cluster.name\"], \"logs-only-test\") where attributes[\"k8s.cluster.name\"] == nil",
        ]
      }
      output {
        logs = [otelcol.processor.filter.default.input]
      }
    }
    
    otelcol.processor.filter "default" {
      error_mode = "ignore"
    
      output {
        logs = [otelcol.processor.batch.batch_processor.input]
      }
    }
    
    otelcol.processor.batch "batch_processor" {
      send_batch_size = 16384
      send_batch_max_size = 0
      timeout = "2s"
      output {
        logs = [otelcol.exporter.loki.logs_converter.input]
      }
    }
    otelcol.exporter.loki "logs_converter" {
      forward_to = [loki.process.pod_logs.receiver]
    }
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      // if the label tmp_container_runtime from above is docker parse using docker
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have
      // cluster, namespace, pod, and container labels.
      // Also drop the temporary container runtime label as it is no longer needed.
      stage.label_drop {
        values = ["filename", "tmp_container_runtime"]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/alloy-events-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-events
  namespace: default
data:
  config.alloy: |-
    // Cluster Events
    loki.source.kubernetes_events "cluster_events" {
      job_name   = "integrations/kubernetes/eventhandler"
      log_format = "logfmt"
      forward_to = [
        loki.process.cluster_events.receiver,
      ]
    }
    
    loki.process "cluster_events" {
      forward_to = [
        loki.process.logs_service.receiver,
      ]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/alloy-logs-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8smon-alloy-logs
  namespace: default
data:
  config.alloy: |-
    // Pod Logs
    discovery.kubernetes "pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + env("HOSTNAME")
      }
    }
    
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "$1"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex         = "(.+)"
        target_label  = "app_kubernetes_io_name"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
        regex         = "(.+)"
        target_label  = "job"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "/var/log/pods/*$1/*.log"
        target_label = "__path__"
      }
    
      // set the container runtime as a label
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_pod_container_id"]
        regex = "^(\\S+):\\/\\/.+$"
        replacement = "$1"
        target_label = "tmp_container_runtime"
      }
    }
    
    discovery.relabel "filtered_pod_logs" {
      targets = discovery.relabel.pod_logs.output
      rule {  // Drop anything with a "falsy" annotation value
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_autogather"]
        regex = "(false|no|skip)"
        action = "drop"
      }
    }
    
    local.file_match "pod_logs" {
      path_targets = discovery.relabel.filtered_pod_logs.output
    }
    
    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      // if the label tmp_container_runtime from above is docker parse using docker
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have
      // cluster, namespace, pod, and container labels.
      // Also drop the temporary container runtime label as it is no longer needed.
      stage.label_drop {
        values = ["filename", "tmp_container_runtime"]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/kubernetes-monitoring-telemetry.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubernetes-monitoring-telemetry
  namespace: default
data:
  metrics.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart as well as a summary of enabled features
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="1.4.2", namespace="default", metrics="disabled", logs="enabled,events,pod_logs", traces="disabled", deployments=""} 1
---
# Source: k8s-monitoring/charts/alloy-events/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-0.5.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.5.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: k8s-monitoring/charts/alloy-events/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-0.5.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-events
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-events
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-logs/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.5.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy-logs
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy-logs
    namespace: default
---
# Source: k8s-monitoring/charts/alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8smon-alloy
subjects:
  - kind: ServiceAccount
    name: k8smon-alloy
    namespace: default
---
# Source: k8s-monitoring/charts/alloy-events/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-0.5.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy-logs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.5.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: k8s-monitoring/charts/alloy/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy-cluster
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  clusterIP: 'None'
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: TCP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: TCP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/charts/alloy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: TCP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: TCP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/templates/grafana-agent-receiver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8smon-grafana-agent
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: prometheus
      port: 9999
      targetPort: 9999
      protocol: TCP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-binary
      port: 6832
      targetPort: 6832
      protocol: TCP
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: TCP
    - name: jaeger-http
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
---
# Source: k8s-monitoring/charts/alloy-logs/templates/controllers/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8smon-alloy-logs
  labels:
    helm.sh/chart: alloy-logs-0.5.0
    app.kubernetes.io/name: alloy-logs
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-logs
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-logs
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-logs
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.2.0
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
            - name: varlog
              mountPath: /var/log
              readOnly: true
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-logs
        - name: varlog
          hostPath:
            path: /var/log
---
# Source: k8s-monitoring/charts/alloy-events/templates/controllers/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8smon-alloy-events
  labels:
    helm.sh/chart: alloy-events-0.5.0
    app.kubernetes.io/name: alloy-events
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-events
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy-events
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy-events
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.2.0
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy-events
---
# Source: k8s-monitoring/charts/alloy/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: k8smon-alloy
  labels:
    helm.sh/chart: alloy-0.5.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: k8smon
    
    app.kubernetes.io/version: "v1.2.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  podManagementPolicy: Parallel
  minReadySeconds: 10
  serviceName: k8smon-alloy
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy
      app.kubernetes.io/instance: k8smon
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
        k8s.grafana.com/logs.job: integrations/alloy
      labels:
        app.kubernetes.io/name: alloy
        app.kubernetes.io/instance: k8smon
    spec:
      serviceAccountName: k8smon-alloy
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.2.0
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --cluster.enabled=true
            - --cluster.join-addresses=k8smon-alloy-cluster
            - --stability.level=generally-available
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
            - containerPort: 4317
              name: otlp-grpc
              protocol: TCP
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 9999
              name: prometheus
              protocol: TCP
            - containerPort: 14250
              name: jaeger-grpc
              protocol: TCP
            - containerPort: 6832
              name: jaeger-binary
              protocol: TCP
            - containerPort: 6831
              name: jaeger-compact
              protocol: TCP
            - containerPort: 14268
              name: jaeger-http
              protocol: TCP
            - containerPort: 9411
              name: zipkin
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
            -
              mountPath: /etc/kubernetes-monitoring-telemetry
              name: kubernetes-monitoring-telemetry
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      volumes:
        - name: config
          configMap:
            name: k8smon-alloy
        - configMap:
            name: kubernetes-monitoring-telemetry
          name: kubernetes-monitoring-telemetry
---
# Source: k8s-monitoring/templates/hooks/validate-configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "validate-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.6.0
    helm.sh/chart: "k8s-monitoring-1.4.2"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
data:
  config.alloy: |-
    discovery.kubernetes "nodes" {
      role = "node"
    }
    
    discovery.kubernetes "services" {
      role = "service"
    }
    
    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }
    
    discovery.kubernetes "pods" {
      role = "pod"
    }
    
    // OTLP Receivers
    otelcol.receiver.otlp "receiver" {
      grpc {
        endpoint = "0.0.0.0:4317"
      }
      http {
        endpoint = "0.0.0.0:4318"
      }
      debug_metrics {
        disable_high_cardinality_metrics = true
      }
      output {
        logs = [otelcol.processor.resourcedetection.default.input]
      }
    }
    
    
    
    
    // Processors
    
    otelcol.processor.resourcedetection "default" {
      detectors = ["env", "system"]
    
      system {
        hostname_sources = ["os"]
      }
    
      output {
        logs    = [otelcol.processor.k8sattributes.default.input]
      }
    }
    
    otelcol.processor.k8sattributes "default" {
      extract {
        metadata = ["k8s.namespace.name","k8s.pod.name","k8s.deployment.name","k8s.statefulset.name","k8s.daemonset.name","k8s.cronjob.name","k8s.job.name","k8s.node.name","k8s.pod.uid","k8s.pod.start_time"]
      }
      pod_association {
        source {
          from = "connection"
        }
      }
    
      output {
        logs    = [otelcol.processor.transform.default.input]
      }
    }
    
    otelcol.processor.transform "default" {
      // Grafana Cloud Kubernetes monitoring expects Loki labels `cluster`, `pod`, and `namespace`
      error_mode = "ignore"
      log_statements {
        context = "resource"
        statements = [
          "set(attributes[\"pod\"], attributes[\"k8s.pod.name\"])",
          "set(attributes[\"namespace\"], attributes[\"k8s.namespace.name\"])",
          "set(attributes[\"loki.resource.labels\"], \"cluster, namespace, job, pod\")",
          "set(attributes[\"k8s.cluster.name\"], \"logs-only-test\") where attributes[\"k8s.cluster.name\"] == nil",
        ]
      }
      output {
        logs = [otelcol.processor.filter.default.input]
      }
    }
    
    otelcol.processor.filter "default" {
      error_mode = "ignore"
    
      output {
        logs = [otelcol.processor.batch.batch_processor.input]
      }
    }
    
    otelcol.processor.batch "batch_processor" {
      send_batch_size = 16384
      send_batch_max_size = 0
      timeout = "2s"
      output {
        logs = [otelcol.exporter.loki.logs_converter.input]
      }
    }
    otelcol.exporter.loki "logs_converter" {
      forward_to = [loki.process.pod_logs.receiver]
    }
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      // if the label tmp_container_runtime from above is docker parse using docker
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have
      // cluster, namespace, pod, and container labels.
      // Also drop the temporary container runtime label as it is no longer needed.
      stage.label_drop {
        values = ["filename", "tmp_container_runtime"]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
  events.alloy: |-
    // Cluster Events
    loki.source.kubernetes_events "cluster_events" {
      job_name   = "integrations/kubernetes/eventhandler"
      log_format = "logfmt"
      forward_to = [
        loki.process.cluster_events.receiver,
      ]
    }
    
    loki.process "cluster_events" {
      forward_to = [
        loki.process.logs_service.receiver,
      ]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
  logs.alloy: |-
    // Pod Logs
    discovery.kubernetes "pods" {
      role = "pod"
      selectors {
        role = "pod"
        field = "spec.nodeName=" + env("HOSTNAME")
      }
    }
    
    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        action = "replace"
        target_label = "namespace"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        action = "replace"
        target_label = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        action = "replace"
        target_label = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "$1"
        target_label = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
        regex         = "(.+)"
        target_label  = "app_kubernetes_io_name"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
        regex         = "(.+)"
        target_label  = "job"
      }
    
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator = "/"
        action = "replace"
        replacement = "/var/log/pods/*$1/*.log"
        target_label = "__path__"
      }
    
      // set the container runtime as a label
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_pod_container_id"]
        regex = "^(\\S+):\\/\\/.+$"
        replacement = "$1"
        target_label = "tmp_container_runtime"
      }
    }
    
    discovery.relabel "filtered_pod_logs" {
      targets = discovery.relabel.pod_logs.output
      rule {  // Drop anything with a "falsy" annotation value
        source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_autogather"]
        regex = "(false|no|skip)"
        action = "drop"
      }
    }
    
    local.file_match "pod_logs" {
      path_targets = discovery.relabel.filtered_pod_logs.output
    }
    
    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }
    
    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      stage.match {
        selector = "{tmp_container_runtime=\"cri-o\"}"
        // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
        stage.cri {}
    
        // Set the extract flags and stream values as labels
        stage.labels {
          values = {
            flags  = "",
            stream  = "",
          }
        }
      }
    
      // if the label tmp_container_runtime from above is docker parse using docker
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        // the docker processing stage extracts the following k/v pairs: log, stream, time
        stage.docker {}
    
        // Set the extract stream value as a label
        stage.labels {
          values = {
            stream  = "",
          }
        }
      }
    
      // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have
      // cluster, namespace, pod, and container labels.
      // Also drop the temporary container runtime label as it is no longer needed.
      stage.label_drop {
        values = ["filename", "tmp_container_runtime"]
      }
      forward_to = [loki.process.logs_service.receiver]
    }
    
    // Logs Service
    remote.kubernetes.secret "logs_service" {
      name = "loki-k8s-monitoring"
      namespace = "default"
    }
    
    loki.process "logs_service" {
      stage.static_labels {
          values = {
            cluster = "logs-only-test",
          }
      }
      forward_to = [loki.write.logs_service.receiver]
    }
    
    // Loki
    loki.write "logs_service" {
      endpoint {
        url = nonsensitive(remote.kubernetes.secret.logs_service.data["host"]) + "/loki/api/v1/push"
        tenant_id = nonsensitive(remote.kubernetes.secret.logs_service.data["tenantId"])
    
        basic_auth {
          username = nonsensitive(remote.kubernetes.secret.logs_service.data["username"])
          password = remote.kubernetes.secret.logs_service.data["password"]
        }
      }
    }
    
    
    logging {
      level  = "info"
      format = "logfmt"
    }
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.6.0
    helm.sh/chart: "k8s-monitoring-1.4.2"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-1"
data:
  testQueries.json: |-
    {
      "queries": null
    }
---
# Source: k8s-monitoring/templates/hooks/validate-configuration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "validate-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.6.0
    helm.sh/chart: "k8s-monitoring-1.4.2"
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  restartPolicy: Never
  nodeSelector:
        kubernetes.io/os: linux
  containers:
    - name: alloy
      image: "docker.io/grafana/alloy:v1.2.0"
      command:
      - bash
      - -c
      - |
        echo Validating Grafana Alloy config file
        if ! alloy fmt /etc/alloy/config.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run "/etc/alloy/config.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy config file is valid"
        echo Validating Grafana Alloy for Events config file
        if ! alloy fmt /etc/alloy/events.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run "/etc/alloy/events.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy for Events config file is valid"
        echo Validating Grafana Alloy for Logs config file
        if ! alloy fmt /etc/alloy/logs.alloy > /dev/null; then
          exit 1
        fi
        output=$(alloy run "/etc/alloy/logs.alloy" 2>&1)
        if ! echo "${output}" | grep "KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined" >/dev/null; then
          echo "${output}"
          exit 1
        fi
        echo "Grafana Alloy for Logs config file is valid"
      env:
        - name: KUBERNETES_SERVICE_HOST  # Intentionally disable its connection to Kubernetes to make it fail in a known way
          value: ""
        - name: KUBERNETES_SERVICE_PORT  # Intentionally disable its connection to Kubernetes to make it fail in a known way
          value: ""
      volumeMounts:
        - name: config
          mountPath: /etc/alloy
  volumes:
    - name: config
      configMap:
        name: "validate-k8smon-k8s-monitoring"
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "analyze-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.6.0
    helm.sh/chart: "k8s-monitoring-1.4.2"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "0"
spec:
  restartPolicy: OnFailure
  nodeSelector:
        kubernetes.io/os: linux
  containers:
    - name: config-analysis
      image: ghcr.io/grafana/k8s-monitoring-test:1.4.2
      command: [/etc/bin/config-analysis.sh]
      env:
        - name: ALLOY_HOST
          value: k8smon-alloy.default.svc:12345
---
# Source: k8s-monitoring/templates/tests/test.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "test-k8smon-k8s-monitoring"
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "k8smon"
    app.kubernetes.io/version: 2.6.0
    helm.sh/chart: "k8s-monitoring-1.4.2"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation
    "helm.sh/hook-weight": "0"
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 9
  template:
    metadata:
      name: "test-k8smon-k8s-monitoring"
      namespace: default
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "k8smon"
        helm.sh/chart: "k8s-monitoring-1.4.2"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: query-test
          image: ghcr.io/grafana/k8s-monitoring-test:1.4.2
          command: ["bash", "-c", "/etc/bin/query-test.sh /etc/test/testQueries.json"]
          volumeMounts:
            - name: test-files
              mountPath: /etc/test
          env:
            - name: SINCE
              value: 
            - name: PROMETHEUS_HOST
              valueFrom:
                secretKeyRef:
                  name: prometheus-k8s-monitoring
                  key: host
                  optional: true
            - name: PROMETHEUS_URL
              value: $(PROMETHEUS_HOST)/api/prom/api/v1/query
            - name: PROMETHEUS_USER
              valueFrom:
                secretKeyRef:
                  name: prometheus-k8s-monitoring
                  key: username
                  optional: true
            - name: PROMETHEUS_PASS
              valueFrom:
                secretKeyRef:
                  name: prometheus-k8s-monitoring
                  key: password
                  optional: true
            - name: LOKI_HOST
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: host
            - name: LOKI_URL
              value: $(LOKI_HOST)/loki/api/v1/query
            - name: LOKI_USER
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: username
                  optional: true
            - name: LOKI_PASS
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: password
                  optional: true
            - name: LOKI_TENANTID
              valueFrom:
                secretKeyRef:
                  name: loki-k8s-monitoring
                  key: tenantId
                  optional: true
            - name: TEMPO_HOST
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: host
                  optional: true
            - name: TEMPO_URL
              value: $(TEMPO_HOST)/api/search
            - name: TEMPO_USER
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: username
                  optional: true
            - name: TEMPO_PASS
              valueFrom:
                secretKeyRef:
                  name: tempo-k8s-monitoring
                  key: password
                  optional: true
            - name: PROFILECLI_URL
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: host
                  optional: true
            - name: PROFILECLI_USERNAME
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: username
                  optional: true
            - name: PROFILECLI_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: password
                  optional: true
            - name: PROFILECLI_TENANT_ID
              valueFrom:
                secretKeyRef:
                  name: pyroscope-k8s-monitoring
                  key: tenantId
                  optional: true

      volumes:
        - name: test-files
          configMap:
            name: "test-k8smon-k8s-monitoring"
